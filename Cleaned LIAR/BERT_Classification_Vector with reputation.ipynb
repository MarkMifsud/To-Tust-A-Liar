{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we first do the classification using the transformer This is our first classification task.\n",
    "\n",
    "The output classification vector from the transformer is saved to be used by the FCNN This is our second classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "Some pre-processing to the dataset has already been done in preparation for various tests, so this processing is not from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedure for getting the data sets and formatting them for the transformer\n",
    " \n",
    "\n",
    "def prepareDataset( filename):\n",
    "     \n",
    "    ReadSet=pd.read_excel(filename )\n",
    "\n",
    "    ReadSet['text']=ReadSet['Statement']\n",
    "    ReadSet['labels']=ReadSet['Label']\n",
    "    \n",
    "    ReadSet=ReadSet.drop(['ID','Label','Statement','Subject','Speaker','Job','From','Affiliation','PantsTotal','NotRealTotal','BarelyTotal','HalfTotal','MostlyTotal' ,'RealTotal','Context'],axis=1)\n",
    "    \n",
    "\n",
    "    return ReadSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>President Obama is a Muslim.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An independent payment advisory board created ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. Sen. Bill Nelson was the deciding vote fo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Large phone companies and their trade associat...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RIPTA has really some of the fullest buses for...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10094</th>\n",
       "      <td>The Georgia Dome has returned $10 billion in e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10095</th>\n",
       "      <td>Then-Gov. Carl Sanders put 56 percent of the s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10096</th>\n",
       "      <td>Nathan Deal saved the HOPE scholarship program.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10097</th>\n",
       "      <td>John Faso took money from fossil fuel companie...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10098</th>\n",
       "      <td>With the exception of slavery and the Chinese ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10099 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  labels\n",
       "0                           President Obama is a Muslim.       0\n",
       "1      An independent payment advisory board created ...       0\n",
       "2      U.S. Sen. Bill Nelson was the deciding vote fo...       2\n",
       "3      Large phone companies and their trade associat...       4\n",
       "4      RIPTA has really some of the fullest buses for...       4\n",
       "...                                                  ...     ...\n",
       "10094  The Georgia Dome has returned $10 billion in e...       1\n",
       "10095  Then-Gov. Carl Sanders put 56 percent of the s...       4\n",
       "10096    Nathan Deal saved the HOPE scholarship program.       4\n",
       "10097  John Faso took money from fossil fuel companie...       3\n",
       "10098  With the exception of slavery and the Chinese ...       4\n",
       "\n",
       "[10099 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the training dataset\n",
    "train=prepareDataset( 'train-clean.xlsx')\n",
    "# and display for inspecting\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Jerseys once-broken pension system is now ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The new health care law will cut $500 billion ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For thousands of public employees, Wisconsin G...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Because as a Senator Toomey stood up for Wall ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The governors budget proposal reduces the stat...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>You can import as many hemp products into this...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>Says when Republicans took over the state legi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>Wisconsin's laws ranked the worst in the world...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>There currently are 825,000 student stations s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>Black people are eight times more likely to be...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1272 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "0     New Jerseys once-broken pension system is now ...       3\n",
       "1     The new health care law will cut $500 billion ...       2\n",
       "2     For thousands of public employees, Wisconsin G...       3\n",
       "3     Because as a Senator Toomey stood up for Wall ...       4\n",
       "4     The governors budget proposal reduces the stat...       5\n",
       "...                                                 ...     ...\n",
       "1267  You can import as many hemp products into this...       5\n",
       "1268  Says when Republicans took over the state legi...       3\n",
       "1269  Wisconsin's laws ranked the worst in the world...       2\n",
       "1270  There currently are 825,000 student stations s...       4\n",
       "1271  Black people are eight times more likely to be...       3\n",
       "\n",
       "[1272 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the evaluation/validation dataset\n",
    "Eval=prepareDataset('valid-clean.xlsx')\n",
    "# and display for inspecting\n",
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a lawsuit between private citizens, a Flori...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Obama-Nelson economic record: Job creation   a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Says George LeMieux even compared Marco Rubio ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gene Green is the NRAs favorite Democrat in Co...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In labor negotiations with city employees, Mil...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>Says Milwaukee County Executive Chris Abele sp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>The words subhuman mongrel, which Ted Nugent c...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>California's Prop 55 prevents $4 billion in ne...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>Says One of the states largest governments mad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>Expanding the sale of full-strength beer and w...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1255 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "0     In a lawsuit between private citizens, a Flori...       4\n",
       "1     Obama-Nelson economic record: Job creation   a...       4\n",
       "2     Says George LeMieux even compared Marco Rubio ...       2\n",
       "3     Gene Green is the NRAs favorite Democrat in Co...       2\n",
       "4     In labor negotiations with city employees, Mil...       2\n",
       "...                                                 ...     ...\n",
       "1250  Says Milwaukee County Executive Chris Abele sp...       1\n",
       "1251  The words subhuman mongrel, which Ted Nugent c...       5\n",
       "1252  California's Prop 55 prevents $4 billion in ne...       2\n",
       "1253  Says One of the states largest governments mad...       0\n",
       "1254  Expanding the sale of full-strength beer and w...       3\n",
       "\n",
       "[1255 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the test set dataset\n",
    "test=prepareDataset('test-clean.xlsx')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the transformer for fine tuning\n",
    "\n",
    "This is where changes are done to optimise the model\n",
    "\n",
    "The simpletransformers library is the quickest way to do this at the time of writing. \n",
    "For more information on the settings and their default value go here:\n",
    "https://github.com/ThilinaRajapakse/simpletransformers#default-settings \n",
    "\n",
    "###### Please do read that reference before changing any parameters. Don't try to be a hero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model variables were set up: \n"
     ]
    }
   ],
   "source": [
    "#Set the model being used here\n",
    "model_class='bert'  # bert or roberta or albert\n",
    "model_version='bert-base-cased' #bert-base-cased, roberta-base, roberta-large, albert-base-v2 OR albert-large-v2\n",
    "\n",
    "\n",
    "output_folder='./TunedModels/'+model_class+'/'+model_version+\"/\"\n",
    "cache_directory= \"./TunedModels/\"+model_class+\"/\"+model_version+\"/cache/\"\n",
    "labels_count=6  # the number of classification classes\n",
    "\n",
    "print('model variables were set up: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\0 finalThesis\\CleanedText\n",
      "./TunedModels/bert/bert-base-cased/\n",
      "./TunedModels/bert/bert-base-cased/cache/\n"
     ]
    }
   ],
   "source": [
    "# use this to test if writing to the directories is working\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "print(output_folder)\n",
    "print(cache_directory)\n",
    "\n",
    "testWrite=train.head(30)\n",
    " \n",
    "testWrite.to_csv(output_folder+'DeleteThisToo.tsv', sep='\\t')\n",
    "testWrite.to_csv(cache_directory+'DeleteThisToo.tsv', sep='\\t')\n",
    "\n",
    "del(testWrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "save_every_steps=1285\n",
    "# assuming training batch size of 8\n",
    "# any number above 1284 saves the model only at every epoch\n",
    "# Saving the model mid training very often will consume disk space fast\n",
    "\n",
    "train_args={\n",
    "    \"output_dir\":output_folder,\n",
    "    \"cache_dir\":cache_directory,\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'num_train_epochs': 2,\n",
    "    \"save_steps\": save_every_steps, \n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"train_batch_size\": 64,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"evaluate_during_training_steps\": 312,\n",
    "    \"max_seq_length\": 64,\n",
    "    \"n_gpu\": 1,\n",
    "}\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(model_class, model_version, num_labels=labels_count, args=train_args) \n",
    "\n",
    "# You can set class weights by using the optional weight argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a saved model (based on above args{})\n",
    "\n",
    "If you stopped training you can continue training from a previously saved check point.\n",
    "The next cell allows you to load a model from any checkpoint.\n",
    "The number of epochs in the train_args{} will be done and continue tuning from your checkpoint.\n",
    "\n",
    "###### HOWEVER\n",
    "It will overwrite previous checkpoints!\n",
    "Example:  If you load an epoch-3 checkpoint, the epoch-1 checkpoint will be overwritten by the 4th epoch and it will be equivalent to a 4th epoch even if you have epoch-1 in the name.\n",
    "###### SO BE CAREFUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model, please wait...\n",
      "model in use is : ./TunedModels/bert/bert-base-cased/checkpoint-316-epoch-2\n"
     ]
    }
   ],
   "source": [
    "# loading a previously saved model based on this particular Transformer Class and model_name\n",
    "\n",
    "# loading the checkpoint that gave the best result\n",
    "CheckPoint='checkpoint-316-epoch-2' \n",
    "\n",
    "\n",
    "preSavedCheckpoint=output_folder+CheckPoint\n",
    "\n",
    "print('Loading model, please wait...')\n",
    "model = ClassificationModel( model_class, preSavedCheckpoint, num_labels=labels_count, args=train_args) \n",
    "print('model in use is :', preSavedCheckpoint )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Transformer\n",
    "\n",
    "Skip the next cell if you want to skip the training and go directly to the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af5fa9c8d7548c0bef8e67bbe0c9ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10099.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f57e0efd5ce4ce3b5df06c97c8a1bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec73a347655495caa8d4dfe109732ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=158.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.855415"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark\\Anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:110: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Seems like `optimizer.step()` has been overridden after learning rate scheduler \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.711620"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark\\Anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.670800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d061d7371dd4bc1988f8cba942b246a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=158.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.612583\n",
      "\n",
      "Training of bert model complete. Saved to ./TunedModels/bert/bert-base-cased/.\n",
      "Training time:  0:06:22.348176\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "current_time = datetime.now()\n",
    "model.train_model(train)\n",
    "print(\"Training time: \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded from cache at ./TunedModels/bert/bert-base-cased/cache/cached_dev_bert_64_6_10099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5719dc0b0b3439d9dd91747bde35e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1263.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': 0.17723501405117215, 'acc': 0.3291414991583325, 'eval_loss': 1.6195976609572096}\n",
      "Features loaded from cache at ./TunedModels/bert/bert-base-cased/cache/cached_dev_bert_64_6_1272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c1b5cbe08649f8bae37a4e1f8fc906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': 0.10218785027119269, 'acc': 0.2672955974842767, 'eval_loss': 1.6896684942005564}\n",
      "Features loaded from cache at ./TunedModels/bert/bert-base-cased/cache/cached_dev_bert_64_6_1255\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2133a098df74b378701db6280bce28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=157.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': 0.08938332836121785, 'acc': 0.2597609561752988, 'eval_loss': 1.6725422295795125}\n",
      "Training Result: 0.3291414991583325\n",
      "Eval Result: 0.2672955974842767\n",
      "Test Set Result: 0.2597609561752988\n"
     ]
    }
   ],
   "source": [
    "TrainResult, TrainModel_outputs, wrong_predictions = model.eval_model(train, acc=sklearn.metrics.accuracy_score)\n",
    "\n",
    "EvalResult, EvalModel_outputs, wrong_predictions = model.eval_model(Eval, acc=sklearn.metrics.accuracy_score)\n",
    "\n",
    "TestResult, TestModel_outputs, wrong_predictions = model.eval_model(test, acc=sklearn.metrics.accuracy_score)\n",
    "\n",
    "print('Training Result:', TrainResult['acc'])\n",
    "#print('Model Out:', TrainModel_outputs)\n",
    "\n",
    "print('Eval Result:', EvalResult['acc'])\n",
    "#print('Model Out:', EvalModel_outputs)\n",
    "\n",
    "print('Test Set Result:', TestResult['acc'])\n",
    "#print('Model Out:', TestModel_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2775879   0.5595703   0.28515625 -0.25268555 -0.3251953  -0.26660156] 1   4 \n",
      "[ 0.09289551  0.6328125   0.41748047  0.1862793  -0.6479492  -0.5097656 ] 1   4 \n",
      "[-0.00778198  0.4309082   0.45263672  0.16296387 -0.14196777 -0.47094727] 2   2 Match 1\n",
      "\n",
      "[-1.1396484   0.33984375 -0.11975098  0.49975586  0.4086914   0.6386719 ] 5   2 \n",
      "[-0.8334961   0.46166992  0.63378906  0.60595703  0.28808594 -0.11645508] 2   2 Match 2\n",
      "\n",
      "[-1.8007812  -0.08428955 -0.3347168   0.7314453   1.2578125   0.6621094 ] 4   5 \n",
      "[-0.07501221  0.43237305  0.33398438  0.21179199 -0.04544067 -0.41577148] 1   3 \n",
      "[-1.0322266   0.41845703  0.47924805  0.62841797  0.2841797   0.01258087] 3   2 \n",
      "[ 0.3112793   0.4387207   0.07104492 -0.3083496  -0.58251953 -0.32836914] 1   1 Match 3\n",
      "\n",
      "[-0.44140625  0.5         0.63720703  0.4074707  -0.17443848 -0.2154541 ] 2   0 \n",
      "[-1.6318359   0.05465698 -0.359375    0.47875977  0.90966797  0.95947266] 5   5 Match 4\n",
      "\n",
      "[-0.5463867   0.59228516  0.7211914   0.5527344   0.08154297 -0.27685547] 2   2 Match 5\n",
      "\n",
      "[-1.8203125   0.02601624 -0.17956543  0.89941406  1.1611328   0.69873047] 4   5 \n",
      "[-1.359375    0.4194336   0.06161499  0.67822266  0.74902344  0.5751953 ] 4   5 \n",
      "[-1.5273438   0.24560547  0.32226562  0.58740234  0.77246094  0.5605469 ] 4   3 \n",
      "[-0.8203125   0.2854004  -0.02577209  0.41210938  0.23388672  0.46118164] 5   1 \n",
      "[-0.85302734  0.2475586  -0.24487305  0.10223389  0.23144531  0.4963379 ] 5   0 \n",
      "[ 0.00274467  0.51953125  0.4572754   0.06555176 -0.47583008 -0.39111328] 1   1 Match 6\n",
      "\n",
      "[-1.6201172   0.21972656  0.15881348  0.7675781   0.99902344  0.45654297] 4   1 \n",
      "[-0.79296875  0.37304688  0.6035156   0.7553711  -0.00340462 -0.2890625 ] 3   2 \n",
      "[-0.7363281   0.3605957   0.48632812  0.69189453 -0.05331421 -0.3503418 ] 3   1 \n",
      "[-1.4511719   0.31420898 -0.11663818  0.5263672   0.6665039   0.74072266] 5   0 \n",
      "[-0.8232422   0.08203125  0.07946777  0.57910156  0.5654297   0.19750977] 3   4 \n",
      "[-0.56152344  0.5385742   0.52978516  0.40649414  0.08508301 -0.0914917 ] 1   2 \n",
      "[-1.5029297   0.14135742  0.46289062  1.0048828   0.6899414   0.31811523] 3   2 \n",
      "[-0.8754883   0.57373047  0.4638672   0.47875977  0.41796875  0.07855225] 1   4 \n",
      "[-0.61621094  0.43579102  0.6977539   0.6738281   0.29418945 -0.4350586 ] 2   1 \n",
      "[-1.2119141   0.20690918  0.2322998   0.6171875   0.59521484  0.3347168 ] 3   3 Match 7\n",
      "\n",
      "[-1.0761719   0.5595703   0.4724121   0.52441406  0.11273193  0.11975098] 1   0 \n",
      "[-1.1464844   0.36914062  0.27001953  0.94140625  0.37695312  0.14416504] 3   2 \n",
      "[-1.2177734   0.4453125   0.55810547  0.5957031   0.39892578  0.08337402] 3   3 Match 8\n",
      "\n",
      "[-1.0644531   0.12493896  0.53027344  0.8208008   0.57421875 -0.18603516] 3   1 \n",
      "[-0.49682617  0.4621582   0.4741211   0.5097656   0.11120605 -0.3334961 ] 3   1 \n",
      "[-0.5024414   0.36621094  0.7290039   0.69189453 -0.05957031 -0.4765625 ] 2   1 \n",
      "[-1.1552734   0.25732422  0.48046875  0.8305664   0.57958984 -0.18493652] 3   1 \n",
      "[ 0.16210938  0.6635742   0.46044922 -0.03479004 -0.5859375  -0.4194336 ] 1   2 \n",
      "[-0.80908203  0.23730469  0.5859375   0.80126953  0.2055664  -0.48632812] 3   2 \n",
      "[-0.3659668   0.46484375  0.6640625   0.5019531  -0.14807129 -0.37451172] 2   3 \n",
      "[-0.64990234  0.42260742  0.37231445  0.55908203  0.07537842 -0.27978516] 3   4 \n",
      "[-0.72021484  0.38598633  0.3317871   0.52197266  0.18237305  0.04797363] 3   2 \n",
      "[-1.6767578   0.06137085 -0.33544922  0.6767578   1.09375     0.70751953] 4   5 \n",
      "[-1.1640625   0.22033691  0.22961426  0.54052734  0.60839844  0.30566406] 4   2 \n",
      "[-1.4648438   0.2064209  -0.03961182  0.43359375  0.7758789   0.8203125 ] 5   1 \n",
      "[ 0.36938477  0.5546875   0.3046875  -0.22875977 -0.4567871  -0.36450195] 1   0 \n",
      "[-1.4267578   0.18823242  0.4699707   0.88964844  0.4567871   0.00540543] 3   2 \n",
      "[-0.49975586  0.4699707   0.53808594  0.60546875 -0.1114502  -0.4831543 ] 3   2 \n",
      "[-1.0693359   0.37182617  0.3371582   0.5703125   0.5527344   0.07739258] 3   1 \n",
      "[-1.1298828   0.20690918  0.18811035  0.56640625  0.5708008   0.24133301] 4   3 \n",
      "[-0.47558594  0.2286377   0.19006348  0.26464844  0.26049805 -0.12194824] 3   3 Match 9\n",
      "\n",
      "[-1.0419922   0.55078125  0.37524414  0.5854492   0.29516602  0.21643066] 3   1 \n",
      "[-1.0263672   0.30126953  0.2479248   0.5810547   0.3491211   0.14013672] 3   2 \n",
      "[-1.6894531   0.04263306 -0.35131836  0.58935547  1.1503906   0.8959961 ] 4   3 \n",
      "[ 0.3149414   0.36279297  0.21179199 -0.26367188 -0.7236328  -0.5854492 ] 1   2 \n",
      "[-1.8613281   0.19311523 -0.06396484  0.80371094  0.86816406  0.84277344] 4   5 \n",
      "[ 0.12646484  0.57666016  0.24609375 -0.01158142 -0.37353516 -0.4104004 ] 1   0 \n",
      "[-1.8955078   0.03814697 -0.30078125  0.65283203  1.3320312   0.7519531 ] 4   5 \n",
      "[-0.11077881  0.3779297   0.4814453   0.38427734 -0.1072998  -0.5517578 ] 2   5 \n",
      "[-0.93847656  0.4140625  -0.00868225  0.3642578   0.3918457   0.24658203] 1   1 Match 10\n",
      "\n",
      "[ 0.1307373   0.55371094  0.32641602  0.02095032 -0.36083984 -0.34179688] 1   3 \n",
      "[-0.2680664   0.31958008 -0.11657715 -0.20947266  0.14135742  0.4104004 ] 5   1 \n",
      "[-0.3334961   0.5229492   0.5649414   0.43066406 -0.19665527 -0.38891602] 2   2 Match 11\n",
      "\n",
      "[-1.4375      0.3244629   0.23217773  0.6904297   0.49902344  0.64160156] 3   5 \n",
      "[-1.5117188   0.10046387  0.03646851  0.5830078   1.0771484   0.78027344] 4   5 \n",
      "[-0.75878906  0.25048828  0.6152344   0.6376953   0.12103271 -0.3059082 ] 3   2 \n",
      "[-0.2722168   0.40014648  0.6142578   0.44091797 -0.23327637 -0.39111328] 2   5 \n",
      "[-1.0605469   0.25170898 -0.20202637  0.19909668  0.45239258  0.7104492 ] 5   5 Match 12\n",
      "\n",
      "[-1.09375     0.30688477  0.47338867  0.6254883   0.31201172  0.265625  ] 3   5 \n",
      "[-1.2900391   0.2130127   0.6088867   0.7841797   0.6401367   0.31811523] 3   2 \n",
      "[-0.70703125  0.34887695  0.2590332   0.3330078   0.02966309  0.36376953] 5   1 \n",
      "[-0.5942383   0.2548828   0.45825195  0.6035156   0.21728516 -0.08953857] 3   3 Match 13\n",
      "\n",
      "[-1.2802734   0.2154541  -0.06384277  0.5595703   0.7631836   0.50146484] 4   1 \n",
      "[-1.1396484   0.3173828   0.46777344  0.6904297   0.3840332   0.01525879] 3   3 Match 14\n",
      "\n",
      "[-0.6357422   0.3269043   0.32836914  0.31884766 -0.0552063   0.04812622] 2   3 \n",
      "[-1.4091797   0.1586914   0.2253418   0.8881836   0.68652344  0.15576172] 3   5 \n",
      "[-0.47485352  0.46044922  0.19299316  0.18981934  0.0760498  -0.04269409] 1   4 \n",
      "[-0.7060547   0.50341797  0.41015625  0.58203125  0.12225342 -0.20349121] 3   1 \n",
      "[-0.73046875  0.54296875 -0.10296631  0.04016113  0.22265625  0.2265625 ] 1   5 \n",
      "[-1.7666016   0.03134155 -0.39746094  0.60595703  1.3105469   1.0068359 ] 4   4 Match 15\n",
      "\n",
      "[-0.90722656  0.55078125  0.49072266  0.68603516  0.27197266 -0.02578735] 3   3 Match 16\n",
      "\n",
      "[-1.4453125   0.140625    0.24719238  0.8461914   0.8359375   0.375     ] 3   1 \n",
      "[-1.3193359   0.14990234  0.31323242  0.86376953  0.6948242   0.21313477] 3   4 \n",
      "[-1.5097656   0.3251953   0.42041016  0.92822266  0.6586914   0.06292725] 3   3 Match 17\n",
      "\n",
      "[-0.84472656  0.2775879   0.33398438  0.671875    0.02763367 -0.08526611] 3   4 \n",
      "[-0.80615234  0.48217773  0.24536133  0.37182617  0.15136719  0.24694824] 1   3 \n",
      "[-0.47680664  0.6069336   0.52734375  0.4741211  -0.12219238 -0.17626953] 1   2 \n",
      "[-1.4560547   0.26635742  0.2944336   0.65527344  0.3984375   0.4025879 ] 3   4 \n",
      "[-1.4033203   0.06616211  0.43164062  0.89160156  0.5083008   0.18652344] 3   4 \n",
      "[-0.80566406  0.6401367   0.4267578   0.50878906  0.06488037  0.01664734] 1   1 Match 18\n",
      "\n",
      "[-1.6738281  -0.04968262 -0.5161133   0.71728516  1.1953125   0.9770508 ] 4   5 \n",
      "[-1.3798828   0.02087402  0.23144531  0.85302734  0.52490234  0.05807495] 3   4 \n",
      "[-1.7861328   0.02853394  0.0725708   0.88916016  1.1103516   0.4584961 ] 4   4 Match 19\n",
      "\n",
      "[-0.8725586   0.43554688  0.6020508   0.79003906  0.23779297 -0.2680664 ] 3   5 \n",
      "[-1.3867188   0.14404297  0.03909302  0.53271484  0.7006836   0.47070312] 4   1 \n",
      "[-1.6650391  -0.01210785 -0.0814209   0.7495117   1.1601562   0.6669922 ] 4   3 \n",
      "[-1.6552734   0.1340332   0.28564453  0.98095703  0.96240234  0.3581543 ] 3   3 Match 20\n",
      "\n",
      "[-1.6445312e+00 -3.6346436e-02  1.3141632e-03  6.5332031e-01\n",
      "  8.9501953e-01  6.4648438e-01] 4   3 \n",
      "[-0.48510742  0.50341797  0.70214844  0.6611328  -0.08331299 -0.47973633] 2   4 \n",
      "[-0.8828125   0.31567383  0.5151367   0.6381836   0.6279297  -0.09643555] 3   3 Match 21\n",
      "\n",
      "[-1.2392578   0.35717773  0.58154297  0.7290039   0.42358398  0.1484375 ] 3   4 \n",
      "[-0.33496094  0.45751953  0.5175781   0.5185547   0.1204834  -0.24938965] 3   4 \n",
      "[-0.5566406   0.3449707   0.5498047   0.61328125  0.16723633 -0.17810059] 3   3 Match 22\n",
      "\n",
      "[-0.8300781   0.12976074  0.51171875  0.77197266  0.51171875 -0.24047852] 3   4 \n",
      "[ 0.01043701  0.49365234  0.13427734 -0.09893799 -0.30029297 -0.12225342] 1   3 \n",
      "[-1.3876953   0.18286133  0.07104492  0.62841797  0.87841797  0.5019531 ] 4   5 \n",
      "[-1.6328125   0.09661865 -0.0715332   0.6459961   1.2373047   0.44335938] 4   4 Match 23\n",
      "\n",
      "[-1.3261719   0.3059082   0.59375     0.7548828   0.59814453  0.2998047 ] 3   4 \n",
      "[-0.03549194  0.60595703  0.21984863 -0.06201172 -0.37231445 -0.08068848] 1   2 \n",
      "[-1.3427734   0.09869385  0.28759766  0.609375    0.45092773  0.33618164] 3   2 \n",
      "[-1.5048828  -0.02861023  0.15209961  0.8652344   0.8901367   0.31469727] 4   3 \n",
      "[-1.4306641   0.05639648  0.05209351  0.79345703  0.6738281   0.19311523] 3   3 Match 24\n",
      "\n",
      "[-1.3945312   0.16796875  0.14526367  0.71191406  0.75390625  0.5522461 ] 4   4 Match 25\n",
      "\n",
      "[-1.6738281  -0.10333252 -0.23510742  0.70458984  1.0986328   0.69189453] 4   4 Match 26\n",
      "\n",
      "[-1.6083984   0.18615723 -0.45825195  0.4116211   1.1796875   0.94189453] 4   4 Match 27\n",
      "\n",
      "[-1.2753906   0.16503906  0.2956543   0.8828125   0.79541016  0.05587769] 3   4 \n",
      "[-1.6113281   0.09631348 -0.60498047  0.5830078   1.1279297   1.0830078 ] 4   4 Match 28\n",
      "\n",
      "[ 0.29296875  0.40698242  0.2590332  -0.2746582  -0.5107422  -0.32983398] 1   2 \n",
      "[-1.5673828   0.06628418 -0.46020508  0.5761719   1.2236328   0.8588867 ] 4   4 Match 29\n",
      "\n",
      "[-0.9355469   0.25097656  0.49658203  0.85595703  0.265625   -0.296875  ] 3   3 Match 30\n",
      "\n",
      "[-0.35253906  0.25854492  0.35791016  0.4116211   0.0397644  -0.16149902] 3   5 \n",
      "[-1.0449219   0.2668457  -0.20825195  0.29248047  0.57128906  0.41479492] 4   3 \n",
      "[-1.1416016  -0.02003479  0.19604492  0.7558594   0.30078125 -0.01847839] 3   3 Match 31\n",
      "\n",
      "[-1.5390625   0.26733398  0.31469727  0.9707031   0.69189453  0.46704102] 3   3 Match 32\n",
      "\n",
      "[-0.73535156  0.38549805 -0.11779785  0.14172363  0.22937012  0.39941406] 5   1 \n",
      "[-0.30126953  0.27807617  0.16723633  0.2421875  -0.1295166   0.00228119] 1   5 \n",
      "[-0.28271484  0.4333496   0.32861328  0.4350586   0.14257812 -0.4091797 ] 3   0 \n",
      "[-1.2958984   0.08258057 -0.4194336   0.30541992  0.85253906  0.8925781 ] 5   1 \n",
      "[-1.6572266  -0.01039886 -0.2890625   0.7451172   0.91064453  0.6616211 ] 4   5 \n",
      "[-1.890625   -0.19042969 -0.5649414   0.7885742   1.2714844   1.2216797 ] 4   4 Match 33\n",
      "\n",
      "[-1.7578125  -0.15576172 -0.35058594  0.7363281   1.2167969   0.7890625 ] 4   4 Match 34\n",
      "\n",
      "[-0.48486328  0.4873047   0.5083008   0.4987793  -0.05651855 -0.15014648] 2   4 \n",
      "[-0.23791504  0.39941406  0.41186523  0.2626953  -0.12646484 -0.2927246 ] 2   2 Match 35\n",
      "\n",
      "[-1.4121094   0.12341309 -0.10302734  0.5708008   0.78515625  0.46948242] 4   4 Match 36\n",
      "\n",
      "[-1.5175781  -0.01605225 -0.32641602  0.55078125  1.1220703   0.54296875] 4   4 Match 37\n",
      "\n",
      "[-1.2617188   0.18591309  0.16088867  0.69433594  0.7446289   0.42797852] 4   4 Match 38\n",
      "\n",
      "[-1.1796875   0.33789062  0.56103516  0.83203125  0.5644531   0.01182556] 3   3 Match 39\n",
      "\n",
      "[-1.4375     -0.08483887  0.12078857  0.8769531   0.8178711   0.1842041 ] 3   5 \n",
      "[-0.24511719  0.34375     0.37841797  0.37597656 -0.01901245 -0.56396484] 2   2 Match 40\n",
      "\n",
      "[-0.6455078   0.33447266  0.3149414   0.48632812 -0.1730957  -0.12280273] 3   4 \n",
      "[ 0.5419922   0.47607422  0.25       -0.14685059 -0.6796875  -0.3564453 ] 0   2 \n",
      "[-1.5917969   0.01402283  0.20251465  0.8466797   0.60839844  0.3334961 ] 3   2 \n",
      "[-0.27001953  0.24645996 -0.2939453  -0.07617188 -0.05249023  0.31469727] 5   0 \n",
      "[ 0.4489746   0.63378906  0.2890625  -0.14294434 -0.63427734 -0.6982422 ] 1   1 Match 41\n",
      "\n",
      "[-1.3525391   0.08825684  0.08227539  0.54833984  0.74560547  0.43017578] 4   5 \n",
      "[-0.46606445  0.42089844  0.7006836   0.65234375  0.05532837 -0.6875    ] 2   3 \n",
      "[-0.43481445  0.18188477  0.3605957   0.25732422  0.1072998  -0.1784668 ] 2   2 Match 42\n",
      "\n",
      "[-0.05618286  0.3244629   0.1418457  -0.02058411 -0.01461792 -0.02819824] 1   3 \n",
      "[-0.97753906  0.3154297   0.3959961   0.6401367   0.24365234  0.05984497] 3   4 \n",
      "[-1.6875     -0.04476929 -0.35351562  0.72509766  0.8847656   1.1630859 ] 5   4 \n",
      "[-1.4160156   0.2668457   0.24536133  0.7319336   0.48608398  0.29296875] 3   1 \n",
      "[-1.0126953   0.20483398  0.27270508  0.62402344  0.20007324  0.2401123 ] 3   5 \n",
      "[-1.5634766   0.25463867  0.19177246  0.7631836   0.9277344   0.4440918 ] 4   4 Match 43\n",
      "\n",
      "[-1.9619141  -0.13500977 -0.43896484  0.6982422   1.2490234   1.2050781 ] 4   5 \n",
      "[-1.8056641  -0.04495239 -0.40161133  0.7949219   1.2011719   0.87402344] 4   5 \n",
      "[-0.9355469   0.30126953  0.6328125   0.7368164   0.23510742 -0.22180176] 3   4 \n",
      "[-1.7587891   0.01991272 -0.36889648  0.65234375  1.1943359   0.96435547] 4   4 Match 44\n",
      "\n",
      "[-1.5810547e+00 -5.9413910e-04 -4.7798157e-03  8.3251953e-01\n",
      "  8.1445312e-01  3.5693359e-01] 3   2 \n",
      "[-0.27563477  0.67871094  0.75634766  0.54833984 -0.15576172 -0.3383789 ] 2   2 Match 45\n",
      "\n",
      "[-1.8066406   0.01000977 -0.10302734  0.79052734  1.1494141   0.80322266] 4   4 Match 46\n",
      "\n",
      "[ 0.40551758  0.54248047  0.31054688 -0.26489258 -0.7758789  -0.52490234] 1   0 \n",
      "[-0.2052002   0.47558594  0.40014648  0.15283203 -0.27148438 -0.21875   ] 1   0 \n",
      "[ 0.5361328   0.4177246   0.20092773 -0.34545898 -0.7319336  -0.48120117] 0   0 Match 47\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.68603516  0.37524414  0.20898438 -0.39941406 -1.0322266  -0.5600586 ] 0   5 \n",
      "[ 0.00383568  0.45922852  0.38745117  0.19946289 -0.4272461  -0.59277344] 1   1 Match 48\n",
      "\n",
      "[-1.1123047   0.19580078  0.04800415  0.6015625   0.65625     0.10675049] 4   1 \n",
      "[-0.96435547  0.07806396  0.00664139  0.30200195  0.52783203  0.43066406] 4   0 \n",
      "[ 0.16369629  0.5083008   0.10180664 -0.09558105 -0.42578125 -0.2006836 ] 1   0 \n",
      "[ 0.30151367  0.39404297  0.0880127  -0.25561523 -0.33642578 -0.27929688] 1   0 \n",
      "[-1.6230469   0.109375   -0.11517334  0.67529297  0.75634766  0.6586914 ] 4   4 Match 49\n",
      "\n",
      "[-1.515625    0.24475098  0.35253906  0.7216797   0.79052734  0.35839844] 4   4 Match 50\n",
      "\n",
      "[ 0.02891541  0.6020508   0.65478516  0.33984375 -0.41137695 -0.8442383 ] 2   3 \n",
      "[-1.2646484   0.17907715  0.23303223  0.7451172   0.51220703  0.19189453] 3   3 Match 51\n",
      "\n",
      "[-1.2138672   0.47607422  0.15148926  0.6713867   0.20690918  0.48876953] 3   5 \n",
      "[-0.95166016  0.3178711   0.24536133  0.64160156  0.24816895  0.17980957] 3   3 Match 52\n",
      "\n",
      "[-0.37353516  0.35180664 -0.3071289  -0.32080078  0.30932617  0.7397461 ] 5   5 Match 53\n",
      "\n",
      "[-1.2099609   0.25024414 -0.14416504  0.62158203  0.50683594  0.6142578 ] 3   5 \n",
      "[-1.8037109   0.1227417  -0.32495117  0.6821289   0.9824219   0.8334961 ] 4   1 \n",
      "[-1.6650391   0.15942383  0.05181885  0.77246094  0.7871094   0.6333008 ] 4   5 \n",
      "[-1.5634766   0.09411621 -0.056427    0.5390625   0.9350586   0.6015625 ] 4   2 \n",
      "[-1.7519531  -0.0252533  -0.70410156  0.5727539   1.2246094   1.1816406 ] 4   4 Match 54\n",
      "\n",
      "[-1.6689453   0.07922363 -0.15063477  0.90283203  1.0771484   0.6147461 ] 4   3 \n",
      "[-1.3212891   0.24353027 -0.06036377  0.4880371   0.5541992   0.61572266] 5   5 Match 55\n",
      "\n",
      "[-1.2392578   0.35058594  0.40576172  0.7084961   0.28686523  0.29614258] 3   0 \n",
      "[-0.93066406  0.3310547   0.5722656   0.79052734  0.05258179 -0.10107422] 3   0 \n",
      "[-1.2597656   0.33569336  0.32714844  0.8066406   0.38793945  0.19799805] 3   2 \n",
      "[-1.2246094   0.24169922 -0.13134766  0.26342773  0.4765625   0.8432617 ] 5   4 \n",
      "[-0.05609131  0.54541016  0.2631836   0.06549072 -0.34033203 -0.42407227] 1   0 \n",
      "[-1.4941406   0.21020508  0.1194458   0.6411133   0.84277344  0.6435547 ] 4   5 \n",
      "[-0.7949219   0.40063477  0.3935547   0.42382812  0.27490234  0.01456451] 3   2 \n",
      "[ 0.4020996   0.46606445  0.28076172 -0.30200195 -0.7260742  -0.46264648] 1   1 Match 56\n",
      "\n",
      "[ 0.01701355  0.51171875  0.19311523 -0.05075073 -0.17749023 -0.21691895] 1   5 \n",
      "[-0.6074219   0.44506836  0.27075195  0.4025879  -0.08691406 -0.27807617] 1   2 \n",
      "[ 0.02970886  0.62060547  0.36694336  0.17785645 -0.23144531 -0.61572266] 1   5 \n",
      "[-0.28808594  0.46240234  0.4831543   0.47338867 -0.45263672 -0.66503906] 2   0 \n",
      "[-1.3193359   0.04681396  0.18579102  0.90234375  0.3251953   0.03994751] 3   4 \n",
      "[-1.1230469   0.1081543  -0.49291992  0.26953125  0.3671875   0.6220703 ] 5   4 \n",
      "[-1.2792969   0.28295898  0.49389648  0.81640625  0.6748047   0.16955566] 3   4 \n",
      "[-1.1083984   0.42456055  0.51953125  0.5908203   0.5541992  -0.04629517] 3   3 Match 57\n",
      "\n",
      "[-0.9902344   0.45532227  0.4519043   0.4008789   0.05200195  0.38427734] 1   5 \n",
      "[ 0.4350586   0.6582031   0.17163086 -0.22973633 -0.66308594 -0.4729004 ] 1   5 \n",
      "[ 0.02409363  0.4880371   0.5761719   0.36206055 -0.29492188 -0.67529297] 2   2 Match 58\n",
      "\n",
      "[-0.53271484  0.41992188 -0.0451355   0.01120758  0.10943604  0.3720703 ] 1   5 \n",
      "[-1.3867188   0.00789642 -0.12127686  0.6352539   0.78222656  0.8388672 ] 5   5 Match 59\n",
      "\n",
      "[ 0.23156738  0.5024414   0.15515137 -0.19091797 -0.46777344 -0.21154785] 1   1 Match 60\n",
      "\n",
      "[-1.2646484   0.10351562  0.20910645  0.5488281   0.70654297  0.36376953] 4   1 \n",
      "[ 0.01327515  0.5449219   0.5839844   0.14587402 -0.23852539 -0.5683594 ] 2   0 \n",
      "[ 0.6479492   0.3918457   0.25317383 -0.35205078 -0.77734375 -0.6586914 ] 0   2 \n",
      "[-0.88671875  0.19262695 -0.12145996  0.23510742  0.30737305  0.7182617 ] 5   0 \n",
      "[-0.42749023  0.60058594  0.5961914   0.39794922 -0.21020508 -0.22692871] 1   0 \n",
      "[ 0.17480469  0.5605469   0.09283447 -0.21362305 -0.36035156 -0.29833984] 1   0 \n",
      "[-0.6640625   0.45776367  0.04922485  0.25048828  0.11065674  0.18029785] 1   0 \n",
      "[-0.76464844  0.4958496   0.22021484  0.34179688 -0.0274353   0.2861328 ] 1   1 Match 61\n",
      "\n",
      "[ 0.4128418   0.42797852  0.2322998  -0.12817383 -0.4086914  -0.6254883 ] 1   0 \n",
      "[-1.4863281  -0.14050293 -0.27124023  0.7246094   1.1298828   0.71484375] 4   1 \n",
      "[-0.72753906  0.48413086  0.51708984  0.51220703  0.08660889 -0.08843994] 2   2 Match 62\n",
      "\n",
      "[-0.13256836  0.53808594  0.09289551  0.02740479 -0.24304199 -0.09735107] 1   2 \n",
      "[-1.4970703   0.10131836 -0.61035156  0.42944336  1.1074219   0.98876953] 4   4 Match 63\n",
      "\n",
      "[ 0.48876953  0.5131836   0.20153809 -0.21166992 -0.89160156 -0.58984375] 1   0 \n",
      "[-1.6962891   0.07427979  0.18591309  0.9301758   0.7739258   0.32495117] 3   2 \n",
      "[-1.1738281   0.25585938  0.22644043  0.60839844  0.63378906  0.15112305] 4   5 \n",
      "[-0.81103516  0.25073242 -0.34472656 -0.02737427  0.30908203  0.7651367 ] 5   5 Match 64\n",
      "\n",
      "[-1.6552734  -0.00199699 -0.24914551  0.5708008   1.1552734   0.94140625] 4   4 Match 65\n",
      "\n",
      "[-1.4023438   0.20825195  0.23132324  0.7167969   0.53759766  0.37036133] 3   5 \n",
      "[-1.4111328   0.06359863 -0.15136719  0.5698242   0.62109375  0.5786133 ] 4   2 \n",
      "[-1.4248047   0.12719727  0.0670166   0.6933594   0.45483398  0.3491211 ] 3   4 \n",
      "[-0.4309082   0.57128906  0.2331543   0.14233398 -0.16101074  0.3022461 ] 1   2 \n",
      "[-1.328125    0.15771484  0.13244629  0.9448242   0.63916016  0.08654785] 3   3 Match 66\n",
      "\n",
      "[-1.8125     -0.04241943 -0.51708984  0.60253906  1.3857422   1.0488281 ] 4   5 \n",
      "[-0.8442383   0.42260742 -0.0980835   0.08947754  0.31860352  0.5185547 ] 5   1 \n",
      "[-1.8193359  -0.08636475 -0.64404297  0.62353516  1.2431641   1.1308594 ] 4   1 \n",
      "[-0.22558594  0.5229492   0.28222656  0.07189941 -0.30371094 -0.09735107] 1   3 \n",
      "[-1.4931641   0.07196045  0.17944336  0.9350586   0.6333008   0.25878906] 3   3 Match 67\n",
      "\n",
      "[-1.2177734   0.02539062 -0.13220215  0.61376953  0.45239258  0.2376709 ] 3   0 \n",
      "[-0.35302734  0.30029297  0.2709961   0.2697754  -0.31640625 -0.35986328] 1   2 \n",
      "[-0.99365234  0.29711914  0.04660034  0.46777344  0.45703125  0.3984375 ] 3   5 \n",
      "[-1.7822266  -0.02104187 -0.23742676  0.8613281   1.1943359   0.63623047] 4   3 \n",
      "[-1.6103516   0.00264168 -0.17102051  0.7963867   0.68652344  0.66748047] 3   1 \n",
      "[-1.4033203   0.2409668   0.4440918   0.83154297  0.62597656  0.06161499] 3   3 Match 68\n",
      "\n",
      "[-0.3310547   0.3959961   0.45141602  0.5097656  -0.07269287 -0.43896484] 3   0 \n",
      "[-1.6894531   0.15478516  0.0619812   0.81347656  0.82470703  0.5449219 ] 4   3 \n",
      "[-1.4697266   0.13623047  0.26342773  0.96728516  0.54052734  0.3659668 ] 3   3 Match 69\n",
      "\n",
      "[-1.7207031   0.18188477  0.18530273  0.8276367   0.60595703  0.5883789 ] 3   4 \n",
      "[-1.4013672   0.22851562  0.28710938  0.9794922   0.5493164   0.2626953 ] 3   3 Match 70\n",
      "\n",
      "[-0.72558594  0.32836914  0.56933594  0.6113281   0.24291992 -0.20727539] 3   3 Match 71\n",
      "\n",
      "[-0.36621094  0.5341797   0.58740234  0.51220703 -0.13745117 -0.25756836] 2   3 \n",
      "[-1.5205078  -0.00548172  0.08294678  0.9707031   0.87939453  0.25219727] 3   4 \n",
      "[-0.6645508   0.34277344 -0.00862885  0.17102051  0.02090454  0.09680176] 1   4 \n",
      "[-1.6298828   0.26660156 -0.18249512  0.5605469   0.85595703  0.69091797] 4   4 Match 72\n",
      "\n",
      "[-1.1972656   0.34179688  0.24890137  0.7241211   0.23925781  0.22680664] 3   3 Match 73\n",
      "\n",
      "[-0.7783203   0.30273438  0.43676758  0.46069336  0.34423828  0.10681152] 3   2 \n",
      "[-1.0185547   0.4272461   0.40405273  0.625       0.3635254   0.13378906] 3   5 \n",
      "[-0.08148193  0.43774414  0.51416016  0.34765625 -0.18322754 -0.7026367 ] 2   2 Match 74\n",
      "\n",
      "[-0.7446289   0.3618164  -0.15075684  0.07513428  0.24645996  0.4729004 ] 5   4 \n",
      "[-0.75927734  0.43603516  0.5449219   0.58984375  0.21606445 -0.15905762] 3   4 \n",
      "[ 0.05096436  0.63378906  0.4711914   0.06088257 -0.28320312 -0.33544922] 1   2 \n",
      "[-0.19885254  0.51171875  0.03683472 -0.2241211  -0.16442871  0.2548828 ] 1   0 \n",
      "[-8.5644531e-01  5.0292969e-01 -1.0235596e-01  7.6232910e-02\n",
      " -2.0873547e-04  7.4609375e-01] 5   4 \n",
      "[-0.7651367   0.3022461   0.13378906  0.42651367  0.5571289   0.1149292 ] 4   3 \n",
      "[-1.7539062  -0.07244873 -0.30151367  0.74121094  1.0830078   0.8173828 ] 4   4 Match 75\n",
      "\n",
      "[-1.3339844  -0.02679443 -0.2319336   0.5698242   0.76123047  0.7011719 ] 4   2 \n",
      "[-1.4541016   0.04107666 -0.53027344  0.4099121   1.0507812   0.81933594] 4   5 \n",
      "[-1.7382812   0.02516174 -0.5488281   0.5419922   1.1083984   1.0917969 ] 4   1 \n",
      "[-0.9321289   0.17102051 -0.24584961 -0.10876465  0.44482422  0.88378906] 5   5 Match 76\n",
      "\n",
      "[-1.4521484   0.01771545 -0.3310547   0.6640625   0.57470703  0.7270508 ] 5   1 \n",
      "[-1.5322266  -0.02583313 -0.36328125  0.4260254   1.0644531   0.8886719 ] 4   4 Match 77\n",
      "\n",
      "[-1.3759766   0.41015625  0.2800293   0.6333008   0.62597656  0.2536621 ] 3   4 \n",
      "[-1.2998047   0.16870117  0.4741211   0.9790039   0.6791992  -0.03436279] 3   3 Match 78\n",
      "\n",
      "[-1.4560547   0.15844727  0.1763916   0.6660156   0.9580078   0.4506836 ] 4   4 Match 79\n",
      "\n",
      "[-0.34301758  0.47436523  0.74316406  0.56152344 -0.10949707 -0.43359375] 2   2 Match 80\n",
      "\n",
      "[-0.7973633   0.26489258  0.5546875   0.68359375  0.3413086  -0.23999023] 3   0 \n",
      "[-1.4902344   0.25756836  0.19177246  0.86621094  0.765625    0.37060547] 3   1 \n",
      "[-1.3671875   0.00507736  0.06445312  0.8129883   0.7626953   0.13391113] 3   4 \n",
      "[-1.171875    0.06713867 -0.38964844  0.03720093  0.3947754   1.3828125 ] 5   1 \n",
      "[-1.6191406   0.25610352  0.25219727  0.8564453   0.7944336   0.48120117] 3   2 \n",
      "[-0.6435547   0.5131836   0.78271484  0.6933594  -0.01669312 -0.23583984] 2   2 Match 81\n",
      "\n",
      "[ 0.5551758   0.4885254   0.36645508 -0.27001953 -0.57714844 -0.55078125] 0   1 \n",
      "[-1.6757812  0.1149292 -0.4182129  0.5131836  1.0683594  1.0253906] 4   4 Match 82\n",
      "\n",
      "[-0.5151367   0.45874023  0.03930664  0.12072754  0.01314545  0.20495605] 1   1 Match 83\n",
      "\n",
      "[-1.4648438  -0.00352859 -0.32495117  0.5439453   0.9741211   0.71533203] 4   5 \n",
      "[-1.5625     -0.04602051 -0.18249512  0.7421875   0.9272461   0.49365234] 4   4 Match 84\n",
      "\n",
      "[-0.78222656  0.2614746   0.578125    0.62109375 -0.05270386 -0.24572754] 3   3 Match 85\n",
      "\n",
      "[-1.4521484  -0.15881348 -0.04754639  0.7475586   0.828125    0.44067383] 4   2 \n",
      "[-0.6274414   0.37670898  0.37841797  0.5473633   0.2890625  -0.22265625] 3   4 \n",
      "[-1.3896484   0.36499023  0.44140625  0.81640625  0.32592773  0.29541016] 3   4 \n",
      "[-1.7060547   0.17199707 -0.43188477  0.3684082   0.73876953  0.96191406] 5   1 \n",
      "[-1.2705078  0.3317871  0.0309906  0.5498047  0.4260254  0.6767578] 5   4 \n",
      "[ 0.06445312  0.53515625  0.5395508   0.29345703 -0.30493164 -0.70654297] 2   1 \n",
      "[ 0.46118164  0.40063477  0.15991211 -0.34375    -0.8466797  -0.35791016] 0   1 \n",
      "[-1.6542969   0.22790527 -0.34301758  0.49145508  0.83935547  1.0166016 ] 5   4 \n",
      "[-0.37597656  0.3034668   0.6381836   0.33203125 -0.07745361 -0.3623047 ] 2   5 \n",
      "[-1.2617188   0.43164062  0.36572266  0.8071289   0.35961914 -0.02017212] 3   3 Match 86\n",
      "\n",
      "[-1.6816406   0.04772949 -0.53125     0.5336914   1.2197266   1.0830078 ] 4   5 \n",
      "[-1.5820312  -0.04745483 -0.25146484  0.87060547  0.8881836   0.64941406] 4   3 \n",
      "[-0.8286133   0.2290039   0.72509766  0.72314453  0.265625   -0.3149414 ] 2   4 \n",
      "[-0.86865234  0.40161133  0.78222656  0.72509766  0.14538574 -0.10327148] 2   4 \n",
      "[-1.1494141   0.08007812 -0.3779297   0.22790527  0.96435547  0.6640625 ] 4   1 \n",
      "[-1.5332031   0.06433105  0.23400879  0.7060547   0.73535156  0.53759766] 4   1 \n",
      "[-0.75683594  0.34960938  0.07269287  0.3244629   0.0254364   0.37475586] 5   2 \n",
      "[-1.3798828   0.12548828  0.23803711  0.8383789   0.7680664   0.48046875] 3   4 \n",
      "[-0.32739258  0.56689453  0.5097656   0.14733887 -0.14465332 -0.17272949] 1   3 \n",
      "[-0.4494629   0.5571289   0.17138672  0.18713379 -0.29052734 -0.14660645] 1   2 \n",
      "[-1.2460938   0.28393555  0.09210205  0.390625    0.13330078  0.51708984] 5   0 \n",
      "[-1.4355469   0.2746582   0.01087952  0.58251953  0.828125    0.45581055] 4   5 \n",
      "[-1.8046875   0.13781738 -0.05703735  0.71728516  0.8745117   0.7890625 ] 4   1 \n",
      "[-0.9819336   0.32617188  0.5751953   0.6489258   0.18041992  0.01546478] 3   2 \n",
      "[-1.2236328   0.26367188 -0.24536133  0.5366211   0.82666016  0.47631836] 4   1 \n",
      "[-1.5029297  -0.06134033 -0.40161133  0.4428711   0.74121094  1.0341797 ] 5   5 Match 87\n",
      "\n",
      "[-1.3945312   0.359375    0.34057617  0.8671875   0.4807129   0.26391602] 3   3 Match 88\n",
      "\n",
      "[-0.4555664   0.48657227  0.18969727 -0.04891968 -0.01556396  0.27026367] 1   1 Match 89\n",
      "\n",
      "[-1.4394531   0.02787781 -0.01391602  0.5644531   0.6142578   0.37402344] 4   4 Match 90\n",
      "\n",
      "[-0.46069336  0.44384766  0.40649414  0.39672852  0.04598999 -0.2355957 ] 1   4 \n",
      "[ 0.45336914  0.41186523  0.11456299 -0.2734375  -0.5107422  -0.45092773] 0   3 \n",
      "[ 0.2434082   0.6586914   0.46069336 -0.0690918  -0.44580078 -0.47021484] 1   0 \n",
      "[-1.1708984   0.28955078  0.05639648  0.5390625   0.5024414   0.5419922 ] 5   3 \n",
      "[-0.67333984  0.42749023  0.7705078   0.73779297 -0.20727539 -0.4724121 ] 2   2 Match 91\n",
      "\n",
      "[-0.10327148  0.4116211   0.18151855  0.09625244 -0.5292969  -0.21850586] 1   4 \n",
      "[ 0.63134766  0.4880371   0.24816895 -0.34277344 -1.0615234  -0.54345703] 0   0 Match 92\n",
      "\n",
      "[-0.75097656  0.40307617  0.6826172   0.7397461   0.23657227 -0.4326172 ] 3   2 \n",
      "[-0.8154297   0.54541016  0.5449219   0.5527344   0.24621582  0.00240517] 3   2 \n",
      "[ 0.39526367  0.43481445  0.30639648 -0.1673584  -0.75927734 -0.44018555] 1   2 \n",
      "[-1.7783203   0.03359985  0.11578369  0.65771484  0.828125    0.87890625] 5   3 \n",
      "[-0.03887939  0.40112305  0.33081055  0.19128418 -0.5620117  -0.31420898] 1   5 \n",
      "[-1.1425781   0.26342773  0.6171875   0.7631836   0.4091797   0.10198975] 3   2 \n",
      "[-1.5410156  -0.00701904  0.13952637  0.9741211   0.7836914   0.38500977] 3   3 Match 93\n",
      "\n",
      "[-0.40454102  0.56884766  0.66259766  0.5727539   0.08630371 -0.47583008] 2   3 \n",
      "[ 0.0970459   0.6616211   0.49194336 -0.03820801 -0.46240234 -0.35473633] 1   3 \n",
      "[-1.2900391   0.3828125   0.50341797  0.5390625   0.7138672   0.19580078] 4   3 \n",
      "[-1.6044922   0.3317871   0.0645752   0.65771484  0.5957031   0.79296875] 5   1 \n",
      "[-0.11669922  0.4050293   0.03616333 -0.3317871  -0.16821289  0.15527344] 1   3 \n",
      "[-0.9819336   0.2409668  -0.08239746  0.32226562  0.66015625  0.49951172] 4   5 \n",
      "[-0.91845703  0.2619629   0.5229492   0.83447266  0.1998291  -0.28393555] 3   1 \n",
      "[-1.3945312   0.16467285  0.35913086  0.8881836   0.64746094  0.16296387] 3   5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.5078125   0.35742188  0.2734375   0.72021484  0.67285156  0.43359375] 3   1 \n",
      "[-1.6650391  -0.02114868  0.02050781  0.80810547  0.61621094  0.5283203 ] 3   3 Match 94\n",
      "\n",
      "[-1.7226562   0.17590332 -0.10168457  0.46264648  0.8857422   0.7919922 ] 4   4 Match 95\n",
      "\n",
      "[-0.80126953  0.40820312  0.45874023  0.55615234  0.3239746  -0.07055664] 3   2 \n",
      "[-1.1250000e+00  3.2739258e-01  3.7084961e-01  7.7734375e-01\n",
      "  3.9794922e-01 -1.1640787e-04] 3   4 \n",
      "[-0.10791016  0.69921875  0.66845703  0.4074707  -0.41235352 -0.52441406] 1   1 Match 96\n",
      "\n",
      "[-1.5078125  -0.0385437  -0.4182129   0.5366211   1.2714844   0.78808594] 4   4 Match 97\n",
      "\n",
      "[ 0.09265137  0.56152344  0.42358398  0.18408203 -0.16833496 -0.61572266] 1   1 Match 98\n",
      "\n",
      "[-1.1201172   0.3918457   0.36669922  0.51416016  0.3166504   0.15014648] 3   2 \n",
      "[-0.5395508   0.31323242  0.27563477  0.44458008  0.16003418 -0.20422363] 3   0 \n",
      "[-1.0546875   0.13195801  0.09515381  0.58447266  0.6381836   0.20263672] 4   5 \n",
      "[-1.0683594   0.18041992 -0.27416992  0.24182129  0.75927734  0.5703125 ] 4   2 \n",
      "[ 0.63964844  0.43701172  0.1973877  -0.34545898 -0.7910156  -0.57373047] 0   3 \n",
      "[-0.2388916   0.41015625  0.28588867  0.13952637 -0.20776367 -0.06744385] 1   0 \n",
      "[-0.38891602  0.25219727 -0.13098145  0.27319336  0.15966797  0.19812012] 3   3 Match 99\n",
      "\n",
      "[-0.25390625  0.39990234 -0.234375   -0.11676025  0.03967285  0.45581055] 5   5 Match 100\n",
      "\n",
      "[ 0.72558594  0.40112305  0.23339844 -0.40966797 -0.7998047  -0.4951172 ] 0   5 \n",
      "[-0.5732422   0.3239746   0.08007812  0.29833984  0.07879639 -0.14489746] 1   2 \n",
      "[ 0.08349609  0.43286133  0.24511719  0.0880127  -0.47558594 -0.3466797 ] 1   2 \n",
      "[-0.5932617   0.4958496  -0.03125     0.18066406  0.2919922   0.15197754] 1   1 Match 101\n",
      "\n",
      "[-0.5444336   0.40771484  0.19433594  0.328125    0.18029785 -0.02281189] 1   2 \n",
      "[ 0.13244629  0.34570312  0.18676758 -0.15319824 -0.55029297 -0.32885742] 1   2 \n",
      "[-1.3613281   0.10986328 -0.01699829  0.7919922   0.9248047   0.44018555] 4   3 \n",
      "[-1.4277344 -0.1348877 -0.2685547  0.6098633  0.6254883  0.4025879] 4   3 \n",
      "[ 0.01024628  0.39257812  0.03503418 -0.0892334  -0.33007812 -0.19958496] 1   3 \n",
      "[-7.4923038e-05  5.2050781e-01  4.5874023e-01  1.5869141e-01\n",
      " -4.9121094e-01 -5.2490234e-01] 1   3 \n",
      "[-0.15698242  0.4194336   0.44873047  0.27294922 -0.06066895 -0.31030273] 2   2 Match 102\n",
      "\n",
      "[-0.95996094  0.05810547 -0.16796875  0.5864258   0.5932617   0.15893555] 4   0 \n",
      "[-0.6245117   0.43041992  0.4645996   0.8198242   0.05633545 -0.4428711 ] 3   3 Match 103\n",
      "\n",
      "[ 0.08843994  0.44580078 -0.00088739 -0.03640747 -0.24182129 -0.17834473] 1   2 \n",
      "[-0.76904297  0.23657227  0.40698242  0.71240234  0.49316406 -0.3413086 ] 3   4 \n",
      "[-0.9926758   0.09570312  0.3647461   0.8095703   0.34716797 -0.29833984] 3   4 \n",
      "[-0.00585556  0.50927734  0.5629883   0.3696289  -0.14001465 -0.72802734] 2   1 \n",
      "[ 0.51464844  0.39257812  0.33813477 -0.23522949 -0.9536133  -0.5234375 ] 0   2 \n",
      "[-1.4609375   0.1352539   0.03158569  0.5859375   0.9135742   0.81884766] 4   0 \n",
      "[ 0.484375    0.3088379  -0.01846313 -0.3317871  -0.7314453  -0.41723633] 0   5 \n",
      "[-0.58740234  0.42333984  0.25927734  0.31469727 -0.18164062 -0.06149292] 1   4 \n",
      "[-0.8305664   0.3269043   0.15698242  0.69384766  0.62841797 -0.06604004] 3   4 \n",
      "[-1.671875    0.10717773 -0.20300293  0.66259766  0.9379883   0.73828125] 4   2 \n",
      "[ 0.11029053  0.3305664   0.01612854 -0.19165039 -0.2220459  -0.09320068] 1   1 Match 104\n",
      "\n",
      "[-1.5634766  -0.09289551 -0.5649414   0.43798828  1.2167969   0.9116211 ] 4   5 \n",
      "[-1.7099609  -0.08843994 -0.3071289   0.62646484  1.1855469   0.7973633 ] 4   1 \n",
      "[-1.6923828  -0.11706543 -0.24719238  0.8964844   1.1933594   0.7753906 ] 4   4 Match 105\n",
      "\n",
      "[-1.3925781   0.17053223 -0.18823242  0.57128906  1.0117188   0.65527344] 4   3 \n",
      "[-0.45947266  0.6176758   0.21862793  0.03192139 -0.14831543  0.12854004] 1   1 Match 106\n",
      "\n",
      "[-0.4501953   0.30664062  0.39111328  0.43041992 -0.18469238 -0.23376465] 3   3 Match 107\n",
      "\n",
      "[-0.14855957  0.43969727  0.13391113  0.00370407 -0.1295166   0.04721069] 1   3 \n",
      "[-0.85546875  0.34838867  0.3515625   0.6225586   0.35083008  0.06640625] 3   2 \n",
      "[-0.10772705  0.42407227  0.5751953   0.25756836 -0.49536133 -0.5097656 ] 2   1 \n",
      "[-0.96875     0.21057129  0.35986328  0.64208984  0.484375   -0.02650452] 3   2 \n",
      "[-0.4013672   0.57177734  0.38964844  0.37524414 -0.18188477 -0.17810059] 1   1 Match 108\n",
      "\n",
      "[-1.5449219   0.09429932 -0.45288086  0.27954102  0.81933594  1.2636719 ] 5   5 Match 109\n",
      "\n",
      "[-0.36767578  0.5576172   0.6303711   0.42504883 -0.08154297 -0.40429688] 2   2 Match 110\n",
      "\n",
      "[-0.78808594  0.4086914   0.11431885  0.32226562  0.07659912  0.25073242] 1   2 \n",
      "[ 0.25976562  0.5253906   0.37963867  0.01785278 -0.7265625  -0.7270508 ] 1   2 \n",
      "[-1.4345703  -0.09161377 -0.54296875  0.5834961   1.1875      0.91845703] 4   4 Match 111\n",
      "\n",
      "[-0.6040039   0.4116211   0.4284668   0.59521484  0.05865479 -0.08221436] 3   1 \n",
      "[-0.9082031   0.07611084  0.40429688  0.63427734 -0.09906006 -0.11175537] 3   4 \n",
      "[ 0.40576172  0.61816406  0.30932617 -0.28955078 -0.7089844  -0.55322266] 1   3 \n",
      "[-1.2822266   0.18774414 -0.39794922  0.29101562  0.6142578   0.9902344 ] 5   5 Match 112\n",
      "\n",
      "[-1.1962891  -0.0581665  -0.27734375  0.55126953  0.96435547  0.37817383] 4   3 \n",
      "[-1.2558594   0.4267578   0.4255371   0.54785156  0.3671875   0.39892578] 3   2 \n",
      "[-1.3369141   0.10943604  0.18225098  0.9326172   0.56689453 -0.01526642] 3   2 \n",
      "[-0.9692383   0.0814209   0.08569336  0.47265625  0.72998047  0.17163086] 4   1 \n",
      "[-1.0966797e+00  2.5561523e-01  3.3691406e-01  8.4082031e-01\n",
      "  3.0664062e-01 -4.7135353e-04] 3   1 \n",
      "[-1.4189453   0.42260742  0.37182617  0.8286133   0.52783203  0.2680664 ] 3   5 \n",
      "[-1.3339844   0.23742676  0.18786621  0.8251953   0.80322266  0.24169922] 3   3 Match 113\n",
      "\n",
      "[ 0.07098389  0.5493164   0.24035645  0.00932312 -0.6269531  -0.49243164] 1   2 \n",
      "[ 0.33081055  0.5253906   0.3203125  -0.00889587 -0.61816406 -0.46191406] 1   0 \n",
      "[-1.7148438   0.14941406  0.02723694  0.9160156   0.91015625  0.5854492 ] 3   3 Match 114\n",
      "\n",
      "[-1.2597656   0.32788086  0.4814453   0.63916016  0.4008789   0.20959473] 3   2 \n",
      "[-1.4482422   0.09259033  0.0044899   0.9057617   0.73779297  0.45532227] 3   1 \n",
      "[-1.6816406  -0.03833008 -0.2602539   0.69433594  1.1904297   0.7583008 ] 4   2 \n",
      "[-0.5229492   0.43945312  0.71777344  0.71240234  0.12384033 -0.47192383] 2   3 \n",
      "[-0.3317871   0.5048828   0.3359375   0.14318848 -0.41210938 -0.19152832] 1   3 \n",
      "[-1.5722656   0.04962158 -0.22802734  0.54541016  0.95703125  0.73291016] 4   2 \n",
      "[-1.0771484   0.1953125  -0.03106689  0.45898438  0.5473633   0.60302734] 5   4 \n",
      "[-0.90478516  0.21594238  0.21240234  0.4345703   0.17834473  0.34301758] 3   1 \n",
      "[-0.52734375  0.38012695  0.28515625  0.54345703 -0.04574585 -0.39941406] 3   3 Match 115\n",
      "\n",
      "[-1.5927734  -0.04528809 -0.39379883  0.7788086   1.0839844   0.75634766] 4   3 \n",
      "[-0.63720703  0.27685547 -0.14685059  0.23950195  0.33032227  0.4868164 ] 5   1 \n",
      "[-1.1328125   0.17297363  0.24072266  0.8041992   0.3413086   0.16564941] 3   1 \n",
      "[ 0.01263428  0.2932129   0.34643555  0.3269043  -0.08294678 -0.6245117 ] 2   0 \n",
      "[ 0.6064453   0.30322266  0.16418457 -0.3942871  -0.84277344 -0.18432617] 0   1 \n",
      "[-0.5175781   0.40063477 -0.03427124  0.11517334  0.25610352  0.2734375 ] 1   0 \n",
      "[-1.1259766  -0.04797363 -0.22387695  0.4169922   0.4880371   0.4951172 ] 5   0 \n",
      "[-0.17102051  0.46411133 -0.03890991 -0.19519043 -0.19018555  0.21105957] 1   2 \n",
      "[-1.5263672   0.08514404  0.30517578  0.9658203   0.7348633   0.3647461 ] 3   0 \n",
      "[-0.62646484  0.31201172  0.06085205  0.19238281  0.00299835  0.10101318] 1   0 \n",
      "[-0.47753906  0.3828125   0.00672531 -0.06027222  0.04595947  0.13110352] 1   1 Match 116\n",
      "\n",
      "[-1.6367188   0.01079559 -0.05148315  0.8198242   1.0615234   0.5390625 ] 4   1 \n",
      "[-1.3691406  0.1730957 -0.5253906  0.2824707  0.9711914  1.0615234] 5   5 Match 117\n",
      "\n",
      "[-0.9506836   0.05407715  0.2088623   0.66308594  0.32299805  0.25146484] 3   3 Match 118\n",
      "\n",
      "[-1.1435547   0.07391357  0.02442932  0.58740234  0.57910156  0.30371094] 3   3 Match 119\n",
      "\n",
      "[-0.41870117  0.3166504   0.7167969   0.56347656  0.29736328 -0.46411133] 2   2 Match 120\n",
      "\n",
      "[-0.29711914  0.6513672   0.6923828   0.45629883 -0.14941406 -0.3779297 ] 2   2 Match 121\n",
      "\n",
      "[-1.1552734   0.24108887 -0.13586426  0.47143555  0.62402344  0.7060547 ] 5   4 \n",
      "[-0.27392578  0.42773438  0.55908203  0.3425293  -0.10913086 -0.18383789] 2   2 Match 122\n",
      "\n",
      "[-0.7583008   0.41259766 -0.07476807  0.26635742  0.4633789   0.29345703] 4   2 \n",
      "[-1.3769531   0.4416504   0.32910156  0.8388672   0.4411621   0.25854492] 3   5 \n",
      "[-0.09643555  0.43017578  0.30371094  0.23266602 -0.50146484 -0.41357422] 1   1 Match 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-0.30126953  0.42749023  0.27075195  0.20056152 -0.12792969 -0.04125977] 1   1 Match 124\n",
      "\n",
      "[-1.2929688   0.12524414  0.24963379  0.5625      0.43115234  0.21557617] 3   2 \n",
      "[-1.1376953   0.09204102  0.06130981  0.6557617   0.6699219   0.3395996 ] 4   3 \n",
      "[ 0.19421387  0.35302734  0.45336914  0.08807373 -0.61816406 -0.5810547 ] 2   0 \n",
      "[-1.7685547   0.11779785  0.06347656  0.7758789   1.0732422   0.43139648] 4   5 \n",
      "[ 0.20690918  0.5136719   0.24523926 -0.14819336 -0.2788086  -0.26733398] 1   4 \n",
      "[-0.41577148  0.5419922   0.6621094   0.625      -0.16223145 -0.30517578] 2   3 \n",
      "[-1.3408203   0.3022461  -0.00556564  0.49951172  1.0410156   0.49804688] 4   5 \n",
      "[-1.1035156   0.3215332   0.44482422  0.6621094   0.53125     0.22583008] 3   4 \n",
      "[-1.2041016  -0.09594727 -0.32104492  0.4855957   0.97753906  0.58740234] 4   5 \n",
      "[-0.68359375  0.3322754   0.6748047   0.640625    0.16723633 -0.34472656] 2   0 \n",
      "[-1.1308594   0.16674805  0.30981445  0.83496094  0.32250977  0.12347412] 3   4 \n",
      "[-1.6171875  -0.06585693 -0.28344727  0.7636719   1.1132812   0.54541016] 4   5 \n",
      "[-0.3461914   0.31396484  0.02786255  0.12359619  0.00646591  0.08892822] 1   4 \n",
      "[-0.5126953   0.44433594  0.44604492  0.18725586  0.15039062  0.03762817] 2   3 \n",
      "[-0.10571289  0.43237305  0.00483704 -0.13122559 -0.5019531   0.19458008] 1   2 \n",
      "[-1.4794922   0.12036133  0.04293823  0.7182617   0.6635742   0.3659668 ] 3   4 \n",
      "[-0.76904297  0.30664062  0.63134766  0.7998047   0.32983398 -0.28588867] 3   2 \n",
      "[-1.4121094   0.22717285  0.5083008   1.0449219   0.6538086   0.1328125 ] 3   2 \n",
      "[-1.5214844   0.04412842 -0.24865723  0.36132812  0.8671875   0.9121094 ] 5   5 Match 125\n",
      "\n",
      "[-1.5869141  -0.04718018 -0.19262695  0.8125      1.1181641   0.7011719 ] 4   4 Match 126\n",
      "\n",
      "[ 0.5551758   0.5307617   0.38842773 -0.33862305 -0.87402344 -0.4621582 ] 0   3 \n",
      "[-0.08605957  0.5258789   0.26293945  0.09918213 -0.48657227 -0.22949219] 1   1 Match 127\n",
      "\n",
      "[ 0.04705811  0.32836914 -0.08410645 -0.08477783 -0.46264648 -0.15222168] 1   2 \n",
      "[-1.546875    0.13574219  0.1697998   0.7392578   0.6879883   0.671875  ] 3   3 Match 128\n",
      "\n",
      "[-1.3125      0.25854492 -0.1038208   0.6123047   0.44555664  0.5102539 ] 3   3 Match 129\n",
      "\n",
      "[-2.0351562  -0.00227356 -0.4345703   0.55078125  1.0664062   1.0957031 ] 5   3 \n",
      "[-1.6572266   0.1616211  -0.12524414  0.62646484  0.80615234  0.91015625] 5   4 \n",
      "[-1.5068359   0.19812012 -0.03878784  0.671875    0.5654297   0.6245117 ] 3   5 \n",
      "[-0.69970703  0.3864746   0.04647827  0.08776855  0.28344727  0.38452148] 1   2 \n",
      "[-0.5024414   0.53564453  0.61572266  0.5957031  -0.1373291  -0.2705078 ] 2   0 \n",
      "[-1.6455078   0.27075195 -0.08660889  0.64453125  1.0800781   0.47143555] 4   1 \n",
      "[-1.7617188  -0.0579834  -0.38208008  0.72998047  1.3730469   0.84277344] 4   2 \n",
      "[-1.6064453   0.10858154  0.13110352  0.92041016  0.6904297   0.44213867] 3   2 \n",
      "[ 0.30493164  0.5473633   0.24169922 -0.22229004 -0.7192383  -0.48291016] 1   0 \n",
      "[-0.21032715  0.38549805 -0.04089355  0.05905151 -0.42211914  0.04495239] 1   0 \n",
      "[-0.15649414  0.45214844  0.4267578   0.23461914 -0.2619629  -0.24853516] 1   1 Match 130\n",
      "\n",
      "[-1.1552734   0.06182861 -0.02026367  0.484375    0.609375    0.2376709 ] 4   5 \n",
      "[-1.2441406   0.00617218 -0.18823242  0.60595703  0.82666016  0.48388672] 4   5 \n",
      "[-1.8945312   0.02597046 -0.23071289  0.82421875  1.0488281   0.83154297] 4   5 \n",
      "[-0.76953125  0.32470703  0.2163086   0.32226562 -0.16931152  0.29101562] 1   2 \n",
      "[ 0.09814453  0.49121094  0.19006348 -0.04470825 -0.4814453  -0.4638672 ] 1   3 \n",
      "[-0.70703125  0.44384766  0.3671875   0.3984375   0.24536133  0.03552246] 1   3 \n",
      "[-1.5214844   0.17114258  0.35253906  0.88427734  0.7089844   0.26171875] 3   4 \n",
      "[-1.6357422  -0.08605957 -0.0451355   0.6582031   0.7890625   1.0566406 ] 5   4 \n",
      "[-1.0058594   0.27270508  0.02720642  0.46655273  0.36767578  0.6347656 ] 5   3 \n",
      "[-0.98291016  0.45336914  0.36108398  0.61865234  0.2536621   0.05404663] 3   5 \n",
      "[ 0.07928467  0.50341797  0.17077637 -0.27612305 -0.61035156 -0.07452393] 1   2 \n",
      "[-1.1875      0.00504684 -0.1463623   0.2055664   0.7324219   0.6958008 ] 4   3 \n",
      "[-1.4873047  -0.09088135 -0.05636597  0.8984375   1.0146484   0.5366211 ] 4   4 Match 131\n",
      "\n",
      "[-0.31298828  0.37597656  0.43115234  0.5161133   0.03942871 -0.44482422] 3   2 \n",
      "[ 0.06158447  0.4345703   0.1538086  -0.11712646 -0.21508789 -0.02168274] 1   5 \n",
      "[-0.4013672   0.4104004   0.31445312  0.38256836 -0.33496094 -0.2512207 ] 1   5 \n",
      "[-1.1816406   0.16894531 -0.15344238  0.40551758  0.9375      0.5332031 ] 4   2 \n",
      "[-0.7734375   0.5566406   0.24401855  0.45239258  0.20507812  0.15771484] 1   3 \n",
      "[-1.0283203   0.09777832  0.2064209   0.609375    0.35205078  0.1907959 ] 3   1 \n",
      "[-1.625      -0.02049255 -0.18713379  0.80615234  1.1806641   0.5917969 ] 4   5 \n",
      "[-1.2841797   0.2993164   0.32958984  0.6557617   0.11462402  0.16918945] 3   3 Match 132\n",
      "\n",
      "[-1.5830078  -0.03735352  0.15136719  1.0058594   0.81347656  0.4423828 ] 3   4 \n",
      "[-0.6123047   0.62890625  0.21130371  0.24536133  0.03689575  0.05212402] 1   2 \n",
      "[-0.92285156  0.06744385 -0.15515137  0.10424805  0.56591797  0.85546875] 5   4 \n",
      "[ 0.02478027  0.46655273  0.4909668   0.34277344 -0.15979004 -0.6591797 ] 2   2 Match 133\n",
      "\n",
      "[-1.9082031  -0.06195068 -0.29296875  0.76171875  1.1240234   0.95751953] 4   5 \n",
      "[-1.7792969   0.08953857 -0.48242188  0.50146484  1.3496094   1.0673828 ] 4   3 \n",
      "[ 0.16271973  0.40112305  0.10949707 -0.26635742 -0.4194336  -0.14489746] 1   1 Match 134\n",
      "\n",
      "[ 0.21289062  0.50634766  0.18395996 -0.04397583 -0.32788086 -0.39746094] 1   3 \n",
      "[-0.17749023  0.4567871   0.32495117  0.22814941 -0.09112549 -0.31103516] 1   4 \n",
      "[-1.1523438   0.14489746 -0.2109375   0.5629883   0.8100586   0.36108398] 4   5 \n",
      "[-1.8105469  -0.08172607 -0.23327637  0.84228516  1.1923828   0.70214844] 4   3 \n",
      "[ 0.11480713  0.4411621   0.2758789  -0.12237549 -0.33422852 -0.15771484] 1   5 \n",
      "[-1.0322266   0.26245117  0.34594727  0.6069336   0.34350586  0.17797852] 3   3 Match 135\n",
      "\n",
      "[-1.0820312   0.27929688  0.2536621   0.50097656  0.5439453   0.36791992] 4   5 \n",
      "[-0.9555664   0.3605957  -0.19592285  0.12457275  0.10302734  0.8955078 ] 5   4 \n",
      "[-0.64160156  0.38208008  0.41552734  0.50097656  0.23608398  0.03561401] 3   3 Match 136\n",
      "\n",
      "[-1.3144531   0.2512207   0.1607666   0.60546875  0.4050293   0.67529297] 5   5 Match 137\n",
      "\n",
      "[-0.8520508   0.08251953  0.12133789  0.26464844  0.1817627   0.2697754 ] 5   3 \n",
      "[ 0.0440979   0.63623047  0.49389648  0.24560547 -0.40283203 -0.6660156 ] 1   2 \n",
      "[-1.1054688   0.31982422  0.0769043   0.6298828   0.4506836   0.2932129 ] 3   2 \n",
      "[-0.9194336   0.35742188  0.09448242  0.43725586  0.24584961  0.2980957 ] 3   2 \n",
      "[-1.0800781   0.38427734  0.41870117  0.67578125  0.37939453  0.23828125] 3   1 \n",
      "[-1.4619141  -0.03015137  0.24487305  0.74902344  0.59228516  0.4375    ] 3   4 \n",
      "[ 0.13110352  0.5419922   0.28515625 -0.09588623 -0.5307617  -0.3515625 ] 1   2 \n",
      "[-1.7832031   0.05844116 -0.12536621  0.6533203   0.9165039   0.85498047] 4   1 \n",
      "[-1.3056641   0.2980957  -0.19824219  0.39257812  0.79296875  0.7446289 ] 4   5 \n",
      "[-1.6318359   0.17468262 -0.2409668   0.6201172   0.9350586   0.9145508 ] 4   3 \n",
      "[-0.94384766  0.3413086   0.5336914   0.8300781   0.12988281 -0.34765625] 3   3 Match 138\n",
      "\n",
      "[-1.2353516   0.24682617 -0.08837891  0.42236328  0.671875    0.60253906] 4   2 \n",
      "[-1.3056641  -0.15014648 -0.48632812  0.53222656  0.89941406  0.65625   ] 4   3 \n",
      "[-1.2324219   0.2692871  -0.09790039  0.38745117  0.25390625  0.73779297] 5   5 Match 139\n",
      "\n",
      "[-0.8930664   0.21923828  0.18347168  0.4675293   0.10919189  0.32104492] 3   4 \n",
      "[-0.9433594   0.28808594  0.31347656  0.6982422   0.22766113 -0.03198242] 3   5 \n",
      "[-1.5878906   0.08703613 -0.06210327  0.81640625  0.8051758   0.5810547 ] 3   2 \n",
      "[-0.42822266  0.5107422   0.5527344   0.53759766  0.05148315 -0.47753906] 2   5 \n",
      "[-0.01618958  0.63720703  0.5859375   0.36938477 -0.14892578 -0.56347656] 1   3 \n",
      "[-1.1445312   0.33569336  0.33911133  0.6201172   0.64746094  0.22302246] 4   4 Match 140\n",
      "\n",
      "[ 0.0401001   0.3359375   0.2854004   0.21862793 -0.01138306 -0.4411621 ] 1   3 \n",
      "[-0.0079422   0.3605957   0.08111572 -0.24182129 -0.25830078  0.05203247] 1   2 \n",
      "[ 0.09979248  0.54589844  0.4423828   0.22253418 -0.15478516 -0.7368164 ] 1   5 \n",
      "[-1.0878906   0.34033203  0.25976562  0.64501953  0.43310547  0.07867432] 3   3 Match 141\n",
      "\n",
      "[-1.5332031   0.3269043   0.10644531  0.62597656  0.5917969   0.76464844] 5   3 \n",
      "[-0.9941406   0.23547363  0.40527344  0.53222656  0.35986328 -0.16845703] 3   2 \n",
      "[-0.81396484  0.35205078 -0.03234863  0.33666992  0.3395996   0.32788086] 1   0 \n",
      "[-0.07330322  0.5258789   0.49853516  0.20239258 -0.43676758 -0.4711914 ] 1   4 \n",
      "[-0.38256836  0.47045898  0.63378906  0.60302734 -0.00909424 -0.30541992] 2   4 \n",
      "[-1.2724609   0.36645508  0.10577393  0.45825195  0.65966797  0.2919922 ] 4   4 Match 142\n",
      "\n",
      "[-0.9536133   0.20288086 -0.34521484  0.10632324  0.44873047  1.0351562 ] 5   1 \n",
      "[-1.7470703  -0.04406738 -0.12792969  0.6152344   0.8935547   0.97314453] 5   3 \n",
      "[-0.89160156  0.3474121   0.40307617  0.61816406  0.37719727  0.3046875 ] 3   1 \n",
      "[-0.79248047  0.28076172 -0.25708008 -0.04077148  0.6323242   0.60253906] 4   5 \n",
      "[-1.7285156   0.19543457  0.18103027  0.95166016  0.70410156  0.41552734] 3   3 Match 143\n",
      "\n",
      "[-0.9536133   0.44848633 -0.17993164  0.3557129   0.76660156  0.34155273] 4   1 \n",
      "[-0.89160156  0.09039307  0.21740723  0.5986328   0.18383789  0.02888489] 3   2 \n",
      "[-1.5449219   0.1907959   0.39575195  0.7011719   0.6147461   0.62158203] 3   5 \n",
      "[-1.2431641   0.46166992  0.34521484  0.51708984  0.51660156  0.22839355] 3   3 Match 144\n",
      "\n",
      "[-1.6533203   0.27368164 -0.44970703  0.5678711   1.0439453   0.9379883 ] 4   5 \n",
      "[-1.9570312  -0.1217041  -0.5449219   0.74658203  1.1816406   1.2158203 ] 5   5 Match 145\n",
      "\n",
      "[-1.7783203  -0.01954651 -0.3239746   0.73583984  0.80126953  1.        ] 5   5 Match 146\n",
      "\n",
      "[-0.97753906  0.37231445  0.22167969  0.5073242   0.11694336  0.28955078] 3   1 \n",
      "[-1.2109375   0.23291016  0.09783936  0.2578125   0.296875    0.70654297] 5   4 \n",
      "[-1.3242188   0.31958008  0.5336914   0.7861328   0.5625      0.28588867] 3   1 \n",
      "[-1.9599609   0.10986328 -0.32739258  0.6953125   1.1601562   1.0400391 ] 4   4 Match 147\n",
      "\n",
      "[-1.3837891   0.25805664 -0.05691528  0.56591797  0.79003906  0.6123047 ] 4   5 \n",
      "[-1.8935547  -0.10449219 -0.12768555  0.953125    1.0976562   0.7685547 ] 4   2 \n",
      "[-1.6298828   0.19091797  0.01641846  0.7373047   0.98779297  0.7216797 ] 4   4 Match 148\n",
      "\n",
      "[-0.80810547  0.25        0.5834961   0.52978516  0.17016602 -0.0319519 ] 2   4 \n",
      "[-1.0927734   0.1887207  -0.27783203  0.4958496   0.49365234  0.48583984] 3   5 \n",
      "[-1.1113281  -0.00411224 -0.2631836   0.44604492  0.57177734  0.6411133 ] 5   4 \n",
      "[-1.3984375  -0.19543457 -0.26489258  0.6640625   1.1816406   0.51904297] 4   4 Match 149\n",
      "\n",
      "[-1.2402344   0.22998047  0.49438477  0.85302734  0.26489258  0.04611206] 3   2 \n",
      "[-1.5722656   0.0189209   0.11584473  1.0009766   0.8666992   0.515625  ] 3   4 \n",
      "[-1.7470703  -0.10217285 -0.12030029  0.7392578   1.1025391   0.9375    ] 4   4 Match 150\n",
      "\n",
      "[-1.7285156   0.18371582  0.10858154  0.8017578   0.8305664   0.69433594] 4   3 \n",
      "[-0.5029297   0.26489258 -0.01205444  0.10015869  0.06842041  0.21044922] 1   4 \n",
      "[-0.02198792  0.48999023  0.27319336  0.02833557 -0.16491699 -0.1829834 ] 1   4 \n",
      "[-1.5927734   0.14282227  0.33398438  0.83447266  0.59521484  0.3083496 ] 3   3 Match 151\n",
      "\n",
      "[-1.8330078   0.11914062  0.12841797  0.8979492   0.74072266  0.61621094] 3   1 \n",
      "[-1.3427734   0.01144409 -0.0026474   0.65234375  0.8769531   0.32836914] 4   1 \n",
      "[-1.8046875  -0.14794922 -0.36254883  0.7451172   1.125       0.7871094 ] 4   4 Match 152\n",
      "\n",
      "[-1.2490234   0.12817383  0.09448242  0.6616211   0.78808594  0.38916016] 4   4 Match 153\n",
      "\n",
      "[-0.7055664   0.4033203   0.2668457   0.26953125  0.11535645  0.13867188] 1   1 Match 154\n",
      "\n",
      "[ 0.1126709   0.39575195  0.20300293 -0.27929688 -0.42358398 -0.04812622] 1   1 Match 155\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8486328   0.18835449 -0.07537842  0.85595703  0.7373047   0.6586914 ] 3   4 \n",
      "[-1.0927734   0.16882324  0.15930176  0.6694336   0.32348633  0.20275879] 3   4 \n",
      "[ 0.11456299  0.5180664   0.28320312  0.05020142 -0.3046875  -0.47192383] 1   0 \n",
      "[ 0.7036133   0.28710938  0.11364746 -0.4555664  -0.9790039  -0.54003906] 0   0 Match 156\n",
      "\n",
      "[ 0.44848633  0.51416016  0.31176758 -0.19836426 -0.58251953 -0.8652344 ] 1   2 \n",
      "[-1.5322266   0.08099365 -0.01353455  0.78808594  0.80810547  0.52001953] 4   1 \n",
      "[-1.5654297   0.29638672  0.08947754  0.71435547  0.6484375   0.5058594 ] 3   1 \n",
      "[-1.5878906   0.15026855  0.39648438  0.70166016  0.5810547   0.5957031 ] 3   4 \n",
      "[-1.5722656   0.09350586  0.05023193  0.75097656  1.0361328   0.57666016] 4   3 \n",
      "[-0.8803711   0.48754883  0.14221191  0.24987793  0.27807617  0.30786133] 1   5 \n",
      "[-1.7392578  -0.11499023 -0.37963867  0.77978516  1.2490234   0.8144531 ] 4   4 Match 157\n",
      "\n",
      "[-0.4194336   0.375      -0.19702148 -0.28100586 -0.02104187  0.5751953 ] 5   1 \n",
      "[-1.1787109   0.17175293  0.5571289   0.7915039   0.50390625  0.09771729] 3   1 \n",
      "[-0.74902344  0.23522949 -0.3203125  -0.05923462  0.4375      0.6879883 ] 5   5 Match 158\n",
      "\n",
      "[-0.49731445  0.6196289   0.23657227  0.01638794 -0.13305664  0.25927734] 1   1 Match 159\n",
      "\n",
      "[-1.5751953   0.0871582   0.14587402  0.85839844  1.0224609   0.5385742 ] 4   5 \n",
      "[-1.0537109   0.23217773 -0.27075195  0.13110352  0.6386719   0.8745117 ] 5   4 \n",
      "[-0.9628906   0.38110352  0.15844727  0.5205078   0.3474121   0.35083008] 3   5 \n",
      "[-1.5722656e+00 -1.4352798e-03 -4.6240234e-01  3.4228516e-01\n",
      "  7.6367188e-01  1.4121094e+00] 5   5 Match 160\n",
      "\n",
      "[ 0.06170654  0.5292969   0.41625977  0.17199707 -0.66796875 -0.5839844 ] 1   4 \n",
      "[-1.3173828   0.16333008 -0.19555664  0.37426758  0.55078125  0.66308594] 5   4 \n",
      "[ 0.15930176  0.51904297  0.090271   -0.09967041 -0.29663086 -0.15600586] 1   1 Match 161\n",
      "\n",
      "[ 0.11901855  0.55029297  0.11804199 -0.19238281 -0.48095703 -0.24267578] 1   4 \n",
      "[-0.7416992   0.50341797  0.40673828  0.5083008   0.14196777  0.31567383] 3   2 \n",
      "[-1.4882812   0.1282959   0.23352051  0.6591797   0.90771484  0.5932617 ] 4   3 \n",
      "[-1.4970703   0.09014893 -0.39111328  0.42895508  1.0322266   1.0947266 ] 5   4 \n",
      "[-1.4208984   0.04989624  0.17871094  0.8701172   0.6894531   0.26635742] 3   4 \n",
      "[-1.1992188   0.16137695 -0.03710938  0.55371094  0.89941406  0.35766602] 4   4 Match 162\n",
      "\n",
      "[-1.1865234   0.34887695 -0.29125977  0.4333496   0.67529297  0.765625  ] 5   1 \n",
      "[-1.6318359   0.02258301 -0.40014648  0.6503906   1.3232422   0.78759766] 4   4 Match 163\n",
      "\n",
      "[-1.3291016   0.20874023  0.20422363  0.60791016  0.37646484  0.30249023] 3   3 Match 164\n",
      "\n",
      "[-2.3205566e-01  4.4506836e-01  1.3351440e-04 -6.9946289e-02\n",
      " -1.7871094e-01  5.9844971e-02] 1   5 \n",
      "[-1.2900391   0.16113281  0.17126465  0.6791992   0.42138672  0.42236328] 3   1 \n",
      "[-0.9453125   0.32788086  0.11938477  0.2680664   0.29101562  0.3947754 ] 5   3 \n",
      "[-0.16296387  0.42822266  0.09094238 -0.07336426 -0.46191406  0.2775879 ] 1   3 \n",
      "[-0.6928711   0.41137695  0.28027344  0.1895752   0.19763184  0.1887207 ] 1   5 \n",
      "[ 0.21350098  0.6665039   0.44067383  0.1517334  -0.2697754  -0.6660156 ] 1   2 \n",
      "[-0.2097168   0.5551758   0.7236328   0.48608398 -0.26171875 -0.5214844 ] 2   1 \n",
      "[-1.6708984   0.12225342 -0.2626953   0.32666016  0.91308594  1.1503906 ] 5   5 Match 165\n",
      "\n",
      "[-0.6826172   0.5180664   0.47558594  0.55078125 -0.17443848 -0.29345703] 3   1 \n",
      "[-0.5776367   0.39501953  0.23632812  0.31982422 -0.0296936   0.11309814] 1   5 \n",
      "[-1.3671875   0.26489258  0.4975586   0.8017578   0.640625    0.13647461] 3   3 Match 166\n",
      "\n",
      "[ 0.10400391  0.62353516  0.3544922   0.24072266 -0.5522461  -0.43139648] 1   1 Match 167\n",
      "\n",
      "[-0.0647583   0.6459961   0.20129395 -0.0916748  -0.5625     -0.17797852] 1   2 \n",
      "[-1.703125    0.09515381 -0.09771729  0.8330078   1.015625    0.5722656 ] 4   1 \n",
      "[-0.46972656  0.4814453   0.38012695  0.44555664  0.2019043  -0.16369629] 1   3 \n",
      "[-0.98535156  0.2854004  -0.19311523  0.10510254  0.54052734  0.71728516] 5   1 \n",
      "[-0.88720703  0.15283203 -0.04626465  0.27905273 -0.01506805  0.45874023] 5   5 Match 168\n",
      "\n",
      "[-1.2207031   0.08483887  0.19128418  0.6958008   0.67626953  0.23461914] 3   5 \n",
      "[-1.1806641   0.27661133  0.00828552  0.59814453  0.29638672  0.44628906] 3   1 \n",
      "[-1.5498047   0.08416748 -0.01274109  0.7915039   0.90478516  0.43920898] 4   3 \n",
      "[-0.98095703 -0.01100922  0.11993408  0.73095703  0.3972168   0.17993164] 3   3 Match 169\n",
      "\n",
      "[-1.9492188  -0.20166016 -0.49536133  0.7402344   1.3369141   1.1914062 ] 4   4 Match 170\n",
      "\n",
      "[-0.9057617   0.32666016  0.3959961   0.8417969   0.26879883 -0.11212158] 3   3 Match 171\n",
      "\n",
      "[ 0.1616211   0.5800781   0.40625    -0.03114319 -0.47827148 -0.46679688] 1   1 Match 172\n",
      "\n",
      "[-1.4150391   0.22314453 -0.08325195  0.5800781   0.8144531   0.49072266] 4   5 \n",
      "[-1.4052734  -0.03582764  0.08312988  0.90771484  0.9042969   0.2541504 ] 3   2 \n",
      "[-1.84375     0.1685791  -0.34521484  0.8071289   0.96191406  0.87060547] 4   4 Match 173\n",
      "\n",
      "[-1.4921875   0.22912598 -0.47460938  0.43725586  0.84521484  0.91064453] 5   5 Match 174\n",
      "\n",
      "[-1.0517578   0.2548828   0.5908203   0.8779297   0.29516602 -0.05648804] 3   0 \n",
      "[-0.7495117   0.25146484  0.41577148  0.5527344   0.4321289  -0.10827637] 3   4 \n",
      "[ 0.08117676  0.41723633  0.01429749 -0.17114258 -0.27368164 -0.21154785] 1   4 \n",
      "[-1.1103516   0.12097168  0.30541992  0.6245117   0.6069336   0.01713562] 3   4 \n",
      "[-0.11834717  0.52441406  0.6035156   0.38330078 -0.10369873 -0.6723633 ] 2   3 \n",
      "[-1.3232422   0.5317383   0.38110352  0.68847656  0.2824707   0.2607422 ] 3   4 \n",
      "[-1.8525391   0.0690918  -0.14697266  0.7338867   0.91748047  0.6069336 ] 4   2 \n",
      "[-1.8544922   0.01441193 -0.35791016  0.76464844  1.0498047   0.9604492 ] 4   5 \n",
      "[-0.7993164   0.53808594  0.45043945  0.68603516  0.20080566 -0.09710693] 3   4 \n",
      "[-1.3232422  -0.06494141  0.04833984  0.65771484  0.70751953  0.31225586] 4   5 \n",
      "[-1.8007812   0.11767578 -0.32983398  0.5419922   0.9375      1.0859375 ] 5   3 \n",
      "[-0.89941406  0.21789551  0.2998047   0.62841797  0.34985352  0.06726074] 3   3 Match 175\n",
      "\n",
      "[-1.5722656  -0.06750488 -0.4255371   0.64746094  0.9946289   0.7001953 ] 4   4 Match 176\n",
      "\n",
      "[-1.0302734   0.41796875  0.47607422  0.73828125  0.18664551 -0.12976074] 3   3 Match 177\n",
      "\n",
      "[-1.0537109   0.0993042   0.48535156  0.6958008   0.1381836  -0.08624268] 3   5 \n",
      "[-0.86865234  0.2310791   0.25805664  0.49267578 -0.11865234  0.3359375 ] 3   5 \n",
      "[-0.9501953   0.14562988 -0.02944946  0.24157715  0.10394287  0.35424805] 5   5 Match 178\n",
      "\n",
      "[-0.8378906   0.203125    0.26708984  0.51953125  0.02540588  0.0824585 ] 3   5 \n",
      "[-0.71777344  0.39379883  0.3881836   0.6435547   0.15612793 -0.17041016] 3   4 \n",
      "[-1.7558594   0.07305908 -0.03436279  0.6088867   1.0390625   0.99658203] 4   2 \n",
      "[-1.6689453   0.14440918  0.14477539  0.89746094  0.8300781   0.6245117 ] 3   4 \n",
      "[ 0.0524292   0.609375    0.29345703  0.07513428 -0.4958496  -0.35546875] 1   2 \n",
      "[-1.5703125   0.08154297  0.06945801  0.64990234  0.8359375   0.8598633 ] 5   1 \n",
      "[-0.2722168   0.48901367  0.5629883   0.64941406 -0.01488495 -0.41870117] 3   3 Match 179\n",
      "\n",
      "[ 0.08551025  0.44360352  0.03044128 -0.15673828 -0.31860352  0.11029053] 1   5 \n",
      "[-1.3632812   0.22607422  0.11920166  0.6904297   0.81347656  0.54589844] 4   4 Match 180\n",
      "\n",
      "[-1.2011719   0.3557129   0.6665039   0.75634766  0.47094727  0.03253174] 3   3 Match 181\n",
      "\n",
      "[-0.3088379   0.5751953   0.42358398  0.3449707  -0.16479492 -0.18969727] 1   4 \n",
      "[-1.0742188   0.29077148  0.34887695  0.45776367  0.36743164  0.22131348] 3   3 Match 182\n",
      "\n",
      "[-0.33251953  0.5439453   0.47045898  0.3322754   0.0295105  -0.11029053] 1   4 \n",
      "[-0.06408691  0.46069336  0.31518555  0.23571777 -0.25976562 -0.42822266] 1   5 \n",
      "[-0.04257202  0.5004883   0.36938477  0.09576416 -0.3330078  -0.33666992] 1   2 \n",
      "[-0.12524414  0.5541992   0.06726074 -0.01376343 -0.29052734 -0.05722046] 1   5 \n",
      "[-0.5644531   0.28442383 -0.04837036  0.18762207  0.39624023  0.18322754] 4   4 Match 183\n",
      "\n",
      "[-0.6274414   0.35839844  0.08380127  0.33129883  0.01922607  0.04785156] 1   2 \n",
      "[-0.40014648  0.4543457   0.22961426  0.31811523 -0.25268555 -0.06933594] 1   3 \n",
      "[-1.7685547   0.22155762 -0.21411133  0.52490234  0.8227539   1.0175781 ] 5   2 \n",
      "[-0.6430664   0.23791504  0.12286377  0.29711914 -0.1619873  -0.05404663] 3   5 \n",
      "[-1.46875     0.1159668  -0.19421387  0.40673828  0.98046875  0.8652344 ] 4   5 \n",
      "[-0.7080078   0.3984375   0.4116211   0.31420898  0.03479004  0.06317139] 2   4 \n",
      "[-1.7578125  -0.22058105 -0.35058594  0.50341797  1.3330078   1.1914062 ] 4   3 \n",
      "[-1.8173828   0.18334961 -0.24084473  0.63427734  0.96191406  0.9589844 ] 4   5 \n",
      "[-1.7236328   0.1083374  -0.03038025  0.7192383   0.9160156   0.81591797] 4   5 \n",
      "[-1.5371094   0.09057617 -0.37329102  0.51220703  1.0068359   0.8701172 ] 4   2 \n",
      "[-0.19836426  0.5336914   0.5317383   0.32666016 -0.23425293 -0.29956055] 1   2 \n",
      "[-1.2890625   0.19299316  0.31323242  0.6933594   0.49169922  0.21069336] 3   4 \n",
      "[-0.8432617   0.18334961 -0.4921875  -0.02146912  0.78759766  1.0292969 ] 5   4 \n",
      "[ 0.25610352  0.5751953   0.5078125   0.02526855 -0.47973633 -0.69970703] 1   1 Match 184\n",
      "\n",
      "[-0.7080078   0.4658203   0.7651367   0.6845703   0.22302246 -0.3701172 ] 2   2 Match 185\n",
      "\n",
      "[-0.5258789   0.6069336   0.50439453  0.4807129   0.06951904 -0.27197266] 1   0 \n",
      "[-0.95703125  0.35791016  0.24035645  0.5942383   0.25854492  0.17553711] 3   2 \n",
      "[-1.3173828   0.33129883  0.21435547  0.66552734  0.5600586   0.3203125 ] 3   4 \n",
      "[-1.4365234   0.01034546 -0.12237549  0.7114258   0.81640625  0.44873047] 4   2 \n",
      "[-0.47851562  0.21850586  0.31176758  0.42016602 -0.03775024 -0.16210938] 3   0 \n",
      "[-0.2512207   0.41210938 -0.08996582 -0.23217773 -0.03105164  0.28881836] 1   3 \n",
      "[-1.6044922   0.18530273  0.00762939  0.85498047  0.78271484  0.5058594 ] 3   5 \n",
      "[-0.35791016  0.42993164  0.7788086   0.47216797  0.08959961 -0.56396484] 2   5 \n",
      "[-1.359375    0.23327637  0.2590332   0.8930664   0.6074219   0.09899902] 3   5 \n",
      "[-0.55810547  0.5048828   0.43579102  0.41503906 -0.09533691 -0.13232422] 1   2 \n",
      "[ 0.09790039  0.5996094   0.3388672   0.00672531 -0.40307617 -0.33740234] 1   4 \n",
      "[-1.1083984   0.48120117  0.3942871   0.6665039   0.2199707   0.12341309] 3   3 Match 186\n",
      "\n",
      "[ 0.02079773  0.47509766  0.22216797  0.01200104 -0.46777344 -0.09844971] 1   3 \n",
      "[-1.3105469   0.13745117  0.32348633  0.8125      0.5263672   0.14501953] 3   5 \n",
      "[-0.48632812  0.359375    0.3371582   0.27734375 -0.16748047  0.01895142] 1   4 \n",
      "[-1.0302734   0.20800781 -0.42700195  0.3100586   0.6010742   0.6958008 ] 5   5 Match 187\n",
      "\n",
      "[-0.7290039   0.4375      0.55810547  0.5859375   0.23510742 -0.21386719] 3   0 \n",
      "[-0.859375    0.25073242 -0.00524521  0.4765625   0.58447266  0.29418945] 4   2 \n",
      "[-1.8222656  -0.01472473 -0.40893555  0.5258789   1.2802734   1.1855469 ] 4   5 \n",
      "[-0.33666992  0.49560547  0.4633789   0.37817383 -0.17944336 -0.4519043 ] 1   1 Match 188\n",
      "\n",
      "[-1.7792969   0.00242805 -0.55566406  0.6479492   1.1865234   1.1445312 ] 4   5 \n",
      "[-1.8867188  -0.06439209 -0.28222656  0.69189453  1.0888672   0.8544922 ] 4   4 Match 189\n",
      "\n",
      "[-1.6220703   0.28588867  0.06494141  0.73583984  0.70654297  0.58935547] 3   1 \n",
      "[-1.2451172   0.35107422  0.12756348  0.73095703  0.40185547  0.36547852] 3   1 \n",
      "[ 0.6040039   0.45776367  0.36694336 -0.32983398 -1.0214844  -0.5263672 ] 0   4 \n",
      "[-1.2851562   0.27270508  0.3503418   0.6723633   0.6791992   0.09515381] 4   3 \n",
      "[-1.9189453  -0.01924133 -0.06921387  0.7192383   1.2207031   0.8544922 ] 4   3 \n",
      "[-0.79589844  0.5410156   0.26538086  0.4189453  -0.07281494  0.15429688] 1   4 \n",
      "[-0.24145508  0.21899414  0.546875    0.5493164   0.16149902 -0.6796875 ] 3   2 \n",
      "[-0.6748047   0.15661621  0.11083984  0.3474121   0.2512207   0.05828857] 3   4 \n",
      "[-0.08032227  0.6035156   0.42871094 -0.05157471 -0.06878662 -0.23669434] 1   2 \n",
      "[ 0.21655273  0.56103516  0.29663086 -0.18225098 -0.4724121  -0.37548828] 1   1 Match 190\n",
      "\n",
      "[ 0.02958679  0.32128906 -0.12670898 -0.09387207 -0.22375488 -0.02938843] 1   0 \n",
      "[-1.5615234  -0.05508423 -0.3947754   0.3701172   0.9506836   1.2705078 ] 5   4 \n",
      "[-1.4589844   0.3947754  -0.30981445  0.53027344  0.7915039   0.7426758 ] 4   1 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8339844   0.16015625  0.10882568  0.9169922   0.8383789   0.61572266] 3   5 \n",
      "[-1.4160156  -0.03096008 -0.25048828  0.5019531   1.1367188   0.74072266] 4   4 Match 191\n",
      "\n",
      "[-1.2011719   0.26049805  0.22692871  0.7631836   0.70458984  0.14758301] 3   2 \n",
      "[-1.6777344  -0.05325317 -0.3095703   0.76904297  1.2880859   0.66259766] 4   4 Match 192\n",
      "\n",
      "[-0.8930664   0.20239258 -0.1751709   0.31958008  0.17285156  0.16882324] 3   4 \n",
      "[-1.2685547   0.2619629  -0.19616699  0.53515625  0.94921875  0.50341797] 4   4 Match 193\n",
      "\n",
      "[-2.0507812e+00  9.0694427e-04 -4.0014648e-01  6.5185547e-01\n",
      "  1.2421875e+00  1.0673828e+00] 4   4 Match 194\n",
      "\n",
      "[-1.1865234   0.02705383  0.12561035  0.7026367   0.6738281   0.36987305] 3   3 Match 195\n",
      "\n",
      "[-1.6767578   0.2211914   0.19104004  0.6621094   0.9760742   0.68408203] 4   1 \n",
      "[-0.7348633   0.36401367 -0.14941406  0.07177734  0.37158203  0.64501953] 5   1 \n",
      "[-0.39916992  0.65527344  0.5209961   0.60253906 -0.22827148 -0.3647461 ] 1   1 Match 196\n",
      "\n",
      "[-1.15625     0.27368164 -0.08746338  0.31225586  0.3330078   0.6274414 ] 5   1 \n",
      "[ 0.16638184  0.44799805  0.21435547 -0.01437378 -0.50683594 -0.43310547] 1   4 \n",
      "[ 0.4633789   0.4243164   0.13330078 -0.25830078 -0.5449219  -0.3071289 ] 0   1 \n",
      "[-0.58496094  0.24475098  0.5151367   0.67626953 -0.13317871 -0.2454834 ] 3   4 \n",
      "[-1.5253906   0.07794189 -0.30908203  0.29663086  0.7324219   1.0283203 ] 5   4 \n",
      "[-0.88427734  0.35717773  0.60595703  0.8378906   0.23022461 -0.31835938] 3   4 \n",
      "[-1.0869141   0.43579102  0.29516602  0.5883789   0.39990234  0.14782715] 3   1 \n",
      "[-1.3632812   0.33642578  0.23034668  0.62109375  0.5576172   0.4169922 ] 3   2 \n",
      "[-0.73828125  0.39013672  0.54589844  0.66748047  0.5239258  -0.2565918 ] 3   5 \n",
      "[-1.4931641   0.16918945 -0.11584473  0.52197266  0.7866211   0.6958008 ] 4   3 \n",
      "[-0.26245117  0.57421875  0.359375    0.17529297 -0.2746582  -0.2175293 ] 1   2 \n",
      "[-1.78125     0.08789062 -0.12280273  0.5996094   0.93603516  0.96435547] 5   5 Match 197\n",
      "\n",
      "[-1.703125    0.01760864 -0.12658691  0.8261719   1.0175781   0.64501953] 4   1 \n",
      "[-1.7988281   0.10620117  0.06311035  0.8183594   0.7597656   0.69628906] 3   3 Match 198\n",
      "\n",
      "[ 0.3413086   0.33081055  0.14038086 -0.28857422 -0.47460938 -0.0892334 ] 0   1 \n",
      "[-1.8945312  -0.0736084  -0.4885254   0.68115234  1.4189453   1.1679688 ] 4   5 \n",
      "[-0.40356445  0.45483398 -0.11358643 -0.10583496 -0.01844788  0.27734375] 1   3 \n",
      "[-1.6855469   0.04818726 -0.11621094  0.7163086   0.8935547   0.73583984] 4   4 Match 199\n",
      "\n",
      "[-1.3203125   0.07049561 -0.04187012  0.27856445  0.70654297  0.8286133 ] 5   5 Match 200\n",
      "\n",
      "[-1.9785156  -0.07818604 -0.49365234  0.7416992   1.2783203   1.2578125 ] 4   4 Match 201\n",
      "\n",
      "[-1.6660156   0.19482422 -0.07287598  0.75878906  0.97265625  0.81396484] 4   5 \n",
      "[-0.10748291  0.42822266  0.23852539 -0.06439209 -0.10302734 -0.06207275] 1   5 \n",
      "[-1.8847656  -0.18652344 -0.4951172   0.78808594  1.3291016   1.0869141 ] 4   5 \n",
      "[-1.3457031   0.24377441  0.1665039   0.79541016  0.45361328  0.13671875] 3   4 \n",
      "[-1.5917969   0.39819336 -0.0869751   0.5546875   0.6064453   0.7363281 ] 5   5 Match 202\n",
      "\n",
      "[ 0.30126953  0.4326172   0.2043457  -0.10223389 -0.6113281  -0.3425293 ] 1   0 \n",
      "[-1.0009766   0.12854004  0.57666016  0.8691406   0.52441406 -0.3125    ] 3   2 \n",
      "[-1.4121094   0.02398682  0.07446289  0.7324219   0.97021484  0.32836914] 4   5 \n",
      "[-1.6171875   0.27929688 -0.05618286  0.5810547   0.9707031   0.6875    ] 4   1 \n",
      "[-1.5185547   0.26000977  0.14941406  0.6875      0.74853516  0.53564453] 4   2 \n",
      "[-0.03057861  0.35205078  0.3383789   0.10644531 -0.328125   -0.3256836 ] 1   2 \n",
      "[-1.3066406   0.2286377   0.0380249   0.60839844  0.7109375   0.328125  ] 4   1 \n",
      "[-1.3759766   0.34765625  0.6904297   0.83203125  0.375       0.02755737] 3   2 \n",
      "[-1.1474609   0.17114258 -0.10803223  0.46069336  0.5541992   0.6484375 ] 5   1 \n",
      "[-1.4755859   0.05026245 -0.48583984  0.4736328   1.1669922   0.8847656 ] 4   5 \n",
      "[-0.7705078   0.1003418  -0.13183594  0.08190918  0.69189453  0.6621094 ] 4   3 \n",
      "[-1.3134766   0.16625977  0.25805664  0.74658203  0.64208984  0.19702148] 3   4 \n",
      "[-1.8251953   0.01966858 -0.27319336  0.61376953  1.2080078   0.91845703] 4   3 \n",
      "[-0.7807617   0.18005371  0.2578125   0.49902344  0.09155273 -0.01464844] 3   4 \n",
      "[-1.0136719   0.44458008  0.3737793   0.57177734  0.33862305  0.31103516] 3   3 Match 203\n",
      "\n",
      "[ 0.36791992  0.6689453   0.35595703 -0.1361084  -0.52246094 -0.578125  ] 1   3 \n",
      "[-1.5117188   0.16137695 -0.16223145  0.6113281   0.8701172   0.6923828 ] 4   2 \n",
      "[-0.63378906  0.56884766  0.15527344  0.3564453   0.05914307 -0.16882324] 1   1 Match 204\n",
      "\n",
      "[-0.8847656   0.3022461   0.10449219  0.40820312  0.19311523  0.18359375] 3   3 Match 205\n",
      "\n",
      "[-0.52001953  0.42089844  0.48999023  0.46655273  0.12347412 -0.18676758] 2   4 \n",
      "[-1.5195312   0.02708435 -0.00720215  0.77783203  0.9873047   0.50439453] 4   4 Match 206\n",
      "\n",
      "[-0.28295898  0.32666016  0.22253418  0.03256226 -0.10571289  0.06317139] 1   4 \n",
      "[-1.1572266   0.21166992 -0.23864746  0.17871094  0.5644531   1.1201172 ] 5   2 \n",
      "[-1.8691406  -0.02111816 -0.23962402  0.77001953  1.1708984   0.83691406] 4   5 \n",
      "[-0.3413086   0.31152344  0.04449463  0.04986572  0.2692871   0.43286133] 5   4 \n",
      "[-1.2607422   0.30639648  0.105896    0.47070312  0.25170898  0.5917969 ] 5   4 \n",
      "[-0.92822266  0.16247559  0.04040527  0.40454102  0.69921875  0.29833984] 4   3 \n",
      "[-0.9760742   0.44702148  0.33325195  0.8178711   0.32348633  0.06347656] 3   2 \n",
      "[ 0.61816406  0.36499023  0.14831543 -0.41381836 -0.75927734 -0.4909668 ] 0   0 Match 207\n",
      "\n",
      "[-1.6230469   0.0083313  -0.16259766  0.640625    1.0566406   0.5576172 ] 4   3 \n",
      "[-1.3349609   0.19226074  0.4663086   0.9638672   0.58154297  0.12164307] 3   4 \n",
      "[-0.91503906  0.09912109 -0.4399414  -0.05685425  0.38061523  1.1533203 ] 5   2 \n",
      "[-1.2519531   0.28076172  0.35839844  0.88378906  0.2590332   0.0690918 ] 3   3 Match 208\n",
      "\n",
      "[-0.01878357  0.42944336  0.0586853  -0.17468262 -0.49780273  0.05661011] 1   4 \n",
      "[ 0.02346802  0.44921875  0.28198242  0.10498047 -0.43017578 -0.4013672 ] 1   1 Match 209\n",
      "\n",
      "[-0.87939453  0.45898438  0.68652344  0.5292969   0.2631836   0.00686646] 2   2 Match 210\n",
      "\n",
      "[-1.6953125   0.06854248 -0.29223633  0.7548828   1.0126953   0.75097656] 4   4 Match 211\n",
      "\n",
      "[-1.2451172   0.38500977  0.09326172  0.63720703  0.5883789   0.38964844] 3   2 \n",
      "[-0.3984375   0.3840332  -0.12731934 -0.15136719  0.10101318  0.31689453] 1   5 \n",
      "[-0.84765625  0.23620605  0.08239746  0.46289062  0.01539612  0.2590332 ] 3   5 \n",
      "[-1.6542969   0.20874023 -0.49194336  0.5332031   0.81689453  0.9584961 ] 5   4 \n",
      "[-0.8305664   0.23132324 -0.17456055  0.4165039   0.66796875  0.34570312] 4   5 \n",
      "[-0.55566406  0.54589844  0.80859375  0.7055664  -0.01316833 -0.56152344] 2   2 Match 212\n",
      "\n",
      "[-0.56591797  0.46411133  0.46850586  0.56152344  0.15637207 -0.20983887] 3   3 Match 213\n",
      "\n",
      "[-1.4511719   0.23132324  0.37817383  0.98779297  0.6274414   0.03170776] 3   4 \n",
      "[-1.5390625   0.12261963 -0.00960541  0.82128906  0.8432617   0.4572754 ] 4   3 \n",
      "[-1.5820312   0.42504883  0.33325195  0.8022461   0.62353516  0.31835938] 3   3 Match 214\n",
      "\n",
      "[-0.32128906  0.4597168   0.32226562  0.359375   -0.12988281 -0.39575195] 1   4 \n",
      "[-2.0234375   0.05319214 -0.44580078  0.6660156   1.0058594   1.1875    ] 5   3 \n",
      "[-1.5449219   0.2088623   0.10748291  0.8100586   0.7182617   0.44213867] 3   2 \n",
      "[-1.6386719   0.15612793  0.14074707  0.8857422   0.8535156   0.34936523] 3   4 \n",
      "[-0.61621094  0.3400879   0.45239258  0.45751953  0.07995605 -0.17492676] 3   3 Match 215\n",
      "\n",
      "[-0.3942871   0.51708984  0.6694336   0.5336914   0.04638672 -0.23266602] 2   0 \n",
      "[-0.6645508   0.30737305  0.01383972  0.08605957  0.17346191  0.37182617] 5   1 \n",
      "[-0.9165039   0.5175781   0.6166992   0.5913086   0.11334229  0.0814209 ] 2   1 \n",
      "[-0.7758789   0.3317871  -0.12634277 -0.00626755  0.21728516  0.5473633 ] 5   5 Match 216\n",
      "\n",
      "[-0.70654297  0.4489746   0.63427734  0.69677734  0.47631836 -0.4074707 ] 3   5 \n",
      "[-1.0556641   0.37890625  0.69433594  0.87402344  0.48535156 -0.22912598] 3   2 \n",
      "[-1.5546875   0.11254883  0.16918945  0.8520508   0.6948242   0.6098633 ] 3   4 \n",
      "[-1.6826172  -0.04171753 -0.10192871  0.6489258   0.9086914   0.7998047 ] 4   3 \n",
      "[-1.5078125   0.04629517 -0.20385742  0.7182617   0.97753906  0.45117188] 4   2 \n",
      "[-0.46875     0.38183594  0.4621582   0.3696289  -0.24963379 -0.13952637] 2   1 \n",
      "[-1.5947266   0.19470215 -0.22375488  0.6455078   0.9711914   0.9169922 ] 4   3 \n",
      "[-1.5908203   0.05783081 -0.16003418  0.7294922   1.1572266   0.58691406] 4   1 \n",
      "[-1.4775391   0.16235352 -0.01527405  0.6459961   0.92822266  0.6298828 ] 4   4 Match 217\n",
      "\n",
      "[-1.2919922   0.07739258 -0.01686096  0.5708008   0.8959961   0.37548828] 4   3 \n",
      "[-0.7270508   0.43676758  0.00655365  0.2241211   0.24694824  0.20605469] 1   2 \n",
      "[-0.04541016  0.5444336   0.44799805  0.1665039  -0.3647461  -0.31567383] 1   0 \n",
      "[-0.53222656  0.3486328   0.00909424  0.08789062  0.13110352  0.23266602] 1   5 \n",
      "[-0.38500977  0.3161621   0.34423828  0.28051758 -0.0184021  -0.13916016] 2   1 \n",
      "[-1.5146484   0.28173828  0.38134766  0.81884766  0.79541016  0.26782227] 3   4 \n",
      "[-0.94628906  0.4033203   0.51660156  0.6245117   0.18408203 -0.07745361] 3   4 \n",
      "[-1.4570312   0.25878906  0.26367188  0.7392578   0.5961914   0.34472656] 3   1 \n",
      "[-0.6640625   0.3317871   0.2529297   0.49780273 -0.08374023 -0.05780029] 3   0 \n",
      "[-0.5341797   0.44458008  0.38256836  0.5078125   0.11016846 -0.14428711] 3   1 \n",
      "[-1.5078125   0.04449463 -0.14587402  0.5673828   1.1601562   0.8676758 ] 4   0 \n",
      "[-1.2089844   0.25512695  0.5888672   0.9248047   0.7089844  -0.13793945] 3   3 Match 218\n",
      "\n",
      "[-0.18188477  0.34936523 -0.06915283 -0.06143188 -0.25439453  0.00077057] 1   1 Match 219\n",
      "\n",
      "[-1.1269531   0.3215332   0.31982422  0.6035156   0.30322266  0.49560547] 3   4 \n",
      "[-0.74121094  0.3071289   0.39282227  0.56347656  0.11437988 -0.24609375] 3   0 \n",
      "[-1.2304688   0.39672852  0.5673828   0.81689453  0.35742188  0.05487061] 3   5 \n",
      "[-0.8261719   0.47485352  0.04602051  0.41308594  0.38720703  0.29174805] 1   2 \n",
      "[ 0.01348877  0.19494629 -0.14453125 -0.10375977 -0.11016846  0.12322998] 1   5 \n",
      "[-1.4130859   0.01968384 -0.04449463  0.62939453  0.6982422   0.52734375] 4   1 \n",
      "[-1.0244141   0.0390625  -0.04833984  0.5229492   0.7998047   0.15942383] 4   0 \n",
      "[-1.3095703   0.35717773 -0.03182983  0.38061523  0.47802734  0.6430664 ] 5   2 \n",
      "[ 0.22949219  0.5288086   0.17175293 -0.16125488 -0.48168945 -0.19580078] 1   0 \n",
      "[-0.92822266  0.4104004   0.2902832   0.5571289   0.34570312  0.2998047 ] 3   2 \n",
      "[ 0.29882812  0.52978516  0.20117188 -0.05621338 -0.7080078  -0.59375   ] 1   1 Match 220\n",
      "\n",
      "[-0.14074707  0.5551758   0.14453125 -0.09594727 -0.24975586 -0.06002808] 1   1 Match 221\n",
      "\n",
      "[-1.203125    0.23571777  0.43579102  0.73779297  0.22351074  0.00959778] 3   1 \n",
      "[-0.40576172  0.5019531   0.52246094  0.52685547 -0.14355469 -0.234375  ] 3   5 \n",
      "[-0.61621094  0.31958008  0.28295898  0.48510742  0.16210938 -0.25341797] 3   2 \n",
      "[-0.09729004  0.5073242   0.68896484  0.4873047  -0.06787109 -0.8017578 ] 2   2 Match 222\n",
      "\n",
      "[-1.40625     0.22070312  0.06988525  0.5131836   0.47485352  0.57666016] 5   2 \n",
      "[-1.3671875   0.08544922 -0.4572754   0.56152344  0.8774414   0.82373047] 4   3 \n",
      "[-0.21862793  0.39331055  0.30615234  0.2866211  -0.22424316 -0.41210938] 1   4 \n",
      "[-1.6826172   0.35009766  0.27197266  0.86035156  0.75        0.5390625 ] 3   4 \n",
      "[-0.58203125  0.49487305  0.0848999   0.00416565  0.09606934  0.39916992] 1   5 \n",
      "[-1.78125     0.11071777 -0.24108887  0.61279297  1.1875      0.86083984] 4   5 \n",
      "[-0.97021484  0.34301758  0.21105957  0.47802734  0.47143555  0.24902344] 3   2 \n",
      "[-0.828125    0.48266602  0.1694336   0.45922852  0.23571777 -0.10839844] 1   3 \n",
      "[-1.0839844   0.33447266  0.2861328   0.58691406  0.52490234  0.39941406] 3   4 \n",
      "[-0.88378906  0.58740234  0.38452148  0.61572266  0.24951172  0.11688232] 3   3 Match 223\n",
      "\n",
      "[-1.1757812   0.21472168 -0.23950195  0.37231445  0.7397461   0.8774414 ] 5   1 \n",
      "[-1.0058594   0.36035156  0.6298828   0.7441406   0.3251953   0.14770508] 3   5 \n",
      "[-0.55615234  0.48266602  0.45507812  0.5986328   0.03164673 -0.19421387] 3   1 \n",
      "[-0.36669922  0.36083984  0.72216797  0.61328125 -0.06137085 -0.49145508] 2   2 Match 224\n",
      "\n",
      "[-0.9379883   0.2631836   0.24938965  0.50878906  0.23742676  0.20812988] 3   4 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.27929688  0.42382812  0.6489258   0.66064453 -0.17468262 -0.67089844] 3   1 \n",
      "[-1.1787109   0.05853271  0.21728516  0.9394531   0.60302734  0.03411865] 3   3 Match 225\n",
      "\n",
      "[ 0.24633789  0.50146484  0.14477539 -0.11010742 -0.5727539  -0.4951172 ] 1   2 \n",
      "[-1.4111328   0.2310791   0.37426758  0.91748047  0.6347656   0.2524414 ] 3   1 \n",
      "[ 0.13806152  0.6152344   0.3630371   0.12988281 -0.31420898 -0.5878906 ] 1   4 \n",
      "[-0.25024414  0.45776367  0.10113525 -0.01238251 -0.24645996  0.02839661] 1   2 \n",
      "[-0.8120117   0.3100586   0.52978516  0.80859375  0.13842773 -0.31567383] 3   1 \n",
      "[-1.5732422   0.06591797 -0.04122925  0.66552734  0.9770508   0.6435547 ] 4   3 \n",
      "[-0.82128906  0.0075798  -0.04504395  0.45458984  0.39770508  0.13269043] 3   4 \n",
      "[-0.81152344  0.4519043   0.47851562  0.56933594  0.07495117  0.02748108] 3   1 \n",
      "[-1.3046875   0.0021534  -0.11279297  0.49536133  0.75878906  0.60546875] 4   2 \n",
      "[-1.8232422  -0.04727173 -0.19372559  0.94677734  0.93652344  0.7495117 ] 3   4 \n",
      "[-0.22888184  0.33374023  0.00884247 -0.09924316 -0.13464355  0.13684082] 1   1 Match 226\n",
      "\n",
      "[-1.7578125  -0.01919556 -0.20935059  0.7939453   0.99316406  0.6611328 ] 4   4 Match 227\n",
      "\n",
      "[-0.0802002   0.5371094   0.5620117   0.35327148 -0.23291016 -0.38842773] 2   1 \n",
      "[-1.8789062   0.0145874  -0.55078125  0.47753906  1.1757812   1.3037109 ] 5   5 Match 228\n",
      "\n",
      "[-1.5830078   0.03302002 -0.23046875  0.52734375  0.8847656   0.6958008 ] 4   5 \n",
      "[-1.9101562  -0.19470215 -0.25097656  0.89404297  1.2167969   0.77197266] 4   3 \n",
      "[-1.1455078   0.21154785 -0.07501221  0.28173828  0.4663086   0.5751953 ] 5   1 \n",
      "[-1.5595703   0.20214844  0.0925293   0.61035156  0.8925781   0.5600586 ] 4   2 \n",
      "[ 0.23632812  0.64941406  0.63378906  0.20373535 -0.46948242 -0.7451172 ] 1   1 Match 229\n",
      "\n",
      "[-1.0537109   0.32910156  0.5488281   0.76123047  0.19104004  0.02423096] 3   5 \n",
      "[-0.69140625  0.4921875   0.76416016  0.7734375   0.08978271 -0.42578125] 3   1 \n",
      "[-1.1523438   0.2878418   0.61279297  0.75146484  0.39208984 -0.23217773] 3   2 \n",
      "[-0.8876953   0.40454102  0.03817749  0.3408203   0.11871338  0.4580078 ] 5   2 \n",
      "[-1.0117188   0.36010742  0.48632812  0.50439453  0.38012695  0.19750977] 3   4 \n",
      "[-0.8066406   0.42871094  0.38330078  0.42895508  0.20446777  0.16882324] 3   2 \n",
      "[ 0.04977417  0.5654297   0.4309082   0.16040039 -0.25512695 -0.45043945] 1   2 \n",
      "[-0.55859375  0.47583008  0.61816406  0.43823242 -0.00346756 -0.25561523] 2   5 \n",
      "[-0.9165039   0.5102539   0.78027344  0.75878906  0.32202148 -0.36010742] 2   3 \n",
      "[-0.9897461   0.2565918   0.47338867  0.77441406  0.4970703  -0.22143555] 3   2 \n",
      "[-1.5810547   0.03512573 -0.14050293  0.7709961   0.82910156  0.6845703 ] 4   3 \n",
      "[-1.6523438   0.04483032  0.27514648  0.89746094  0.6899414   0.41796875] 3   4 \n",
      "[-1.4960938   0.15075684 -0.06658936  0.54003906  0.60498047  0.8095703 ] 5   2 \n",
      "[-1.5878906   0.22619629  0.31933594  0.89208984  0.6352539   0.5834961 ] 3   1 \n",
      "[-1.2246094   0.32202148  0.11560059  0.53466797  0.38964844  0.33422852] 3   4 \n",
      "[-0.6640625   0.3239746   0.00072336  0.00802612  0.10778809  0.5234375 ] 5   1 \n",
      "[-0.21484375  0.3395996   0.01470947 -0.28955078 -0.17529297  0.34448242] 5   3 \n",
      "[-1.2919922   0.1640625   0.18981934  0.9013672   0.8041992   0.0947876 ] 3   5 \n",
      "[-0.5595703   0.3154297   0.61279297  0.68652344  0.15283203 -0.45166016] 3   3 Match 230\n",
      "\n",
      "[-1.0976562  -0.01403046 -0.5751953   0.42529297  0.8886719   0.90625   ] 5   5 Match 231\n",
      "\n",
      "[-0.54785156  0.52197266  0.5307617   0.5415039   0.05014038 -0.26538086] 3   3 Match 232\n",
      "\n",
      "[-1.2177734   0.19165039  0.1038208   0.83935547  0.54345703  0.12036133] 3   5 \n",
      "[-1.3896484   0.09124756  0.21032715  0.8520508   0.5576172   0.26879883] 3   4 \n",
      "[-0.67041016  0.28100586  0.5600586   0.57421875 -0.01495361 -0.29956055] 3   0 \n",
      "[-1.1035156   0.25219727  0.22216797  0.7236328   0.7011719   0.12133789] 3   1 \n",
      "[-0.93359375  0.4375      0.1920166   0.5800781   0.37280273  0.13989258] 3   1 \n",
      "[-0.8725586   0.41210938  0.61865234  0.64746094  0.48632812 -0.32470703] 3   0 \n",
      "[-0.9760742   0.37939453  0.11859131  0.46166992  0.07977295  0.2758789 ] 3   4 \n",
      "[ 0.03546143  0.5810547   0.3815918  -0.02125549 -0.48999023 -0.44921875] 1   3 \n",
      "[-1.7587891   0.17980957 -0.5263672   0.3125      1.1904297   1.0722656 ] 4   4 Match 233\n",
      "\n",
      "[-1.0517578   0.3544922   0.12805176  0.10565186  0.21459961  0.54541016] 5   1 \n",
      "[-1.8642578   0.02580261 -0.31640625  0.46606445  1.1269531   0.93603516] 4   3 \n",
      "[-1.5107422  -0.02029419 -0.19628906  0.859375    1.0683594   0.59521484] 4   3 \n",
      "[-1.875      -0.11956787 -0.40185547  0.79248047  1.2089844   0.90283203] 4   3 \n",
      "[-1.7001953  -0.0847168  -0.7553711   0.59375     1.4296875   0.90771484] 4   4 Match 234\n",
      "\n",
      "[-0.1694336   0.56396484 -0.15332031 -0.29858398 -0.08081055  0.35351562] 1   5 \n",
      "[-1.2275391   0.26538086 -0.4638672   0.22155762  0.42211914  1.0029297 ] 5   4 \n",
      "[-1.7265625  -0.03182983 -0.3708496   0.68896484  1.2675781   0.90625   ] 4   5 \n",
      "[-1.8076172   0.08087158 -0.08349609  0.70410156  1.1015625   0.6274414 ] 4   3 \n",
      "[-0.23266602  0.5727539   0.10705566  0.02105713 -0.21984863  0.09838867] 1   3 \n",
      "[-0.265625    0.49267578  0.6381836   0.62109375  0.10113525 -0.625     ] 2   4 \n",
      "[-1.5478516   0.14221191 -0.07611084  0.64501953  0.78759766  0.7050781 ] 4   2 \n",
      "[-1.6689453   0.08062744 -0.7163086   0.48901367  1.1005859   1.1855469 ] 5   4 \n",
      "[-1.3369141   0.3623047   0.55908203  0.7558594   0.5986328   0.0539856 ] 3   3 Match 235\n",
      "\n",
      "[-1.0976562   0.21313477  0.30566406  0.71435547  0.39404297  0.09814453] 3   3 Match 236\n",
      "\n",
      "[-1.7880859  -0.11535645 -0.30786133  0.8208008   1.1591797   0.8261719 ] 4   2 \n",
      "[-0.22509766  0.40112305  0.47485352  0.37402344 -0.01834106 -0.5234375 ] 2   2 Match 237\n",
      "\n",
      "[-1.6083984  -0.08312988 -0.26098633  0.7915039   1.0488281   0.62646484] 4   4 Match 238\n",
      "\n",
      "[-1.1132812   0.25219727  0.17089844  0.53515625  0.34985352  0.01197815] 3   2 \n",
      "[-1.3408203   0.27612305 -0.05651855  0.62353516  0.77441406  0.34033203] 4   5 \n",
      "[-0.8227539   0.29736328 -0.17602539  0.00519562  0.48876953  0.70410156] 5   1 \n",
      "[-0.25170898  0.38549805  0.06109619  0.10302734 -0.26635742 -0.1751709 ] 1   4 \n",
      "[-0.21191406  0.6328125   0.44506836  0.2902832  -0.24914551 -0.39208984] 1   2 \n",
      "[-0.3708496   0.3256836  -0.17663574 -0.12719727  0.21594238  0.38867188] 5   4 \n",
      "[-1.6992188   0.28735352 -0.19799805  0.3737793   0.72021484  0.91015625] 5   5 Match 239\n",
      "\n",
      "[-1.46875     0.18652344 -0.04562378  0.5986328   0.7060547   0.5541992 ] 4   3 \n",
      "[-0.34472656  0.28735352  0.05264282  0.30444336 -0.12054443 -0.24023438] 3   2 \n",
      "[-1.5029297   0.06988525 -0.04776001  0.58251953  1.0771484   0.5253906 ] 4   5 \n",
      "[-1.0498047   0.16882324 -0.5361328   0.3100586   0.26342773  0.56347656] 5   4 \n",
      "[-0.9580078   0.22229004  0.2841797   0.66796875  0.12359619  0.08404541] 3   3 Match 240\n",
      "\n",
      "[ 0.1673584   0.3046875  -0.05804443 -0.32495117 -0.24206543  0.12469482] 1   0 \n",
      "[-1.6435547   0.1348877   0.13122559  0.91503906  0.85253906  0.46557617] 3   5 \n",
      "[-0.73876953  0.3894043  -0.26123047  0.06201172  0.46533203  0.45776367] 4   3 \n",
      "[-0.53466797  0.390625    0.01617432  0.10131836  0.06021118  0.2319336 ] 1   1 Match 241\n",
      "\n",
      "[-0.890625    0.3334961   0.10839844  0.7026367   0.39648438  0.10888672] 3   3 Match 242\n",
      "\n",
      "[ 0.27490234  0.39453125  0.08770752 -0.06015015 -0.31445312 -0.12609863] 1   1 Match 243\n",
      "\n",
      "[-0.16271973  0.5527344   0.30126953  0.22387695 -0.27954102 -0.23181152] 1   1 Match 244\n",
      "\n",
      "[ 0.625       0.3774414   0.1907959  -0.44091797 -0.84033203 -0.39892578] 0   0 Match 245\n",
      "\n",
      "[-1.2578125   0.31811523  0.49780273  0.81347656  0.43164062  0.08068848] 3   2 \n",
      "[ 0.37353516  0.5488281   0.30810547 -0.10290527 -0.40527344 -0.5527344 ] 1   0 \n",
      "[-0.5185547   0.35620117  0.2734375   0.4194336   0.24829102  0.07287598] 3   3 Match 246\n",
      "\n",
      "[ 0.16503906  0.5         0.47705078  0.03311157 -0.3564453  -0.6982422 ] 1   5 \n",
      "[-0.8100586   0.43579102  0.5683594   0.6503906   0.22277832 -0.09283447] 3   1 \n",
      "[-0.78271484  0.39501953  0.2644043   0.49682617  0.23059082  0.17175293] 3   2 \n",
      "[-1.3251953   0.41015625  0.23303223  0.71240234  0.7167969   0.29077148] 4   3 \n",
      "[-1.8808594  -0.13830566 -0.22680664  0.93066406  1.2636719   0.84375   ] 4   4 Match 247\n",
      "\n",
      "[-1.0371094   0.13122559  0.23803711  0.6801758   0.20690918  0.13977051] 3   5 \n",
      "[-0.61572266  0.3947754   0.5029297   0.43920898  0.1907959   0.00173855] 2   4 \n",
      "[-1.7207031  -0.04855347 -0.48535156  0.6972656   1.2148438   0.87158203] 4   1 \n",
      "[-0.53271484  0.31982422  0.48583984  0.69970703  0.41015625 -0.54248047] 3   1 \n",
      "[-1.2412109   0.10009766 -0.24450684  0.51708984  0.91845703  0.4013672 ] 4   1 \n",
      "[-1.5625      0.06976318  0.05728149  0.8647461   0.69970703  0.35546875] 3   3 Match 248\n",
      "\n",
      "[-1.2412109   0.12817383  0.06921387  0.58984375  0.3076172   0.28735352] 3   5 \n",
      "[-1.1962891   0.40600586  0.10852051  0.45117188  0.21813965  0.66259766] 5   5 Match 249\n",
      "\n",
      "[-0.89160156  0.41186523  0.43603516  0.7246094   0.12536621 -0.14282227] 3   3 Match 250\n",
      "\n",
      "[-1.3193359   0.00965881 -0.35766602  0.35864258  0.90966797  0.71533203] 4   1 \n",
      "[-1.2724609   0.26000977  0.32299805  0.8125      0.6635742   0.14050293] 3   1 \n",
      "[-0.08654785  0.56933594  0.63720703  0.45874023  0.02108765 -0.74658203] 2   3 \n",
      "[-0.5073242   0.37719727 -0.21936035  0.00536346  0.09472656  0.5078125 ] 5   1 \n",
      "[-1.390625    0.2709961  -0.06567383  0.54541016  0.74316406  0.65478516] 4   1 \n",
      "[-0.0020237   0.5126953   0.14331055 -0.18347168 -0.46020508 -0.2512207 ] 1   1 Match 251\n",
      "\n",
      "[-1.2392578   0.14428711  0.24377441  0.8027344   0.35253906  0.07598877] 3   1 \n",
      "[-0.3178711   0.35180664  0.19580078  0.06585693 -0.24816895 -0.15527344] 1   3 \n",
      "[-0.2536621   0.5126953   0.01898193 -0.08776855 -0.0947876   0.10583496] 1   1 Match 252\n",
      "\n",
      "[-1.7001953   0.24621582 -0.45581055  0.39526367  0.84521484  1.0205078 ] 5   2 \n",
      "[-1.5449219   0.20825195  0.23693848  0.7446289   0.80566406  0.40771484] 4   4 Match 253\n",
      "\n",
      "[ 0.4897461   0.421875    0.35595703 -0.27001953 -0.7392578  -0.5292969 ] 0   3 \n",
      "[-1.7216797   0.13110352  0.09613037  0.9863281   0.7661133   0.5058594 ] 3   3 Match 254\n",
      "\n",
      "[ 0.7495117   0.47216797  0.24414062 -0.37719727 -0.8823242  -0.61376953] 0   0 Match 255\n",
      "\n",
      "[-0.95166016  0.2619629   0.5341797   0.9277344   0.33911133 -0.29785156] 3   3 Match 256\n",
      "\n",
      "[-1.2324219   0.33032227  0.4272461   0.80908203  0.62158203  0.2019043 ] 3   3 Match 257\n",
      "\n",
      "[-1.1826172   0.24487305  0.23413086  0.5234375   0.49804688  0.35131836] 3   1 \n",
      "[-0.6430664   0.17126465  0.28637695  0.48339844 -0.3474121  -0.19274902] 3   3 Match 258\n",
      "\n",
      "[-1.2392578   0.25219727  0.5341797   0.61865234  0.7739258   0.07293701] 4   3 \n",
      "[ 0.15393066  0.47583008  0.38476562  0.04434204 -0.36865234 -0.6699219 ] 1   1 Match 259\n",
      "\n",
      "[-0.0057869   0.51953125  0.47973633  0.04138184 -0.42163086 -0.5678711 ] 1   1 Match 260\n",
      "\n",
      "[-0.69189453  0.421875    0.61328125  0.47998047 -0.12213135 -0.08764648] 2   0 \n",
      "[-1.5195312   0.27807617 -0.2052002   0.39941406  0.7993164   0.8227539 ] 5   5 Match 261\n",
      "\n",
      "[-1.1015625   0.10205078 -0.32763672  0.39624023  0.40234375  0.66064453] 5   5 Match 262\n",
      "\n",
      "[-1.8105469  -0.14697266 -0.3022461   0.7758789   1.1074219   0.96777344] 4   5 \n",
      "[-1.8173828  -0.0209198  -0.38500977  0.5751953   1.3203125   0.9448242 ] 4   3 \n",
      "[-1.7119141   0.11407471 -0.16723633  0.42578125  0.640625    1.0419922 ] 5   3 \n",
      "[-0.9536133   0.11999512  0.10198975  0.52783203  0.42163086  0.0925293 ] 3   3 Match 263\n",
      "\n",
      "[-1.2353516   0.29760742  0.01687622  0.61279297  0.10498047  0.6826172 ] 5   1 \n",
      "[-1.7080078   0.012146    0.05700684  1.0556641   1.0126953   0.47045898] 3   2 \n",
      "[-0.62841797  0.4873047   0.40527344  0.49975586  0.1048584  -0.08691406] 3   1 \n",
      "[-1.4228516   0.15551758 -0.16772461  0.65771484  0.66552734  0.6176758 ] 4   3 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6494141   0.2775879   0.19543457  0.73535156  0.6845703   0.5239258 ] 3   4 \n",
      "[-0.4038086   0.55615234  0.3232422   0.17944336 -0.38354492  0.05270386] 1   0 \n",
      "[-1.7050781   0.04724121 -0.09667969  0.76660156  0.80908203  0.6816406 ] 4   4 Match 264\n",
      "\n",
      "[-1.3613281  -0.01357269 -0.12792969  0.7949219   0.48828125  0.29663086] 3   4 \n",
      "[-1.1953125   0.3137207   0.13110352  0.6171875   0.45996094  0.5991211 ] 3   5 \n",
      "[-1.5966797  -0.08856201 -0.00597     0.96533203  0.8364258   0.2553711 ] 3   3 Match 265\n",
      "\n",
      "[ 0.65966797  0.50878906  0.2631836  -0.37280273 -0.7680664  -0.7949219 ] 0   0 Match 266\n",
      "\n",
      "[ 0.16870117  0.4152832   0.26538086  0.12347412 -0.41333008 -0.44702148] 1   3 \n",
      "[-1.2695312   0.11627197 -0.1673584   0.59228516  0.8378906   0.65527344] 4   0 \n",
      "[-1.6904297   0.08862305 -0.16540527  0.63916016  1.0419922   0.91064453] 4   3 \n",
      "[-1.328125    0.18383789 -0.2590332   0.36254883  0.7138672   0.7084961 ] 4   5 \n",
      "[-1.9257812  -0.12322998 -0.44604492  0.85058594  1.1552734   1.1572266 ] 5   2 \n",
      "[-0.00496674  0.48120117  0.17077637  0.03134155 -0.35864258 -0.31835938] 1   5 \n",
      "[-0.29663086  0.5859375   0.43701172  0.36376953 -0.2322998  -0.46166992] 1   1 Match 267\n",
      "\n",
      "[-1.3925781   0.1472168  -0.39672852  0.48779297  0.95458984  0.8520508 ] 4   2 \n",
      "[-1.6650391  -0.03707886 -0.2734375   0.83447266  0.81933594  0.5761719 ] 3   5 \n",
      "[ 0.06970215  0.5332031   0.51660156  0.24743652 -0.31762695 -0.64404297] 1   1 Match 268\n",
      "\n",
      "[-1.1904297   0.3269043   0.44580078  0.74609375  0.63427734  0.29467773] 3   1 \n",
      "[-1.15625     0.2861328   0.28125     0.82714844  0.625      -0.05801392] 3   4 \n",
      "[-1.7509766  -0.00622559 -0.66503906  0.4116211   1.5869141   1.0332031 ] 4   5 \n",
      "[-0.5517578   0.47631836  0.5917969   0.3701172  -0.06945801 -0.10107422] 2   1 \n",
      "[ 0.29858398  0.41918945  0.20837402 -0.21325684 -0.7714844  -0.35620117] 1   3 \n",
      "[-1.0839844   0.42944336  0.14294434  0.4243164   0.4416504   0.2524414 ] 4   1 \n",
      "[-0.9321289   0.37475586  0.66845703  0.8666992   0.2861328  -0.20593262] 3   2 \n",
      "[-0.48095703  0.5175781   0.5595703   0.5019531  -0.12268066 -0.37475586] 2   2 Match 269\n",
      "\n",
      "[-1.5263672   0.07446289  0.14025879  0.65966797  0.70703125  0.47436523] 4   4 Match 270\n",
      "\n",
      "[-1.2363281   0.03607178 -0.0236969   0.5288086   0.4543457   0.46704102] 3   4 \n",
      "[-1.4960938   0.11914062  0.0461731   0.7441406   0.71875     0.43139648] 3   2 \n",
      "[-1.8193359   0.05535889  0.19763184  0.9584961   0.7607422   0.5498047 ] 3   3 Match 271\n",
      "\n",
      "[-1.9912109   0.05938721 -0.29589844  0.6855469   1.0410156   1.0810547 ] 5   4 \n",
      "[-1.6650391   0.0038166  -0.09008789  0.80566406  1.1103516   0.6098633 ] 4   3 \n",
      "[-1.7441406   0.11865234 -0.2121582   0.5654297   1.1054688   0.9770508 ] 4   2 \n",
      "[-1.1904297   0.15393066  0.13562012  0.7011719   0.5522461   0.22509766] 3   2 \n",
      "[ 0.29003906  0.6279297   0.421875    0.11309814 -0.6201172  -0.83203125] 1   2 \n",
      "[-1.125       0.22558594  0.5488281   0.7421875   0.60546875  0.0031395 ] 3   4 \n",
      "[-0.6826172   0.32788086  0.05599976  0.23828125  0.19274902  0.20385742] 1   2 \n",
      "[-0.7290039   0.4169922   0.48486328  0.63671875  0.21765137 -0.31860352] 3   3 Match 272\n",
      "\n",
      "[-1.5507812  -0.10870361  0.17675781  0.89453125  0.4741211   0.36669922] 3   1 \n",
      "[-0.9033203   0.37670898  0.5307617   0.59521484  0.2927246   0.01713562] 3   3 Match 273\n",
      "\n",
      "[-1.4365234   0.2956543   0.15783691  0.69677734  0.5888672   0.38598633] 3   2 \n",
      "[-0.82421875  0.5839844   0.01148224  0.31958008  0.1965332   0.19128418] 1   1 Match 274\n",
      "\n",
      "[-0.27441406  0.4880371   0.64453125  0.49902344  0.13171387 -0.6508789 ] 2   3 \n",
      "[-0.8935547   0.21960449  0.43945312  0.6176758   0.1328125  -0.1628418 ] 3   4 \n",
      "[-0.2800293   0.7011719   0.7163086   0.2578125  -0.42773438 -0.3815918 ] 2   3 \n",
      "[-0.7944336   0.48291016  0.35913086  0.5         0.08459473  0.13659668] 3   3 Match 275\n",
      "\n",
      "[-1.1738281   0.41967773  0.21618652  0.58447266  0.5004883   0.16516113] 3   1 \n",
      "[-1.5175781   0.13635254  0.31860352  1.0166016   0.6225586   0.07940674] 3   2 \n",
      "[-0.48461914  0.24645996 -0.1038208   0.18603516  0.1114502   0.35986328] 5   1 \n",
      "[-0.21887207  0.4645996  -0.040802   -0.24597168 -0.09790039  0.20910645] 1   1 Match 276\n",
      "\n",
      "[-1.9589844   0.07739258 -0.4284668   0.7392578   1.0351562   1.0751953 ] 5   4 \n",
      "[-1.4208984   0.21582031  0.02429199  0.5229492   0.31518555  0.62402344] 5   4 \n",
      "[-1.2949219   0.2397461   0.03833008  0.54003906  0.69433594  0.4411621 ] 4   5 \n",
      "[-1.5546875   0.31079102 -0.02282715  0.88134766  0.7290039   0.5341797 ] 3   1 \n",
      "[-1.8203125   0.1850586  -0.04888916  0.7216797   0.87890625  0.89404297] 5   3 \n",
      "[-0.87353516  0.3642578   0.5805664   0.6152344   0.21240234 -0.16711426] 3   3 Match 277\n",
      "\n",
      "[-1.8320312  -0.08862305 -0.38964844  0.7871094   1.2841797   0.93310547] 4   5 \n",
      "[-1.671875    0.15844727  0.06365967  0.83496094  0.7973633   0.48486328] 3   3 Match 278\n",
      "\n",
      "[-1.046875    0.47192383  0.67871094  0.8574219   0.33276367 -0.01686096] 3   1 \n",
      "[-0.43652344  0.32836914 -0.35058594 -0.23474121 -0.03729248  0.62109375] 5   4 \n",
      "[-1.375       0.32714844  0.3491211   0.7011719   0.6484375   0.3581543 ] 3   3 Match 279\n",
      "\n",
      "[ 0.1875      0.56152344  0.24743652  0.00499725 -0.34838867 -0.62158203] 1   3 \n",
      "[-1.7636719  -0.04925537 -0.10253906  0.9111328   1.0859375   0.5629883 ] 4   3 \n",
      "[-1.7880859  -0.01661682 -0.30444336  0.76220703  1.0351562   0.85498047] 4   3 \n",
      "[-1.5556641   0.04885864 -0.04403687  0.49316406  0.71533203  0.94873047] 5   4 \n",
      "[-1.3417969   0.24572754  0.3059082   0.89208984  0.6074219   0.0227356 ] 3   3 Match 280\n",
      "\n",
      "[-1.328125    0.11749268 -0.265625    0.28564453  0.78222656  0.9370117 ] 5   3 \n",
      "[-0.86279297  0.47094727  0.60791016  0.7011719   0.40771484 -0.22314453] 3   3 Match 281\n",
      "\n",
      "[-0.05343628  0.5708008   0.39379883  0.30029297 -0.31713867 -0.47583008] 1   2 \n",
      "[-1.7373047  -0.12451172 -0.11694336  0.87158203  1.0175781   0.6660156 ] 4   5 \n",
      "[-1.1318359   0.2602539  -0.00387192  0.39819336  0.20422363  0.38256836] 3   2 \n",
      "[-0.36499023  0.5053711   0.34765625  0.375      -0.08337402 -0.25854492] 1   1 Match 282\n",
      "\n",
      "[-1.3574219   0.23376465 -0.17529297  0.40942383  0.63964844  0.77001953] 5   2 \n",
      "[-1.3671875   0.23083496  0.25732422  0.7602539   0.5854492   0.35424805] 3   5 \n",
      "[-0.17407227  0.3371582   0.3227539   0.46020508  0.16809082 -0.42895508] 3   1 \n",
      "[-1.8085938   0.05688477 -0.09259033  0.9189453   1.1279297   0.6386719 ] 4   5 \n",
      "[-1.5078125   0.18395996 -0.32421875  0.55029297  0.9975586   0.80615234] 4   1 \n",
      "[-0.05749512  0.49829102  0.11914062 -0.12145996 -0.37670898 -0.12927246] 1   3 \n",
      "[-1.7070312   0.14416504 -0.06027222  0.7285156   1.0976562   0.52197266] 4   0 \n",
      "[-1.1582031   0.36767578  0.47973633  0.81933594  0.3388672   0.01335144] 3   3 Match 283\n",
      "\n",
      "[-0.48364258  0.5307617   0.47583008  0.46044922  0.17980957 -0.37817383] 1   3 \n",
      "[-0.88964844  0.19360352  0.55126953  0.75927734  0.296875   -0.21008301] 3   1 \n",
      "[-0.88378906  0.29663086  0.36279297  0.7397461   0.0329895  -0.23376465] 3   1 \n",
      "[-0.3371582   0.17858887 -0.19445801  0.24597168  0.29614258 -0.01756287] 4   1 \n",
      "[-1.3554688  -0.12780762 -0.45458984  0.4416504   1.0205078   0.7241211 ] 4   1 \n",
      "[-0.6044922   0.46948242  0.07385254  0.15637207  0.04095459  0.3449707 ] 1   3 \n",
      "[-0.8173828   0.09899902 -0.67626953  0.0791626   0.3762207   0.7373047 ] 5   5 Match 284\n",
      "\n",
      "[-1.1679688   0.2668457  -0.01794434  0.6821289   0.27172852  0.3642578 ] 3   3 Match 285\n",
      "\n",
      "[-1.3984375   0.00591278 -0.06402588  0.5493164   0.640625    0.4567871 ] 4   3 \n",
      "[-1.46875     0.05001831 -0.12475586  0.5620117   0.9326172   0.7080078 ] 4   2 \n",
      "[-0.35668945  0.3947754   0.18518066  0.12573242 -0.24084473 -0.01426697] 1   3 \n",
      "[ 0.6669922   0.39086914  0.2512207  -0.45703125 -0.7548828  -0.48168945] 0   4 \n",
      "[-0.87158203  0.28320312  0.03210449  0.55566406  0.64160156  0.21154785] 4   5 \n",
      "[-1.0625      0.36767578  0.27026367  0.48461914  0.33422852  0.24206543] 3   2 \n",
      "[-1.1904297   0.24951172  0.3791504   0.671875    0.60253906  0.13146973] 3   1 \n",
      "[-0.5991211   0.43481445  0.5209961   0.33691406 -0.17138672 -0.0149231 ] 2   1 \n",
      "[-0.16174316  0.53808594  0.45874023  0.35253906 -0.1083374  -0.36865234] 1   1 Match 286\n",
      "\n",
      "[-0.93896484  0.44604492  0.84814453  0.796875    0.33618164 -0.45947266] 2   2 Match 287\n",
      "\n",
      "[-1.2099609   0.27929688 -0.23742676  0.29101562  0.80566406  0.57958984] 4   1 \n",
      "[-0.33447266  0.43554688  0.4970703   0.34936523 -0.47314453 -0.6230469 ] 2   2 Match 288\n",
      "\n",
      "[-0.32250977  0.5620117   0.25878906  0.19348145 -0.2541504  -0.21435547] 1   0 \n",
      "[-1.7226562  -0.01035309 -0.37573242  0.5620117   1.1171875   0.9277344 ] 4   4 Match 289\n",
      "\n",
      "[ 0.02638245  0.51708984  0.2800293   0.02354431 -0.3605957  -0.5600586 ] 1   1 Match 290\n",
      "\n",
      "[-0.73828125  0.47143555  0.05300903  0.38623047 -0.08404541  0.05099487] 1   1 Match 291\n",
      "\n",
      "[-0.8378906   0.44311523  0.5263672   0.54003906  0.23815918  0.10479736] 3   2 \n",
      "[-0.0824585   0.4736328  -0.02536011 -0.14575195 -0.23876953  0.10852051] 1   1 Match 292\n",
      "\n",
      "[-1.5683594   0.30981445  0.17895508  0.6591797   0.7675781   0.54248047] 4   4 Match 293\n",
      "\n",
      "[-1.1835938   0.4165039   0.6694336   0.84814453  0.4506836   0.00514984] 3   3 Match 294\n",
      "\n",
      "[-1.7089844   0.14562988  0.05352783  0.83984375  0.8359375   0.51953125] 3   3 Match 295\n",
      "\n",
      "[-1.3652344   0.25976562  0.1739502   0.7597656   0.32202148  0.44140625] 3   3 Match 296\n",
      "\n",
      "[-1.7080078  -0.08447266 -0.2475586   0.67529297  0.9345703   0.7006836 ] 4   3 \n",
      "[-1.4443359   0.14611816 -0.03607178  0.7573242   0.7397461   0.7416992 ] 3   0 \n",
      "[-1.9335938   0.00811768 -0.4934082   0.5410156   1.3740234   1.1542969 ] 4   4 Match 297\n",
      "\n",
      "[-1.3164062   0.27563477  0.39208984  0.9433594   0.54052734  0.09906006] 3   2 \n",
      "[-0.29882812  0.44458008 -0.08453369 -0.15698242 -0.04052734  0.3305664 ] 1   3 \n",
      "[-0.2397461   0.5292969   0.24401855  0.07757568 -0.3232422  -0.1348877 ] 1   1 Match 298\n",
      "\n",
      "[-1.5019531   0.09246826  0.04003906  0.7216797   0.6386719   0.81640625] 5   4 \n",
      "[-0.01564026  0.44458008  0.07122803  0.00084114 -0.13781738 -0.07067871] 1   1 Match 299\n",
      "\n",
      "[-0.9316406   0.38989258  0.55322266  0.7631836   0.1541748  -0.03149414] 3   4 \n",
      "[-0.16931152  0.5473633   0.6040039   0.4638672  -0.28100586 -0.71533203] 2   2 Match 300\n",
      "\n",
      "[-1.3671875  -0.04071045  0.05148315  0.7998047   0.8413086   0.27270508] 4   3 \n",
      "[ 0.7055664   0.38671875  0.23339844 -0.42749023 -0.8198242  -0.66845703] 0   2 \n",
      "[ 0.27661133  0.4814453   0.24438477 -0.0980835  -0.55566406 -0.31420898] 1   3 \n",
      "[-1.0595703   0.23754883 -0.07775879  0.32348633  0.57958984  0.5307617 ] 4   1 \n",
      "[-1.6748047  -0.11065674 -0.31420898  0.69140625  1.1396484   0.8076172 ] 4   5 \n",
      "[ 0.06829834  0.5698242   0.27441406  0.14660645 -0.4230957  -0.31713867] 1   5 \n",
      "[-0.88916016  0.30444336  0.56103516  0.6503906   0.5292969  -0.26000977] 3   1 \n",
      "[-1.7460938   0.20324707  0.09606934  0.7558594   0.7939453   0.87597656] 5   2 \n",
      "[-1.2539062   0.17346191  0.16149902  0.68652344  0.3269043   0.15576172] 3   1 \n",
      "[-1.2421875  -0.00960541 -0.48461914  0.1616211   0.60058594  1.0351562 ] 5   4 \n",
      "[-1.2626953   0.13879395  0.00537872  0.57666016  0.62158203  0.7084961 ] 5   3 \n",
      "[-1.1796875  -0.15649414 -0.20446777  0.63964844  0.9863281   0.57666016] 4   5 \n",
      "[-0.20751953  0.5708008   0.5546875   0.34179688 -0.34521484 -0.5654297 ] 1   0 \n",
      "[-1.5478516   0.2319336   0.15881348  0.8691406   0.9355469   0.34594727] 4   4 Match 301\n",
      "\n",
      "[-0.72216797  0.44091797  0.5678711   0.7163086   0.1149292  -0.3569336 ] 3   1 \n",
      "[-1.7441406   0.04156494 -0.13330078  0.81933594  0.93603516  0.51660156] 4   2 \n",
      "[-0.3503418   0.45361328  0.23547363  0.24633789 -0.10888672 -0.11254883] 1   5 \n",
      "[-1.0722656   0.12353516  0.1105957   0.6738281   0.38989258  0.2265625 ] 3   5 \n",
      "[-1.5146484   0.04058838 -0.26733398  0.54003906  0.9716797   0.7128906 ] 4   5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0566406   0.22705078  0.37817383  0.69189453  0.18457031  0.15161133] 3   3 Match 302\n",
      "\n",
      "[-0.9301758   0.32910156  0.50878906  0.6640625   0.15039062  0.05825806] 3   2 \n",
      "[-0.30981445  0.48168945  0.47045898  0.37036133 -0.20410156 -0.29858398] 1   3 \n",
      "[ 0.16210938  0.48828125  0.2915039  -0.01626587 -0.4555664  -0.3137207 ] 1   1 Match 303\n",
      "\n",
      "[-1.7373047   0.01007843 -0.19799805  0.74365234  1.109375    0.9067383 ] 4   1 \n",
      "[-1.4892578   0.12670898  0.17919922  0.79589844  0.7885742   0.32958984] 3   5 \n",
      "[-0.99853516  0.4404297   0.28393555  0.6767578   0.23779297  0.07849121] 3   1 \n",
      "[-0.23461914  0.50341797  0.23022461  0.05453491 -0.1973877  -0.09588623] 1   0 \n",
      "[-1.9453125  -0.06335449 -0.2902832   0.72216797  1.2236328   1.0185547 ] 4   0 \n",
      "[-1.0869141   0.39257812  0.6904297   0.8876953   0.5341797  -0.19970703] 3   5 \n",
      "[-0.80126953  0.5336914   0.28125     0.59472656  0.140625    0.08221436] 3   1 \n",
      "[-0.53808594  0.53125     0.23339844 -0.04318237 -0.05764771  0.37573242] 1   4 \n",
      "[-0.02099609  0.5996094   0.41967773  0.07739258 -0.4494629  -0.5541992 ] 1   3 \n",
      "[-0.62060547  0.3635254   0.23303223  0.3955078   0.1784668   0.0916748 ] 3   1 \n",
      "[-1.0087891   0.42797852  0.72021484  0.81152344  0.25048828 -0.00747681] 3   4 \n",
      "[-1.1181641   0.23828125  0.2927246   0.67578125  0.35351562  0.09735107] 3   1 \n",
      "[-0.6928711   0.35205078  0.36279297  0.48461914  0.21240234 -0.07049561] 3   0 \n",
      "[ 0.46289062  0.57910156  0.4741211  -0.10168457 -0.79785156 -0.7529297 ] 1   0 \n",
      "[-0.99658203  0.2849121   0.32128906  0.60595703  0.2932129   0.13562012] 3   4 \n",
      "[-1.3623047   0.3178711   0.27148438  0.8544922   0.44335938  0.4819336 ] 3   4 \n",
      "[-1.0068359   0.20715332  0.12902832  0.30126953  0.06140137  0.27978516] 3   1 \n",
      "[-1.7304688  -0.06945801 -0.10876465  0.76123047  1.0605469   0.6665039 ] 4   4 Match 304\n",
      "\n",
      "[-1.2109375   0.24865723  0.5644531   0.7241211   0.6816406   0.01415253] 3   2 \n",
      "[-1.6230469  -0.00694275 -0.11651611  0.74609375  0.76220703  0.5727539 ] 4   1 \n",
      "[-0.11273193  0.37182617  0.5083008   0.2626953  -0.13342285 -0.41503906] 2   3 \n",
      "[-1.5654297  -0.13684082 -0.39697266  0.77685547  0.97509766  0.8540039 ] 4   5 \n",
      "[-0.31420898  0.6298828   0.43530273  0.36669922 -0.10375977 -0.39208984] 1   5 \n",
      "[-0.14172363  0.5620117   0.28955078  0.10083008 -0.33666992 -0.20324707] 1   5 \n",
      "[-0.8642578   0.5600586   0.66503906  0.7314453   0.19189453 -0.03735352] 3   2 \n",
      "[-0.38989258  0.5180664   0.58935547  0.29614258  0.08422852 -0.3828125 ] 2   3 \n",
      "[-1.8378906   0.03988647 -0.08062744  0.8647461   1.1943359   0.71777344] 4   5 \n",
      "[-0.71533203  0.42944336  0.01047516  0.3215332   0.37597656  0.26611328] 1   4 \n",
      "[-1.1738281   0.48242188  0.38842773  0.45874023  0.5024414   0.31176758] 4   1 \n",
      "[ 0.25634766  0.5522461   0.26123047 -0.12646484 -0.2944336  -0.4560547 ] 1   2 \n",
      "[-1.2949219   0.3203125   0.26733398  0.8251953   0.7758789   0.09484863] 3   1 \n",
      "[-0.3876953   0.37963867  0.40307617  0.3623047  -0.11633301 -0.21447754] 2   2 Match 305\n",
      "\n",
      "[-0.93896484  0.13171387  0.17138672  0.5698242   0.4946289   0.08123779] 3   3 Match 306\n",
      "\n",
      "[ 0.24963379  0.5756836   0.3959961   0.0579834  -0.58984375 -0.8105469 ] 1   1 Match 307\n",
      "\n",
      "[ 0.24023438  0.5390625   0.26391602 -0.2376709  -0.2915039  -0.21765137] 1   3 \n",
      "[ 0.5361328   0.5908203   0.31567383 -0.19238281 -0.77246094 -0.6323242 ] 1   4 \n",
      "[-0.7792969   0.5307617   0.3918457   0.43896484  0.12384033  0.07373047] 1   2 \n",
      "[-1.0634766   0.19445801  0.47314453  0.73876953  0.6191406  -0.15136719] 3   5 \n",
      "[-0.25317383  0.3935547   0.36401367  0.3334961  -0.1977539  -0.2442627 ] 1   1 Match 308\n",
      "\n",
      "[-1.0615234   0.2121582  -0.009552    0.34521484  0.32592773  0.6245117 ] 5   4 \n",
      "[-1.3613281   0.24511719  0.47680664  0.8022461   0.47314453  0.08642578] 3   4 \n",
      "[-1.2167969   0.13989258  0.25219727  0.68652344  0.4621582   0.30029297] 3   2 \n",
      "[ 0.3618164   0.58740234  0.34887695 -0.06262207 -0.4038086  -0.66796875] 1   2 \n",
      "[-1.1044922   0.18920898  0.00759506  0.5756836   0.56347656  0.29956055] 3   2 \n",
      "[ 0.02729797  0.46777344  0.36791992  0.06921387 -0.39453125 -0.37109375] 1   0 \n",
      "[-0.55126953  0.5053711   0.6928711   0.48046875 -0.08673096 -0.14477539] 2   1 \n",
      "[-1.4501953   0.33544922  0.38916016  0.5913086   0.59228516  0.5180664 ] 4   5 \n",
      "[-0.82910156  0.41015625  0.32714844  0.44799805  0.15893555  0.23718262] 3   1 \n",
      "[-0.7792969   0.39111328  0.6850586   0.73583984  0.18066406 -0.4033203 ] 3   1 \n",
      "[-1.3427734  -0.0418396  -0.4230957   0.6689453   1.0556641   0.55615234] 4   2 \n",
      "[-1.5253906   0.10290527  0.0413208   0.66015625  0.8183594   0.4963379 ] 4   4 Match 309\n",
      "\n",
      "[-1.7890625   0.04376221 -0.14208984  0.6767578   1.0009766   1.0166016 ] 5   2 \n",
      "[-1.6083984   0.140625   -0.1842041   0.72265625  0.7734375   0.76416016] 4   5 \n",
      "[-0.83984375  0.40234375  0.08508301  0.38330078  0.2919922   0.10137939] 1   1 Match 310\n",
      "\n",
      "[-1.0029297   0.5180664   0.5371094   0.49853516  0.27954102  0.11999512] 2   3 \n",
      "[-1.4804688   0.2590332  -0.02346802  0.6069336   0.7080078   0.7314453 ] 5   3 \n",
      "[-1.8623047  -0.28125    -0.36035156  0.86816406  1.2109375   1.0605469 ] 4   3 \n",
      "[-1.4404297   0.32788086  0.4362793   0.7480469   0.64941406  0.3527832 ] 3   2 \n",
      "[-0.6479492   0.26660156  0.515625    0.65527344  0.37939453 -0.45947266] 3   3 Match 311\n",
      "\n",
      "[-0.23291016  0.578125    0.48388672  0.45532227  0.04742432 -0.35620117] 1   2 \n",
      "[-0.953125    0.19091797  0.04934692  0.49145508  0.45141602  0.08154297] 3   5 \n",
      "[-1.7304688  -0.04400635 -0.37646484  0.6845703   1.3398438   0.80859375] 4   4 Match 312\n",
      "\n",
      "[-1.765625    0.05160522 -0.1583252   0.8232422   1.0664062   0.7739258 ] 4   3 \n",
      "[-0.10314941  0.5571289   0.37524414  0.05392456 -0.5029297  -0.47143555] 1   4 \n",
      "[-0.9736328   0.25805664  0.16833496  0.6064453   0.2849121   0.2944336 ] 3   3 Match 313\n",
      "\n",
      "[-1.1386719   0.23962402  0.37768555  0.78564453  0.45947266  0.10717773] 3   4 \n",
      "[-0.1307373   0.49682617  0.09545898 -0.05212402 -0.5151367  -0.21520996] 1   1 Match 314\n",
      "\n",
      "[-1.6201172   0.25854492  0.2220459   0.84521484  0.5292969   0.3244629 ] 3   1 \n",
      "[-0.46704102  0.55566406  0.88916016  0.7080078   0.13806152 -0.6640625 ] 2   3 \n",
      "[-0.46044922  0.5102539   0.42797852  0.36865234 -0.03356934 -0.01225281] 1   1 Match 315\n",
      "\n",
      "[-1.6044922   0.05535889  0.03173828  0.84521484  0.94433594  0.609375  ] 4   3 \n",
      "[-0.02365112  0.5136719   0.40844727  0.2697754  -0.2644043  -0.58935547] 1   3 \n",
      "[-1.5332031   0.10705566  0.34887695  0.9213867   0.78466797  0.30737305] 3   3 Match 316\n",
      "\n",
      "[-0.10192871  0.5175781   0.10742188 -0.11791992 -0.43139648 -0.00230789] 1   1 Match 317\n",
      "\n",
      "[-1.0703125   0.18237305  0.32910156  0.7998047   0.34472656  0.12976074] 3   5 \n",
      "[ 0.03189087  0.6376953   0.24060059 -0.16394043 -0.4716797  -0.43847656] 1   0 \n",
      "[ 0.14086914  0.38842773 -0.00691605 -0.19116211 -0.3659668  -0.01040649] 1   0 \n",
      "[-1.6513672   0.17687988 -0.06286621  0.6669922   0.89941406  0.7089844 ] 4   4 Match 318\n",
      "\n",
      "[-0.57714844  0.6567383   0.42456055  0.53759766 -0.03225708 -0.27783203] 1   3 \n",
      "[-1.2363281   0.19445801  0.11535645  0.56884766  0.4638672   0.54785156] 3   4 \n",
      "[-0.74658203  0.4724121   0.4033203   0.5864258  -0.06427002 -0.28955078] 3   2 \n",
      "[-1.3144531   0.1673584   0.04373169  0.5415039   0.41723633  0.6074219 ] 5   0 \n",
      "[-0.89990234  0.04153442 -0.25683594  0.3894043   0.3540039   0.48510742] 5   2 \n",
      "[-0.09509277  0.52783203  0.5288086   0.2355957  -0.5605469  -0.56591797] 2   3 \n",
      "[-1.2441406  -0.01921082 -0.28833008  0.48706055  0.91748047  0.39794922] 4   4 Match 319\n",
      "\n",
      "[ 0.6118164   0.4013672   0.25756836 -0.2668457  -0.81396484 -0.74316406] 0   1 \n",
      "[-0.9189453   0.26635742  0.22045898  0.65527344  0.2163086  -0.01490021] 3   5 \n",
      "[-0.51220703  0.50146484  0.2980957   0.1394043  -0.20324707  0.22045898] 1   0 \n",
      "[-0.26293945  0.2854004  -0.03277588  0.14318848 -0.01345062  0.07049561] 1   1 Match 320\n",
      "\n",
      "[-0.04803467  0.4765625   0.24975586 -0.1151123  -0.46484375 -0.19555664] 1   0 \n",
      "[-1.1728516   0.42211914  0.3623047   0.65771484  0.59716797  0.26367188] 3   4 \n",
      "[ 0.16223145  0.44335938  0.05795288 -0.27905273 -0.3227539  -0.1307373 ] 1   0 \n",
      "[-0.26220703  0.40576172  0.29589844  0.10662842 -0.33447266 -0.20739746] 1   1 Match 321\n",
      "\n",
      "[-0.671875    0.09936523  0.19995117  0.32080078  0.3112793   0.15551758] 3   2 \n",
      "[-1.4140625   0.0869751  -0.33276367  0.64501953  0.94921875  0.76123047] 4   2 \n",
      "[-1.4580078   0.01608276 -0.35498047  0.5097656   0.77685547  0.87597656] 5   0 \n",
      "[-0.15759277  0.5878906   0.18273926  0.02505493 -0.17944336  0.05404663] 1   1 Match 322\n",
      "\n",
      "[ 0.67089844  0.38061523  0.19384766 -0.40942383 -0.6196289  -0.35986328] 0   2 \n",
      "[-1.1914062  -0.05493164 -0.38183594  0.56884766  0.6748047   0.80566406] 5   5 Match 323\n",
      "\n",
      "[-0.4194336   0.484375    0.20751953  0.17126465 -0.14453125  0.1348877 ] 1   2 \n",
      "[-0.44458008  0.40234375  0.40771484  0.5385742   0.02172852 -0.09594727] 3   5 \n",
      "[-0.89160156  0.41235352  0.1505127   0.42895508  0.28295898  0.29589844] 3   5 \n",
      "[-1.6074219   0.19812012 -0.14794922  0.5410156   0.87890625  0.7060547 ] 4   4 Match 324\n",
      "\n",
      "[-1.3134766  -0.03433228 -0.29516602  0.5991211   1.0332031   0.42333984] 4   3 \n",
      "[-0.5751953   0.3330078  -0.11224365 -0.13903809  0.14782715  0.4025879 ] 5   4 \n",
      "[-1.0273438   0.07507324 -0.5986328   0.13769531  0.70410156  0.7973633 ] 5   3 \n",
      "[-0.70458984  0.4038086   0.10229492  0.20935059  0.05447388  0.43847656] 5   1 \n",
      "[-1.8378906   0.15673828 -0.43188477  0.6953125   1.0976562   0.9663086 ] 4   1 \n",
      "[ 0.0723877   0.41357422  0.11273193 -0.19787598 -0.56396484 -0.2010498 ] 1   1 Match 325\n",
      "\n",
      "[-0.6933594   0.40161133  0.58251953  0.63623047  0.16394043 -0.31201172] 3   1 \n",
      "[ 0.7495117   0.34716797  0.23046875 -0.56884766 -0.75       -0.38500977] 0   5 \n",
      "[-0.90625     0.33447266  0.58203125  0.8876953   0.2993164  -0.2553711 ] 3   2 \n",
      "[-1.6230469   0.24938965  0.02781677  0.65185547  0.73339844  0.56396484] 4   0 \n",
      "[-0.91308594  0.37890625  0.44335938  0.7949219   0.32202148 -0.10089111] 3   3 Match 326\n",
      "\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "Pred=[]\n",
    "\n",
    "countCorrect=0\n",
    "\n",
    "for row in range(TestModel_outputs.shape[0]):\n",
    "    outputs=TestModel_outputs[row]\n",
    "    #print(test.iloc[row,0])\n",
    "    print(outputs, end=' ')\n",
    "    \n",
    "    result=0\n",
    "    if outputs[0]<outputs[1]:result=1\n",
    "    if outputs[result]<outputs[2]:result=2\n",
    "    if outputs[result]<outputs[3]:result=3\n",
    "    if outputs[result]<outputs[4]:result=4\n",
    "    if outputs[result]<outputs[5]:result=5\n",
    "    Pred.append(result)\n",
    "    print(result, ' ',test.iloc[row,1], end=' ')\n",
    "    if result==test.iloc[row,1]:\n",
    "        countCorrect+=1\n",
    "        print('Match',countCorrect)\n",
    "    print('')\n",
    "\n",
    "print(countCorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7  41   9  17   9   8]\n",
      " [  6  69  13  72  43  30]\n",
      " [  5  56  31  82  32  15]\n",
      " [  4  45  19 105  65  17]\n",
      " [  2  33  10  89  78  37]\n",
      " [  4  31   6  54  75  36]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(test['labels'],Pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Pants       0.25      0.08      0.12        91\n",
      "       False       0.25      0.30      0.27       233\n",
      " Barely-True       0.35      0.14      0.20       221\n",
      "   Half-True       0.25      0.41      0.31       255\n",
      " Mostly-True       0.26      0.31      0.28       249\n",
      "        True       0.25      0.17      0.21       206\n",
      "\n",
      "    accuracy                           0.26      1255\n",
      "   macro avg       0.27      0.24      0.23      1255\n",
      "weighted avg       0.27      0.26      0.25      1255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Pants', 'False', 'Barely-True','Half-True','Mostly-True','True']\n",
    "\n",
    "print(metrics.classification_report(test['labels'], Pred,target_names =target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Saving Complete on 2020-04-19 12:57:07.866949 in: ./TunedModels/bert/bert-base-cased/Saves/\n"
     ]
    }
   ],
   "source": [
    "# saving the output of the models to CSVs\n",
    "#these are 1X6 classification vectors\n",
    "\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Saves/\"\n",
    "print('Saving...')\n",
    "trainOut = pd.DataFrame(data= TrainModel_outputs )\n",
    "trainOut.to_csv(SavesDirectory+'trainOut.tsv', sep='\\t',  index=False)\n",
    "\n",
    "evalOut = pd.DataFrame(data= EvalModel_outputs )\n",
    "evalOut.to_csv(SavesDirectory+'evalOut.tsv', sep='\\t',  index=False)\n",
    "\n",
    "testOut = pd.DataFrame(data= TestModel_outputs )\n",
    "testOut.to_csv(SavesDirectory+'testOut.tsv', sep='\\t',  index=False)\n",
    "\n",
    "print('Saving Complete on',datetime.now() ,'in:', SavesDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)\n",
    "#del(train,Eval,test)\n",
    "del(trainOut,evalOut,testOut)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Adding the reputation vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section takes the output results from the transformer used above and uses it together with the speaker's reputation to enhance the classification.\n",
    "\n",
    "Before running this section it is suggested that you halt the program and start running it again from this cell. The neural net will likely have an error caused by some unreleased variable used by thr simple transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PantsTotal</th>\n",
       "      <th>NotRealTotal</th>\n",
       "      <th>BarelyTotal</th>\n",
       "      <th>HalfTotal</th>\n",
       "      <th>MostlyTotal</th>\n",
       "      <th>RealTotal</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.405273</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>-0.474121</td>\n",
       "      <td>-0.929688</td>\n",
       "      <td>-0.497314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082458</td>\n",
       "      <td>0.471680</td>\n",
       "      <td>0.402588</td>\n",
       "      <td>0.063293</td>\n",
       "      <td>-0.761230</td>\n",
       "      <td>-0.521484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.357910</td>\n",
       "      <td>0.583984</td>\n",
       "      <td>0.496826</td>\n",
       "      <td>0.019791</td>\n",
       "      <td>-0.104431</td>\n",
       "      <td>-0.106079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.498047</td>\n",
       "      <td>0.184692</td>\n",
       "      <td>0.141968</td>\n",
       "      <td>0.796387</td>\n",
       "      <td>0.678223</td>\n",
       "      <td>0.501953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.678711</td>\n",
       "      <td>0.028656</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>0.613770</td>\n",
       "      <td>1.092773</td>\n",
       "      <td>0.895020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10094</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.728516</td>\n",
       "      <td>0.396484</td>\n",
       "      <td>0.320557</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.066589</td>\n",
       "      <td>-0.038055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10095</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.663086</td>\n",
       "      <td>0.326660</td>\n",
       "      <td>0.301758</td>\n",
       "      <td>0.840332</td>\n",
       "      <td>0.741699</td>\n",
       "      <td>0.410400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10096</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.478516</td>\n",
       "      <td>0.354004</td>\n",
       "      <td>0.528809</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.115173</td>\n",
       "      <td>-0.348633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10097</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.354004</td>\n",
       "      <td>0.335693</td>\n",
       "      <td>0.586426</td>\n",
       "      <td>0.459229</td>\n",
       "      <td>-0.136353</td>\n",
       "      <td>-0.257568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10098</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.899414</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>-0.303711</td>\n",
       "      <td>0.242554</td>\n",
       "      <td>0.462646</td>\n",
       "      <td>0.690918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10099 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PantsTotal  NotRealTotal  BarelyTotal  HalfTotal  MostlyTotal  \\\n",
       "0           0.005         0.000         0.00      0.000        0.000   \n",
       "1           0.005         0.000         0.01      0.000        0.000   \n",
       "2           0.005         0.000         0.01      0.000        0.000   \n",
       "3           0.000         0.000         0.00      0.000        0.005   \n",
       "4           0.000         0.000         0.00      0.000        0.005   \n",
       "...           ...           ...          ...        ...          ...   \n",
       "10094       0.000         0.005         0.00      0.000        0.010   \n",
       "10095       0.000         0.005         0.00      0.000        0.010   \n",
       "10096       0.000         0.005         0.00      0.000        0.010   \n",
       "10097       0.000         0.000         0.00      0.005        0.000   \n",
       "10098       0.000         0.000         0.00      0.000        0.005   \n",
       "\n",
       "       RealTotal         0         1         2         3         4         5  \n",
       "0            0.0  0.767578  0.405273  0.121094 -0.474121 -0.929688 -0.497314  \n",
       "1            0.0  0.082458  0.471680  0.402588  0.063293 -0.761230 -0.521484  \n",
       "2            0.0 -0.357910  0.583984  0.496826  0.019791 -0.104431 -0.106079  \n",
       "3            0.0 -1.498047  0.184692  0.141968  0.796387  0.678223  0.501953  \n",
       "4            0.0 -1.678711  0.028656 -0.227539  0.613770  1.092773  0.895020  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "10094        0.0 -0.728516  0.396484  0.320557  0.609375  0.066589 -0.038055  \n",
       "10095        0.0 -1.663086  0.326660  0.301758  0.840332  0.741699  0.410400  \n",
       "10096        0.0 -0.478516  0.354004  0.528809  0.556152  0.115173 -0.348633  \n",
       "10097        0.0 -0.354004  0.335693  0.586426  0.459229 -0.136353 -0.257568  \n",
       "10098        0.0 -0.899414  0.102600 -0.303711  0.242554  0.462646  0.690918  \n",
       "\n",
       "[10099 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train=pd.read_excel('train-clean-Reputation.xlsx' )\n",
    "train=train.iloc[:,:-1].astype(float)\n",
    "train=train/200  #for scaling\n",
    "#train\n",
    "\n",
    "model_class='bert'  # bert or roberta or albert\n",
    "model_version='bert-base-cased' #bert-base-cased, roberta-base, roberta-large, albert-base-v2 OR albert-large-v2\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Saves/\"\n",
    "TF_Output=pd.read_csv( SavesDirectory+'trainOut.tsv', sep='\\t')\n",
    "\n",
    "train=pd.concat([train,TF_Output], axis=1)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10094</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10095</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10096</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10097</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10098</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10099 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4  5\n",
       "0      1  0  0  0  0  0\n",
       "1      1  0  0  0  0  0\n",
       "2      0  0  1  0  0  0\n",
       "3      0  0  0  0  1  0\n",
       "4      0  0  0  0  1  0\n",
       "...   .. .. .. .. .. ..\n",
       "10094  0  1  0  0  0  0\n",
       "10095  0  0  0  0  1  0\n",
       "10096  0  0  0  0  1  0\n",
       "10097  0  0  0  1  0  0\n",
       "10098  0  0  0  0  1  0\n",
       "\n",
       "[10099 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLables=pd.read_excel('train-clean-Reputation.xlsx' )\n",
    "TrainLables=TrainLables.iloc[:,-1] \n",
    "\n",
    "TrainLables=pd.get_dummies(TrainLables)\n",
    "TrainLables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0050,  0.0000,  0.0000,  ..., -0.4741, -0.9297, -0.4973],\n",
       "        [ 0.0050,  0.0000,  0.0100,  ...,  0.0633, -0.7612, -0.5215],\n",
       "        [ 0.0050,  0.0000,  0.0100,  ...,  0.0198, -0.1044, -0.1061],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0050,  0.0000,  ...,  0.5562,  0.1152, -0.3486],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4592, -0.1364, -0.2576],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.2426,  0.4626,  0.6909]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.tensor(train.values)\n",
    " \n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets=torch.tensor(TrainLables.astype(float).values)\n",
    " \n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: 12\n",
      "output size: 6\n"
     ]
    }
   ],
   "source": [
    " \n",
    "size= torch.tensor(input[0].size())\n",
    "InputSize=size.item()\n",
    "\n",
    "OutputSize=torch.tensor(targets[0].size()).item()\n",
    "\n",
    "print('input size:', InputSize)\n",
    "print('output size:', OutputSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "         \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(InputSize, 24)   \n",
    "        self.fc2 = nn.Linear(24, 12)\n",
    "        self.fc3 = nn.Linear(12, OutputSize)  #classifies 'outputsize' different classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x)) \n",
    "        x = torch.tanh(self.fc3(x)).double()\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "#now we use it\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we  setup the neural network parameters\n",
    "# pick an optimizer (Simple Gradient Descent)\n",
    "\n",
    "learning_rate = 9e-4\n",
    "criterion = nn.MSELoss()  #computes the loss Function\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# creating optimizer\n",
    "#optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 0\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 3\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 4\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 5\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 6\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 7\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 8\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 9\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 10\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 11\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 12\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 13\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 14\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 15\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 16\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 17\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 18\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 19\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 20\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 21\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 22\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 23\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 24\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 25\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 26\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 27\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 28\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 29\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 30\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 31\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 32\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 33\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 34\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 35\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 36\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 37\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 38\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 39\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 40\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 41\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 42\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 43\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 44\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 45\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 46\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 47\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 48\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 49\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 50\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 51\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 52\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 53\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 54\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 55\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 56\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 57\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 58\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 59\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 60\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 61\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 62\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 63\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 64\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 65\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 66\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 67\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 68\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 69\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 70\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 71\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 72\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 73\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 74\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 75\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 76\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 77\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 78\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 79\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 80\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 81\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 82\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 83\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 84\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 85\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 86\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 87\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 88\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 89\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 90\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 91\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 92\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 93\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 94\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 95\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 96\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 97\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 98\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 99\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 100\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 101\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 102\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 103\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 104\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 106\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 107\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 108\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 109\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 110\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 111\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 112\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 113\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 114\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 115\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 116\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 117\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 118\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 119\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 120\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 121\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 122\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 123\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 124\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 125\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 126\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 127\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 128\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 129\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 130\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 131\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 132\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 133\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 134\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 135\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 136\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 137\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 138\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 139\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 140\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 141\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 142\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 143\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 144\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 145\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 146\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 147\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 148\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 149\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 150\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 151\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 152\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 153\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 154\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 155\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 156\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 157\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 158\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 159\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 160\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 161\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 162\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 163\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 164\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 165\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 166\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 167\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 168\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 169\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 170\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 171\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 172\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 173\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 174\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 175\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 176\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 177\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 178\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 179\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 180\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 181\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 182\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 183\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 184\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 185\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 186\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 187\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 188\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 189\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 190\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 191\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 192\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 193\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 194\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 195\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 196\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 197\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 198\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 199\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 200\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 201\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 202\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 203\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 204\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 205\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 206\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 207\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 208\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 209\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 210\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 212\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 213\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 214\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 215\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 216\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 217\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 218\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 219\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 220\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 221\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 222\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 223\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 224\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 225\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 226\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 227\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 228\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 229\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 230\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 231\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 232\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 233\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 234\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 235\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 236\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 237\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 238\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 239\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 240\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 241\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 242\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 243\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 244\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 245\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 246\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 247\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 248\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 249\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 250\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 251\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 252\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 253\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 254\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 255\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 256\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 257\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 258\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 259\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 260\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 261\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 262\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 263\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 264\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 265\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 266\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 267\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 268\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 269\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 270\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 271\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 272\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 273\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 274\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 275\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 276\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 277\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 278\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 279\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 280\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 281\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 282\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 283\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 284\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 285\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 286\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 287\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 288\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 289\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 290\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 291\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 292\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 293\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 294\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 295\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 296\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 297\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 298\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 299\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 300\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 301\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 302\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 303\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 304\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 305\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 306\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 307\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 308\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 309\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 310\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 311\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 312\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 313\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 315\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 316\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 317\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 318\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 319\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 320\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 321\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 322\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 323\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 324\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 325\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 326\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 327\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 328\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 329\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 330\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 331\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 332\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 333\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 334\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 335\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 336\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 337\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 338\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 339\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 340\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 341\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 342\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 343\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 344\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 345\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 346\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 347\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 348\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 349\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 350\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 351\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 352\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 353\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 354\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 355\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 356\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 357\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 358\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 359\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 360\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 361\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 362\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 363\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 364\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 365\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 366\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 367\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 368\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 369\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 370\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 371\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 372\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 373\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 374\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 375\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 376\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 377\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 378\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 379\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 380\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 381\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 382\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 383\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 384\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 385\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 386\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 387\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 388\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 389\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 390\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 391\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 392\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 393\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 394\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 395\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 396\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 397\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 398\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 399\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 400\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 401\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 402\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 403\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 404\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 405\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 406\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 407\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 408\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 409\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 410\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 411\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 412\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 413\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 414\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 415\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 416\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 417\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 418\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 419\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 420\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 421\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 422\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 423\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 424\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 425\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 426\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 427\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 428\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 430\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 431\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 432\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 433\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 434\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 435\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 436\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 437\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 438\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 439\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 440\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 441\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 442\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 443\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 444\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 445\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 446\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 447\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 448\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 449\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 450\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 451\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 452\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 453\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 454\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 455\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 456\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 457\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 458\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 459\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 460\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 461\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 462\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 463\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 464\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 465\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 466\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 467\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 468\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 469\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 470\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 471\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 472\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 473\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 474\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 475\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 476\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 477\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 478\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 479\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 480\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 481\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 482\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 483\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 484\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 485\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 486\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 487\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 488\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 489\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 490\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 491\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 492\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 493\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 494\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 495\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 496\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 497\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 498\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 499\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):  \n",
    "        \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net(input.float())\n",
    "\n",
    "    loss = criterion(output, targets)\n",
    "    print('Loss:', loss, ' at epoch:', epoch)\n",
    "\n",
    "    loss.backward()  #backprop\n",
    "    optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load previously saved FCNN model \n",
    "\n",
    "stage='NNetwork6WayClass/'\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/\"+stage\n",
    "#PATH = SavesDirectory+'4885.pth'\n",
    "\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 2 4 4 5 3 4 1 4 5 5 4 4 3 3 3 2 3 3 1 1 3 1 1 1 5 1 4 3 5 1 1 4 1 0 3 3 3 4 2 3 2 1 4 2 4 1 1 2 5 5 1 1 5 5 5 1 1 2 1 2 5 2 5 5 5 5 5 1 5 5 5 5 5 1 5 5 5 5 2 2 2 5 4 5 5 5 3 5 3 5 3 4 1 0 0 0 0 5 2 5 2 1 5 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 4 3 3 3 3 1 1 4 1 3 3 3 1 3 3 3 5 1 4 4 2 4 1 2 1 2 5 4 1 4 5 5 1 1 2 2 1 2 1 1 1 2 1 1 5 1 1 1 1 1 3 3 2 2 2 2 2 2 5 4 4 4 4 0 2 2 2 3 3 4 4 4 4 4 4 2 2 2 3 3 3 3 3 3 2 2 2 3 1 3 0 2 2 2 2 4 4 4 4 1 3 0 0 4 5 3 3 5 2 3 3 1 2 2 2 1 1 1 1 1 2 2 1 1 2 2 2 3 3 3 3 4 4 4 2 2 2 2 2 2 3 1 3 3 3 1 3 1 1 2 3 3 3 3 2 2 2 5 2 4 4 4 3 1 3 3 4 3 3 0 1 5 5 5 5 5 3 3 0 0 0 3 1 4 3 4 1 3 2 4 4 4 3 3 3 4 1 2 2 1 2 2 3 3 3 3 1 3 3 2 5 5 4 4 4 3 2 5 3 3 3 3 3 3 3 3 4 2 1 1 4 1 4 3 4 2 3 2 3 3 0 4 4 3 4 4 3 3 1 1 4 2 4 3 3 4 4 3 4 4 4 1 4 4 1 4 4 1 4 5 4 1 2 3 3 5 5 5 5 4 5 5 1 4 4 4 4 1 4 2 5 2 2 5 5 3 1 1 1 1 4 2 4 5 4 4 3 3 4 3 4 4 3 1 1 3 4 3 3 4 1 1 4 4 4 3 4 2 3 4 4 5 3 4 4 3 4 4 5 3 3 4 3 4 4 1 1 3 4 4 4 1 3 1 3 4 3 4 3 1 3 4 4 4 5 1 3 4 3 3 4 4 4 3 4 3 3 4 4 3 3 1 1 3 3 1 3 4 3 4 4 4 3 4 4 4 1 3 3 4 3 4 4 1 5 4 3 3 3 1 4 1 4 4 3 3 3 5 4 3 4 1 3 3 4 3 3 1 4 3 3 3 3 1 4 4 3 3 4 3 4 1 4 1 3 4 4 4 4 4 1 3 4 1 4 4 4 3 1 4 3 1 1 1 1 2 4 3 1 3 1 1 1 1 4 5 1 3 4 4 3 3 3 1 1 4 3 4 4 4 4 3 3 4 3 3 4 4 4 3 4 1 1 3 4 4 4 4 3 4 4 3 3 1 3 4 3 1 3 4 4 3 4 1 4 4 4 3 3 1 4 4 1 3 4 3 3 4 3 4 1 1 3 3 1 3 4 4 4 3 4 1 4 4 3 3 4 4 3 3 4 3 3 4 4 4 1 4 3 4 4 3 3 4 5 3 4 4 3 4 4 3 4 3 4 3 4 4 3 4 4 3 4 3 3 4 3 4 3 3 4 4 4 3 4 1 3 3 4 4 1 3 4 4 3 4 4 4 4 3 4 4 3 4 3 3 4 4 4 3 4 5 3 3 4 3 4 3 3 1 3 3 4 3 3 3 3 4 3 4 1 4 4 3 4 3 4 3 4 1 4 4 4 3 4 4 3 3 3 4 4 3 4 3 3 3 4 5 4 4 3 3 4 3 4 5 1 4 4 3 3 3 3 4 4 4 4 4 4 4 3 2 3 4 4 4 4 4 4 4 1 4 3 4 4 4 5 5 4 4 4 2 4 2 3 4 3 4 1 3 4 3 1 1 3 4 1 3 4 4 4 3 4 4 4 4 3 3 4 3 4 5 4 1 4 4 4 3 3 4 4 4 3 4 4 3 3 4 3 4 4 3 1 3 3 3 3 4 4 3 3 4 4 0 3 4 4 2 1 4 3 4 4 5 5 3 2 2 3 3 3 3 4 2 5 1 2 4 4 2 3 3 3 5 4 4 1 5 4 4 1 1 1 4 2 2 5 0 3 2 1 1 4 1 1 4 4 0 3 5 5 5 5 1 1 5 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 5 1 1 1 4 4 4 4 4 4 4 4 4 4 1 4 4 4 4 1 4 1 4 3 4 4 4 4 4 3 4 4 1 1 4 4 4 1 4 4 4 2 4 2 4 4 4 2 3 3 4 1 4 3 4 4 4 4 4 3 1 4 4 2 2 3 4 4 3 5 4 4 4 4 4 4 4 2 4 4 4 4 4 3 3 4 1 4 3 4 4 4 3 3 3 0 0 1 2 2 3 5 1 4 4 4 4 4 1 1 3 4 2 5 5 0 0 5 5 0 5 4 4 2 5 5 5 0 0 3 5 3 3 5 3 5 3 5 3 3 3 1 3 5 3 5 5 3 5 5 5 5 3 5 3 3 3 3 3 3 2 2 4 3 3 4 4 4 5 4 4 3 5 3 0 2 2 2 5 4 4 4 4 0 2 4 1 1 4 4 4 4 4 4 4 5 1 2 1 3 1 1 5 5 3 5 4 3 3 3 5 5 5 3 3 5 4 4 3 3 5 3 3 3 3 3 5 3 3 3 0 1 1 1 1 1 5 1 1 1 1 1 5 1 1 1 2 4 4 4 4 4 3 4 4 1 4 0 1 2 4 4 4 4 1 4 4 4 3 4 4 4 4 4 3 3 1 5 5 3 3 3 1 3 1 5 2 5 1 5 5 5 5 5 1 5 5 4 5 1 2 2 0 0 0 1 2 0 1 0 1 0 2 1 0 0 0 0 0 1 2 1 0 2 2 1 2 0 2 0 1 0 1 1 0 0 0 0 0 1 1 1 2 0 0 2 0 1 0 0 1 2 0 0 0 1 0 0 0 1 1 3 5 4 5 1 5 5 4 1 3 3 3 3 3 3 3 3 3 4 3 3 1 4 4 4 4 4 4 4 4 4 2 4 4 5 5 4 5 5 4 5 5 5 5 3 3 3 5 3 1 5 5 3 4 2 2 1 1 1 1 5 3 3 3 5 5 5 5 5 3 5 5 3 1 5 5 5 1 3 3 5 2 5 5 1 1 5 5 5 5 5 5 5 2 4 1 2 1 1 1 3 5 0 4 4 3 3 3 1 0 0 2 0 4 4 4 1 1 1 3 1 1 2 1 0 0 1 3 5 5 5 5 2 5 5 1 1 1 1 1 4 4 1 5 5 5 1 1 1 3 4 5 2 2 4 4 3 0 0 3 3 1 1 1 2 5 3 2 3 1 1 1 5 5 5 2 2 5 3 1 0 2 3 3 4 1 5 5 5 5 3 2 2 4 3 3 2 5 5 0 5 2 4 2 3 2 2 0 4 4 1 2 5 5 5 1 5 4 2 5 2 2 2 2 2 4 1 1 4 4 1 1 1 2 5 1 1 2 1 1 1 5 1 1 1 1 1 1 2 2 1 3 4 3 5 4 5 3 3 3 5 2 3 4 4 4 4 5 1 5 5 5 1 5 3 0 0 3 3 1 3 4 4 3 4 4 3 0 5 3 4 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 5 0 2 2 2 2 2 3 1 3 2 2 4 2 0 3 1 1 4 4 4 3 4 3 3 1 5 2 3 3 5 3 3 5 2 5 2 2 3 5 3 5 3 2 2 3 2 5 3 5 2 3 5 3 5 3 3 2 3 3 3 5 2 3 3 1 2 5 3 3 2 3 3 3 3 5 3 3 2 3 3 3 3 3 3 3 3 3 3 3 2 3 3 5 5 5 5 5 5 5 5 3 0 0 1 3 3 4 4 5 4 4 5 3 3 2 1 1 1 5 5 4 5 5 1 1 1 1 1 3 3 1 1 3 3 3 3 2 1 1 0 5 5 3 5 3 3 5 3 3 3 1 1 5 5 5 3 5 5 1 3 1 5 5 3 3 5 1 5 5 5 3 5 3 5 3 3 5 5 3 1 1 1 3 3 3 5 3 5 5 3 5 5 3 3 5 3 3 5 5 3 3 3 3 1 5 5 5 1 1 3 3 1 1 1 1 2 1 5 2 1 4 3 3 3 3 4 4 3 4 0 1 3 3 3 1 1 4 3 1 1 1 1 2 2 1 1 1 1 5 0 3 1 3 1 1 1 2 5 5 4 4 4 4 4 4 5 4 3 3 0 0 5 5 5 0 3 5 1 0 0 1 4 5 5 5 1 3 3 0 1 0 4 4 4 3 3 2 0 3 1 3 5 1 3 2 5 5 5 5 5 5 1 2 3 3 3 3 3 2 3 1 5 0 3 3 3 5 5 2 5 5 3 3 1 2 2 1 4 3 3 3 2 1 4 4 1 2 3 3 3 3 3 3 2 2 1 1 1 3 1 1 5 1 1 3 1 2 3 2 1 2 5 0 4 3 4 3 4 2 3 4 5 5 2 2 3 5 4 4 4 0 0 5 2 2 0 3 3 4 4 4 0 4 4 3 4 5 1 3 2 2 3 5 1 2 1 1 5 4 1 2 4 5 4 3 2 0 2 2 2 2 2 1 4 1 1 1 2 2 3 1 4 5 1 5 1 5 5 5 5 5 5 1 5 5 4 5 4 5 5 5 5 1 2 2 2 2 2 4 3 2 0 3 3 0 2 2 4 3 2 5 0 2 2 1 2 2 2 2 2 2 2 2 2 3 2 2 4 3 1 2 0 5 5 4 0 1 1 2 5 4 3 4 4 2 2 4 1 1 2 2 0 0 2 5 3 3 1 3 3 0 0 0 0 5 1 4 4 4 4 2 4 4 1 0 4 1 4 4 1 4 4 0 0 3 3 2 1 4 5 4 3 3 4 2 5 2 1 1 2 3 5 2 2 5 5 1 5 1 5 3 5 1 2 2 2 2 2 1 1 1 2 5 5 5 5 1 1 1 4 4 1 1 1 4 1 5 5 5 4 3 4 2 2 2 2 3 0 1 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 4 3 2 4 3 5 2 3 5 5 5 5 4 5 5 2 2 5 2 5 2 5 2 5 2 3 3 5 5 5 2 5 5 5 2 5 5 5 5 5 1 3 1 1 1 3 1 1 1 3 3 3 4 2 1 5 1 3 1 3 2 5 3 3 3 5 3 3 2 1 2 1 3 2 3 2 2 2 2 2 5 1 4 4 3 4 2 2 5 2 5 2 2 2 5 2 3 4 4 5 4 4 4 2 4 2 3 2 2 4 3 3 2 4 3 5 5 1 1 3 3 1 1 4 3 3 2 3 2 1 1 5 5 5 4 5 5 5 4 5 2 5 5 5 5 1 4 4 1 1 4 1 5 1 5 1 3 1 3 3 3 1 5 1 5 1 1 5 5 1 5 5 1 1 5 3 5 5 3 2 2 1 1 2 5 3 1 2 5 3 5 4 3 3 3 3 1 3 3 3 5 5 5 5 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 0 5 0 0 2 3 3 3 5 5 3 3 5 3 3 5 3 3 3 3 5 3 3 2 5 3 2 3 3 3 2 3 3 3 4 4 4 4 4 4 3 3 1 3 1 3 3 3 5 5 3 3 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 3 1 4 4 1 4 1 1 5 5 4 3 4 4 1 5 4 5 4 4 4 1 1 1 0 1 2 2 1 1 1 1 2 1 1 2 2 1 1 3 1 1 5 0 0 4 4 0 4 4 3 4 3 1 4 3 3 1 5 2 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 3 1 1 1 4 1 1 1 1 4 1 3 1 1 1 1 4 1 1 1 1 3 1 3 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 4 1 1 1 1 1 1 1 1 1 1 1 1 4 1 1 1 1 4 1 1 1 1 1 4 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 4 4 1 1 1 2 4 1 2 1 1 1 1 1 1 1 3 3 3 1 1 4 2 1 1 1 2 1 2 1 3 3 1 4 1 3 1 1 2 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 2 3 1 2 3 1 1 1 1 4 2 3 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 3 1 2 1 2 4 1 1 1 1 1 2 1 1 1 1 1 2 3 1 1 2 1 1 2 1 1 1 1 4 1 1 4 1 1 3 1 1 1 2 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 0 1 4 4 4 4 4 3 4 4 5 3 3 3 3 3 3 4 1 5 3 1 3 4 3 4 4 4 4 3 4 1 5 1 0 5 4 5 5 4 5 3 1 1 2 3 5 3 3 3 5 1 1 2 2 4 4 4 4 4 4 1 5 1 4 3 1 2 2 1 5 1 2 3 3 1 5 3 3 0 2 2 4 5 5 4 1 4 1 5 4 0 4 2 1 3 3 3 1 3 5 5 5 5 5 1 5 5 5 5 5 4 4 5 4 5 4 4 1 1 3 5 1 3 3 1 1 2 2 3 5 1 5 5 5 5 0 3 3 3 3 3 3 3 3 2 3 3 0 3 4 4 4 3 3 3 4 3 3 3 3 1 1 1 4 1 4 3 5 3 3 5 5 1 2 2 5 3 4 3 5 3 5 4 4 3 3 4 3 4 2 5 0 5 5 2 3 1 4 2 2 2 3 3 3 3 3 3 1 3 3 4 2 3 2 2 2 2 1 3 2 3 4 2 2 1 2 1 2 2 5 2 1 1 2 5 5 1 2 5 1 2 1 2 1 4 3 3 3 3 5 5 5 3 3 3 3 3 5 1 1 1 5 0 4 1 1 1 4 3 5 4 5 4 0 4 1 3 3 5 1 5 0 2 2 2 2 2 3 3 3 1 2 3 3 3 1 4 1 3 3 3 3 3 1 3 2 2 3 1 1 1 2 1 1 3 3 1 1 2 1 2 2 1 2 1 3 1 3 3 2 1 3 3 5 1 1 4 3 3 3 2 2 3 2 1 2 1 1 2 1 1 2 1 2 1 3 2 3 2 2 2 2 2 1 2 1 0 5 4 2 3 1 3 0 3 3 4 1 2 4 2 1 4 0 4 4 4 2 3 4 3 2 2 2 2 2 2 2 1 2 3 1 1 2 2 2 3 2 4 1 2 4 3 0 3 2 1 1 1 1 4 1 1 1 4 3 1 2 3 3 3 3 3 3 3 3 4 4 3 4 1 3 5 5 3 3 5 3 3 2 3 1 3 0 2 1 5 1 3 3 3 4 4 5 3 2 0 1 4 1 5 5 5 1 5 5 5 1 1 5 1 5 1 2 0 1 1 2 1 3 3 4 1 2 2 2 5 4 5 4 2 2 4 2 2 2 2 4 4 1 2 3 5 5 5 4 4 1 2 4 3 4 1 3 4 3 3 4 3 4 3 3 4 1 0 0 3 5 5 0 1 4 4 3 2 4 4 4 4 3 4 3 2 5 4 1 5 1 5 5 5 5 5 5 5 1 1 5 5 5 1 5 1 1 2 5 3 0 0 3 3 3 3 3 3 3 3 3 3 2 3 3 2 1 2 4 4 5 5 5 1 4 5 3 5 5 5 5 1 5 2 5 5 5 5 1 5 5 5 5 2 1 5 5 1 5 5 1 5 3 3 2 4 3 4 1 3 4 3 2 5 4 2 3 3 1 5 3 5 1 1 3 3 3 3 3 3 1 1 3 1 1 4 1 1 3 4 5 5 4 5 2 5 5 5 5 5 5 5 5 3 3 3 0 4 1 2 1 1 2 1 1 1 5 3 2 1 3 2 1 1 2 2 3 1 1 4 1 1 2 3 1 1 5 1 4 4 1 2 1 3 5 1 4 1 3 3 0 0 0 0 0 0 0 4 0 3 3 2 1 2 1 1 3 2 1 3 2 2 5 2 2 5 2 2 2 5 5 2 2 5 2 2 2 2 2 5 5 5 5 2 2 5 2 5 5 2 5 5 2 5 2 4 1 2 2 2 2 1 4 0 0 1 4 4 1 5 3 1 1 4 5 5 5 5 2 2 1 5 3 5 1 3 1 2 5 3 1 5 1 2 2 3 3 3 1 3 3 3 4 3 3 3 3 4 3 0 1 1 1 1 1 1 1 1 2 2 1 2 1 5 1 1 1 5 5 1 5 5 2 2 2 1 3 5 5 5 2 0 5 5 4 2 5 5 1 5 5 5 2 1 3 4 4 2 4 3 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 3 4 0 4 3 4 4 4 4 4 3 4 5 3 3 3 3 4 5 1 3 5 5 5 1 1 5 4 3 4 3 5 4 3 5 3 3 4 5 4 4 2 4 4 4 2 4 2 5 5 4 4 2 3 4 5 5 5 4 3 4 3 4 3 4 4 4 4 4 5 4 3 4 5 4 4 4 4 4 4 3 4 2 5 4 5 4 4 4 4 4 4 4 3 3 5 3 4 5 4 5 1 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 5 4 4 4 4 3 5 3 4 4 1 3 3 4 5 4 4 4 3 4 4 5 1 4 4 5 4 4 4 5 4 4 3 4 4 4 4 1 1 4 3 4 1 5 4 5 1 5 1 3 4 3 4 4 5 3 4 3 4 4 5 4 3 4 4 4 4 4 3 4 4 4 4 2 5 3 3 3 5 4 3 4 2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4 5 3 4 4 5 3 4 5 5 3 3 4 2 1 1 5 4 1 4 4 4 3 4 5 5 4 4 4 4 4 1 4 3 3 4 4 3 1 4 4 4 4 1 1 2 2 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 5 1 4 3 5 5 4 5 0 0 5 5 4 3 2 4 1 5 1 1 1 3 1 2 2 5 4 4 5 4 5 4 3 4 4 3 2 2 5 4 4 3 1 5 5 3 5 5 5 5 1 4 2 5 5 4 4 5 2 1 5 1 3 2 4 2 4 1 2 4 3 4 4 4 4 1 3 1 4 5 5 5 5 3 0 0 5 3 4 5 5 5 4 3 5 3 2 2 2 2 2 2 4 1 4 1 3 3 4 3 4 3 3 3 4 4 4 5 5 3 5 4 1 5 5 3 0 1 4 2 2 5 2 4 2 5 5 2 3 1 4 4 4 2 4 2 5 5 0 2 4 4 5 5 4 2 4 5 5 1 3 4 0 0 4 0 0 4 1 3 3 3 3 3 3 3 3 1 1 4 5 5 3 5 5 2 5 3 4 3 2 5 5 4 5 5 2 5 3 4 3 2 2 1 3 5 2 5 3 3 3 5 2 2 5 5 2 2 3 5 1 5 2 3 3 3 5 5 5 5 3 5 3 5 5 2 5 2 5 2 5 5 1 1 1 1 1 4 4 3 3 5 5 4 3 1 3 4 1 4 4 4 4 4 4 2 4 1 4 4 2 4 5 5 3 3 3 3 3 3 2 3 4 5 5 5 5 0 0 0 3 3 4 1 4 4 4 4 4 3 4 4 3 2 4 4 4 3 3 1 3 3 4 3 4 5 5 5 5 3 3 3 5 5 0 0 4 4 5 1 1 0 3 3 3 3 3 2 2 2 4 2 4 4 4 4 5 1 4 1 5 3 5 3 3 3 3 5 1 0 4 4 1 4 4 3 4 4 4 1 4 2 5 0 0 0 5 4 3 3 2 2 1 5 2 1 5 2 5 1 1 1 3 3 4 3 5 2 2 1 4 3 4 3 3 4 4 4 3 5 5 5 5 5 1 0 3 3 3 2 3 3 2 5 4 4 4 4 4 4 4 4 4 4 4 4 2 5 3 2 1 4 3 4 2 2 2 4 3 3 3 3 3 3 2 1 3 2 3 1 4 1 1 5 1 1 5 1 1 1 3 3 3 3 3 5 2 4 4 4 4 4 4 3 3 3 5 5 4 4 4 4 4 2 3 4 2 4 5 0 2 3 2 2 2 2 2 3 3 3 3 3 3 5 3 2 5 5 2 5 1 5 5 3 3 5 3 3 3 3 3 3 3 3 5 5 3 3 3 1 1 1 3 3 1 5 3 3 3 3 5 3 3 3 3 3 3 5 5 3 5 5 3 1 3 3 3 1 3 0 3 4 2 1 1 2 3 3 1 4 4 4 4 5 3 5 4 5 1 3 3 3 5 4 4 5 5 4 1 1 3 5 3 4 1 1 4 1 2 3 3 3 3 3 3 1 1 1 1 1 1 5 4 2 2 4 5 1 1 1 1 1 1 1 5 1 1 3 3 1 1 1 1 3 5 1 1 5 5 5 5 1 1 3 1 1 3 1 5 5 3 1 1 1 5 5 1 5 1 1 5 1 1 3 1 5 1 4 3 4 5 2 4 3 1 1 4 1 1 1 4 1 4 4 4 3 1 1 5 5 2 1 5 5 5 1 2 3 5 5 5 5 3 5 5 5 5 3 5 1 0 1 1 1 2 1 2 5 5 4 4 5 5 3 3 3 3 5 1 3 3 3 1 5 1 1 1 1 4 1 5 1 5 5 5 1 5 3 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 3 1 5 5 5 5 5 1 5 1 5 3 5 5 5 5 5 5 5 1 3 4 4 4 4 4 1 4 4 5 1 3 3 3 5 3 5 5 1 1 1 4 5 3 0 3 4 1 1 1 1 5 5 3 3 3 4 3 4 4 2 3 2 4 5 1 1 4 1 4 1 1 5 5 4 3 1 1 1 3 1 4 3 1 1 1 3 4 1 1 5 1 1 3 5 4 5 4 5 1 3 1 1 1 4 5 4 3 5 1 1 1 1 1 3 4 1 5 3 5 1 1 5 1 1 3 4 2 1 3 3 1 1 1 1 1 5 5 5 1 1 1 1 4 2 3 4 1 1 1 5 4 1 5 4 4 4 1 2 5 3 3 1 1 1 1 1 4 1 1 1 1 3 5 1 1 4 1 1 5 1 3 5 5 3 3 1 3 4 5 4 1 1 4 5 4 1 4 4 3 3 1 3 2 2 1 5 3 5 5 5 3 3 3 5 5 3 1 5 4 4 5 5 5 3 3 5 3 5 5 2 0 0 3 5 5 5 5 5 5 3 2 2 1 1 1 3 0 5 2 1 2 2 2 0 3 4 1 1 5 1 5 5 1 5 5 3 4 5 5 1 0 0 2 2 2 3 3 2 3 3 3 2 2 3 4 3 3 2 4 4 4 4 4 1 2 2 3 4 0 0 3 3 4 3 1 3 3 1 3 4 1 1 1 1 1 4 5 1 5 5 5 3 1 4 4 4 4 3 3 4 3 5 5 4 3 2 2 5 1 1 3 1 1 3 3 4 4 3 2 1 2 1 1 1 4 0 3 2 1 5 4 4 5 1 5 5 3 3 5 2 1 1 1 2 1 5 2 5 2 1 5 5 2 5 5 4 0 3 5 3 4 5 3 2 3 4 2 3 2 3 2 2 5 5 5 3 2 2 4 3 3 3 0 3 1 1 2 1 5 3 5 5 5 3 5 5 5 4 3 3 4 3 1 0 2 4 5 1 1 4 3 4 4 2 3 1 3 1 1 2 2 4 2 4 4 1 3 1 5 1 1 1 4 2 4 4 1 4 4 4 4 4 4 5 5 3 3 5 3 3 5 5 5 5 5 3 5 3 2 2 0 0 2 4 4 5 5 1 3 3 3 1 3 3 3 3 5 1 1 4 5 4 2 4 4 4 4 4 4 1 3 3 4 5 3 3 2 2 4 5 0 2 5 5 5 5 5 5 1 1 1 5 5 5 5 3 1 3 3 5 4 4 4 4 2 0 1 1 5 5 5 5 3 1 3 3 1 1 3 5 2 1 2 2 1 1 2 1 4 4 4 4 4 4 4 2 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 5 1 1 5 1 1 1 5 1 1 1 1 1 1 2 2 2 2 1 2 3 4 1 4 3 3 4 0 1 1 1 1 1 4 3 5 5 2 5 2 2 5 5 4 4 4 1 4 4 5 2 3 1 2 2 1 4 5 5 3 0 0 1 1 1 2 2 4 2 3 3 3 5 5 4 4 4 4 1 5 4 2 4 5 5 2 2 3 3 3 3 2 4 2 3 3 5 5 1 5 5 2 1 2 1 1 5 5 5 1 1 3 5 1 5 1 2 1 1 1 5 1 3 1 1 5 4 5 4 1 1 1 1 1 1 3 4 0 0 1 3 5 4 1 4 1 4 4 1 4 5 5 3 3 5 5 1 1 4 3 4 2 4 2 4 3 1 1 4 1 5 3 5 5 3 3 3 3 4 4 4 0 1 2 4 5 2 3 1 5 4 4 5 3 1 5 3 0 2 1 3 1 1 1 1 3 3 3 5 3 3 1 3 3 3 2 1 3 3 2 4 4 0 3 1 1 3 3 5 5 5 4 3 2 1 1 1 1 1 1 5 1 5 5 5 3 5 5 2 1 1 5 4 4 4 1 3 1 3 1 3 3 3 3 3 0 2 4 2 3 3 1 4 5 4 4 5 4 4 5 4 4 4 2 2 2 3 2 2 0 3 3 3 1 3 5 5 5 5 5 3 5 5 5 5 1 5 5 3 4 1 4 1 1 4 4 3 3 4 4 1 1 1 1 1 1 1 1 4 4 0 4 0 2 2 4 4 4 4 4 2 4 0 3 5 5 5 5 3 4 4 4 0 0 4 4 4 4 4 4 4 4 3 5 5 1 5 2 3 3 5 4 4 4 4 4 4 3 4 4 1 5 5 5 5 4 5 1 4 3 2 5 3 2 2 2 4 3 2 2 2 5 4 4 3 3 1 3 4 3 5 2 4 3 3 4 2 3 5 3 5 3 2 2 2 2 1 1 2 3 5 1 1 2 4 3 1 3 4 1 5 4 4 4 2 1 4 3 1 2 3 4 3 2 3 4 1 4 1 3 2 2 4 3 4 2 3 2 4 4 3 3 4 5 3 3 3 3 3 1 1 2 3 4 3 3 3 5 1 3 3 5 2 1 1 2 5 1 4 5 5 3 4 4 3 3 3 3 5 5 3 5 3 4 1 3 5 5 2 5 3 2 3 3 3 5 4 4 4 4 4 2 3 4 4 4 3 0 3 1 3 5 4 2 4 1 1 4 5 5 1 5 5 5 3 3 3 5 2 5 1 3 2 1 3 1 2 1 4 4 4 1 1 5 5 5 4 4 5 2 2 1 1 4 5 5 1 2 2 5 5 3 5 5 3 3 3 5 5 5 5 3 5 3 5 5 1 5 5 5 2 4 1 2 2 2 5 5 2 4 3 2 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 2 1 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 1 5 3 3 3 3 3 3 3 3 1 3 3 1 5 4 2 4 4 4 5 5 5 2 2 5 3 3 3 3 3 3 2 3 4 3 3 4 4 5 2 3 4 4 4 4 4 1 4 2 5 2 2 2 4 4 2 2 2 3 5 5 3 1 4 5 3 5 2 1 1 3 5 5 5 3 5 0 5 1 1 4 1 5 5 5 1 1 1 5 5 5 5 5 1 5 3 5 4 2 2 2 1 4 1 4 4 0 5 2 5 1 1 1 3 5 3 3 3 5 2 0 4 2 2 4 5 1 4 2 4 4 4 3 1 1 4 4 4 1 4 1 3 1 1 4 4 3 5 1 2 2 5 2 2 2 2 1 1 0 0 4 5 5 5 5 5 5 5 3 3 3 3 3 5 2 4 3 3 3 2 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 5 1 1 1 3 4 2 2 2 4 4 4 4 4 4 3 4 4 4 5 3 3 2 0 0 5 1 1 1 3 2 3 3 3 3 3 3 3 3 5 3 5 5 4 4 4 4 1 5 3 5 5 1 1 1 1 5 2 5 2 3 5 2 5 2 2 2 2 2 2 2 2 3 3 2 2 5 2 2 2 2 2 2 2 2 2 5 2 2 3 2 5 2 2 0 1 3 3 3 4 5 5 5 5 5 5 5 3 3 1 3 3 1 3 1 1 1 1 3 3 2 3 3 1 3 3 3 3 3 3 3 1 2 1 4 3 5 0 3 4 4 4 4 4 4 3 2 5 0 5 5 0 0 3 3 3 3 4 3 3 5 4 4 4 4 1 5 1 2 1 3 1 4 5 1 1 1 1 3 5 1 1 2 2 1 1 2 5 2 1 5 2 1 2 3 1 2 5 2 1 2 3 5 5 5 2 3 1 1 3 1 3 5 1 1 3 2 3 1 2 2 5 5 2 3 1 2 1 3 1 3 3 5 3 3 3 5 2 1 2 3 3 2 3 5 1 3 3 3 3 1 1 3 3 3 3 3 3 3 5 3 3 3 2 1 2 2 3 1 5 1 3 3 3 3 1 1 3 1 3 3 5 1 3 1 2 3 3 2 2 5 3 3 2 3 1 1 5 1 2 3 3 3 3 3 3 3 5 3 3 3 3 3 3 1 3 1 2 2 3 1 2 1 1 1 2 3 1 2 1 3 1 5 1 2 3 1 2 3 3 3 3 3 4 3 4 1 1 3 3 2 3 5 3 3 2 2 5 5 5 2 3 3 2 3 3 1 2 2 5 5 3 3 3 2 4 1 4 1 1 1 1 0 2 2 1 2 4 5 5 1 5 5 3 2 5 3 5 5 5 1 1 5 0 0 0 4 5 1 0 0 5 4 4 4 0 5 4 1 2 2 3 5 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1 5 1 3 0 5 5 5 4 4 4 4 2 2 2 2 3 5 5 5 1 5 2 5 1 5 3 5 5 5 5 5 5 5 1 3 3 3 5 5 3 5 5 3 1 5 5 3 5 1 2 5 5 3 5 3 5 5 0 0 3 4 4 4 3 5 0 0 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 1 4 4 4 4 2 2 1 1 4 2 1 1 2 5 1 1 1 5 1 1 5 0 4 1 4 4 2 2 1 1 2 2 1 1 5 4 4 4 2 2 3 3 4 3 4 4 4 3 2 2 2 5 5 5 0 4 3 3 4 2 5 2 5 2 3 3 3 3 3 3 3 3 2 2 3 1 1 3 1 2 3 3 2 2 3 3 2 5 2 2 3 3 2 5 2 5 3 2 3 2 3 2 2 2 1 3 3 3 3 3 3 3 3 1 3 2 1 3 1 3 2 2 1 3 3 3 0 0 5 3 1 1 5 2 5 2 4 4 4 1 1 1 1 5 0 4 1 4 2 1 5 4 4 4 1 4 5 5 5 1 3 2 2 3 3 4 5 4 5 1 3 1 1 1 0 1 4 5 2 3 3 3 3 3 1 1 1 3 3 1 1 1 1 2 2 3 4 3 4 3 2 2 3 2 5 2 1 0 2 5 3 3 1 1 2 2 2 4 4 4 3 1 5 3 5 4 1 5 3 4 1 5 4 3 3 1 4 2 3 3 1 5 2 2 2 3 3 3 0 2 2 4 4 3 3 0 3 4 4 3 5 3 0 1 1 4 4 4 1 4 4 3 1 1 3 3 4 0 0 0 5 5 5 2 0 3 1 2 2 2 2 2 5 3 3 4 2 0 3 3 3 3 3 2 2 4 4 4 2 4 3 4 4 4 4 4 3 2 3 2 4 4 4 4 1 2 2 2 0 2 4 2 4 1 2 2 2 2 5 2 2 2 2 1 1 1 5 1 1 1 1 2 1 3 1 1 1 1 1 1 1 1 1 1 1 1 5 5 0 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 4 1 1 1 3 3 3 2 3 3 3 3 5 3 3 3 5 2 3 5 3 2 3 3 3 3 2 3 2 2 3 2 5 3 2 3 2 2 2 5 3 3 3 2 5 5 3 2 3 2 5 2 2 2 2 2 5 4 5 4 5 4 3 5 3 3 4 2 5 4 2 4 2 2 3 3 4 5 3 5 3 1 2 1 4 4 4 1 3 1 4 4 4 4 4 4 4 1 4 1 3 3 3 5 5 0 4 4 4 4 4 4 2 3 4 1 2 2 5 2 2 2 3 5 1 3 3 3 3 1 2 1 1 1 1 1 3 1 1 1 1 1 1 4 2 2 1 1 1 3 3 3 5 5 0 4 5 5 5 5 5 2 2 3 1 3 3 3 1 2 5 5 5 5 3 1 1 0 3 3 3 2 3 3 3 2 1 5 5 0 1 3 2 5 5 2 2 2 2 2 2 3 1 5 2 5 1 1 2 3 4 1 5 1 4 3 1 2 1 2 1 1 1 5 1 1 2 1 1 1 1 1 1 1 5 5 3 1 2 4 4 4 4 4 4 3 4 2 4 2 5 4 2 1 1 1 0 4 4 4 4 3 3 3 5 1 5 5 5 5 1 5 5 1 5 1 5 5 5 5 5 5 1 5 1 5 5 5 5 3 1 2 1 5 1 5 1 5 1 1 5 1 3 5 5 5 3 5 5 5 5 5 5 0 2 0 5 5 1 3 3 3 1 0 5 4 3 1 4 1 3 4 0 2 1 1 3 3 3 3 3 3 3 1 3 3 2 5 4 2 5 0 3 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 3 3 2 3 3 3 3 3 3 3 3 3 2 3 3 3 2 3 3 4 2 2 3 3 2 3 2 1 1 2 3 3 3 3 3 1 3 1 3 3 3 2 3 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 4 2 1 4 1 2 4 4 2 2 2 5 2 2 1 3 2 3 2 4 2 2 3 0 0 0 3 3 3 3 3 3 3 3 4 5 3 3 2 4 0 1 5 3 5 3 3 2 4 5 5 3 1 3 1 5 2 2 5 4 3 1 2 1 1 1 1 1 1 3 0 5 5 2 2 2 0 2 3 1 1 2 3 5 3 3 3 1 1 5 1 3 2 3 3 3 3 3 5 5 3 3 5 2 1 1 1 1 1 3 5 3 2 1 1 1 3 3 3 3 5 1 3 1 1 3 1 1 1 1 3 1 3 1 3 3 3 2 3 1 1 1 3 3 2 2 1 3 3 1 1 3 1 5 2 2 5 3 3 1 2 1 3 3 1 5 3 3 1 2 3 1 3 1 1 2 3 3 3 3 1 3 3 1 3 3 5 3 5 5 1 1 3 5 5 2 3 2 5 3 3 5 5 5 3 3 1 1 1 5 1 1 1 3 2 3 1 5 1 5 3 5 3 1 4 1 1 2 1 2 3 1 3 3 3 1 1 3 2 1 2 2 1 2 1 1 1 3 1 1 1 2 2 1 3 1 1 2 1 1 1 1 1 1 1 5 1 1 1 1 1 2 3 4 3 3 2 2 3 3 5 2 1 2 3 3 5 2 4 3 5 2 3 3 3 3 2 3 2 3 3 3 3 3 4 3 3 3 3 3 5 5 4 3 1 2 3 3 4 3 1 2 2 4 3 4 1 1 3 4 3 3 3 3 4 3 3 3 3 2 3 4 3 3 3 4 2 3 4 1 4 3 4 3 3 3 3 3 3 2 3 3 4 2 3 2 3 3 4 4 2 4 1 3 3 3 4 3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 2 3 3 1 1 5 5 2 1 1 5 4 3 3 5 3 3 3 3 0 4 1 3 1 2 5 4 5 1 3 4 5 5 5 5 5 5 5 3 3 5 5 2 1 2 1 1 1 5 3 1 4 2 1 4 5 3 5 1 5 5 3 4 5 1 1 2 4 3 1 4 4 5 5 2 0 2 1 4 1 1 4 4 4 4 4 4 4 5 3 5 5 5 1 3 5 5 5 3 3 5 5 5 5 5 5 5 3 5 5 3 1 5 5 3 5 5 1 5 5 5 1 5 3 1 2 5 4 1 4 3 3 3 1 3 1 4 4 4 2 4 4 1 3 4 5 4 0 5 1 1 1 2 2 3 3 2 3 1 1 4 2 5 3 2 2 4 5 4 4 2 3 2 2 2 2 2 5 2 1 3 3 3 2 5 3 3 5 2 2 5 2 3 3 2 1 2 2 2 1 3 1 2 2 3 3 3 1 1 1 2 2 4 2 5 5 5 1 5 3 5 5 3 1 1 5 5 1 5 1 5 3 1 5 3 5 1 5 5 1 5 5 5 1 5 1 1 1 4 4 5 4 4 4 4 4 4 5 5 1 4 2 5 0 3 3 3 3 3 3 1 4 2 2 3 3 3 3 3 3 5 3 4 1 2 1 5 1 3 5 2 1 3 2 3 3 1 3 1 2 1 5 3 1 5 3 5 1 1 1 2 5 5 1 1 1 2 1 1 1 4 5 5 5 3 5 3 1 1 2 1 2 1 2 1 1 1 2 1 2 1 2 1 1 1 2 2 2 1 1 2 1 1 2 2 1 2 2 2 1 1 2 1 5 1 1 3 3 3 3 3 3 3 1 3 3 3 3 1 1 4 3 4 3 1 3 5 2 5 5 5 5 5 5 4 5 1 4 1 3 3 3 3 3 3 5 3 5 3 3 5 5 5 1 3 0 5 5 5 2 4 0 0 0 1 1 3 3 4 4 4 1 4 4 3 4 5 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 1 1 5 1 1 5 5 1 1 1 1 1 1 1 1 1 1 1 1 5 5 1 1 1 1 1 5 1 4 5 5 2 3 3 5 4 3 3 4 1 4 4 3 1 4 4 2 4 1 4 1 1 4 4 4 4 2 2 4 2 1 1 5 1 4 5 1 1 0 0 4 3 4 1 1 4 2 4 2 4 3 2 3 4 4 1 4 1 1 1 3 1 4 1 4 1 1 1 1 4 4 1 4 4 1 3 1 2 1 2 1 4 1 1 4 4 1 3 2 1 1 1 3 4 4 3 4 4 4 3 3 2 4 4 3 3 4 4 3 1 3 4 4 1 4 1 1 4 4 4 1 3 4 3 5 3 1 2 4 4 4 4 1 5 4 4 4 3 3 3 3 4 4 4 4 1 4 4 3 4 4 3 1 4 3 4 4 4 2 1 4 4 4 4 2 3 4 1 3 4 2 5 4 4 4 1 1 1 1 1 4 1 1 1 4 4 4 4 4 1 4 4 4 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 5 3 1 5 1 1 5 2 2 5 5 3 5 5 4 2 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 2 2 2 0 3 3 3 3 4 5 3 2 3 2 2 2 3 3 2 0 2 2 2 2 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 2 2 1 3 3 3 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 1 5 5 5 5 5 5 4 3 2 2 0 3 5 3 3 5 0 3 3 2 2 2 2 3 3 3 5 5 1 1 5 0 0 3 1 4 1 5 0 2 1 4 4 4 4 4 0 0 3 3 3 5 1 1 5 1 1 1 2 1 1 2 1 1 1 1 1 1 1 2 2 2 5 4 5 3 3 3 4 4 4 4 4 4 4 4 5 2 5 2 3 5 5 4 4 4 4 1 2 2 2 2 4 3 1 1 4 3 3 3 5 3 3 5 3 3 4 4 4 4 4 4 0 4 3 4 3 2 5 5 5 5 3 3 4 5 1 1 3 3 5 0 0 3 3 3 3 3 3 4 1 5 3 3 5 4 4 3 3 3 3 3 3 0 5 5 4 5 1 4 4 2 2 4 5 3 4 4 4 1 1 3 4 1 1 2 1 3 1 1 3 1 1 1 1 1 1 1 3 2 1 4 1 4 3 0 0 5 0 1 3 1 4 5 3 1 1 1 5 5 5 5 5 4 4 4 4 1 2 4 1 3 1 3 3 3 3 3 3 3 3 5 3 3 5 3 3 3 1 3 5 2 4 2 1 4 2 3 3 3 3 1 4 2 2 1 1 1 2 3 2 2 2 1 1 2 1 2 2 2 3 3 2 1 2 2 2 2 2 4 4 2 1 1 4 4 1 1 3 1 2 4 3 2 4 3 1 2 1 1 2 1 2 2 2 4 2 2 4 2 1 2 2 2 2 2 5 2 2 4 2 1 1 2 3 2 1 2 2 2 2 1 2 2 2 4 2 2 2 1 4 2 1 2 4 5 3 2 3 3 3 0 5 1 3 1 3 3 3 3 3 3 1 3 3 3 3 1 3 3 3 3 1 3 2 4 4 4 4 4 1 1 3 1 3 0 5 0 1 2 2 2 1 5 2 5 2 2 2 2 2 2 2 2 5 2 2 5 5 2 2 2 2 1 5 2 2 1 5 2 2 4 4 4 1 4 4 4 2 3 4 1 4 5 5 3 1 1 1 1 5 2 1 2 1 1 4 2 2 2 3 2 3 1 1 1 0 0 2 2 2 3 4 4 4 4 2 4 4 4 1 5 5 1 3 3 3 3 3 3 3 5 3 3 3 5 5 3 3 3 5 5 5 3 2 3 3 3 3 1 1 3 5 3 5 3 3 3 2 5 2 5 3 5 5 3 3 3 3 4 2 5 5 1 4 3 4 4 1 4 4 4 1 1 1 1 4 4 4 4 4 4 4 4 4 5 4 2 5 2 4 3 5 3 4 3 1 5 3 1 3 3 3 1 3 3 3 3 3 3 3 3 3 1 3 1 1 3 3 3 3 3 1 1 3 1 3 3 3 3 3 5 5 5 1 2 3 2 2 1 1 1 4 1 3 2 1 5 2 4 3 3 2 5 5 3 3 5 5 5 3 4 4 4 4 1 1 5 5 1 5 0 2 3 1 1 3 1 1 1 1 1 1 2 1 1 2 1 1 1 5 1 1 1 2 2 1 1 5 3 1 5 1 1 1 3 3 3 3 3 3 3 3 3 3 3 4 2 1 4 2 1 1 1 1 3 4 1 4 2 3 1 1 0 1 1 3 1 1 4 2 2 2 3 2 3 3 2 5 1 2 0 0 1 1 1 1 1 4 2 1 5 2 1 1 4 2 2 2 2 1 4 4 4 3 3 2 2 0 4 2 2 2 3 2 2 2 2 1 1 2 2 2 2 3 4 2 2 2 2 1 5 4 4 4 4 2 4 4 5 5 5 5 1 3 5 3 4 3 4 5 5 2 0 4 1 3 1 4 4 1 1 1 3 4 4 3 3 3 3 3 5 2 2 2 2 4 2 2 2 2 2 2 0 0 0 2 1 2 2 1 1 5 3 5 4 4 4 4 5 1 3 1 1 3 3 3 3 1 1 3 0 0 3 1 1 1 5 3 5 5 2 3 1 3 3 1 2 0 4 0 2 2 2 2 3 3 1 4 3 3 4 3 3 4 3 2 4 3 3 2 2 2 3 3 3 4 0 1 3 1 1 5 3 2 4 0 5 5 5 0 1 4 3 1 3 3 3 3 3 4 3 3 3 4 3 3 1 3 2 4 1 3 3 3 3 3 3 3 3 2 3 1 1 3 1 5 4 5 0 4 4 2 3 4 4 1 4 2 2 2 2 5 5 0 0 0 0 0 0 0 5 5 4 2 5 5 3 1 3 3 3 4 1 5 5 5 0 4 4 4 3 4 Correct: 5334 out of: 10099\n",
      "Accuracy of the network :  52.8171106050104\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "countCorrect0=0\n",
    "countCorrect1=0\n",
    "count0=0\n",
    "count1=0\n",
    "labels=pd.read_excel('train-clean-Reputation.xlsx' )\n",
    "\n",
    "Y=[]  #target\n",
    "Pred=[]  #predicted\n",
    "\n",
    "with torch.no_grad():\n",
    "    for row in range(len(input)):\n",
    "        outputs = net(input[row,:].float())\n",
    "        result=0\n",
    "        total+=1\n",
    "        if outputs[0]<outputs[1]:result=1\n",
    "        if outputs[result]<outputs[2]:result=2\n",
    "        if outputs[result]<outputs[3]:result=3\n",
    "        if outputs[result]<outputs[4]:result=4\n",
    "        if outputs[result]<outputs[5]:result=5\n",
    "        \n",
    "        if TrainLables.iloc[row,result]==1: correct+=1\n",
    "        \n",
    "        Y.append(labels.iloc[row])\n",
    "        Pred.append(result)\n",
    "        \n",
    "        print(result, end=' ')\n",
    "        \n",
    "    \n",
    "print('Correct:', correct, 'out of:', total )\n",
    "print('Accuracy of the network : ',( 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.5371,  0.3501,  0.4424],\n",
       "        [ 0.0050,  0.0000,  0.0100,  ...,  0.3889, -0.3740, -0.5796],\n",
       "        [ 0.0000,  0.0000,  0.0050,  ...,  0.4365, -0.3293, -0.5288],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0150,  ...,  0.5327,  0.9238,  0.9131],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.7422,  1.2305,  0.8237],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.5117,  1.0781,  0.8599]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the validation data\n",
    "\n",
    "ValidData=pd.read_excel('valid-clean-Reputation.xlsx' )\n",
    "ValidData=ValidData.iloc[:,:-1].astype(float)\n",
    "ValidData=ValidData/200\n",
    "\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Saves/\"\n",
    "TF_Output=pd.read_csv( SavesDirectory+'evalOut.tsv', sep='\\t')\n",
    "\n",
    "ValidData=pd.concat([ValidData,TF_Output], axis=1)\n",
    "\n",
    "\n",
    "ValidData=torch.tensor(ValidData.values)\n",
    "ValidData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1272 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5\n",
       "0     0  0  0  1  0  0\n",
       "1     0  0  1  0  0  0\n",
       "2     0  0  0  1  0  0\n",
       "3     0  0  0  0  1  0\n",
       "4     0  0  0  0  0  1\n",
       "...  .. .. .. .. .. ..\n",
       "1267  0  0  0  0  0  1\n",
       "1268  0  0  0  1  0  0\n",
       "1269  0  0  1  0  0  0\n",
       "1270  0  0  0  0  1  0\n",
       "1271  0  0  0  1  0  0\n",
       "\n",
       "[1272 rows x 6 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=pd.read_excel('valid-clean-Reputation.xlsx' )\n",
    "\n",
    "labels=labels.iloc[:,-1] \n",
    "labelsOneHot=pd.get_dummies(labels)\n",
    "labelsOneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ValidLables =torch.tensor(labelsOneHot.values)\n",
    "ValidLables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2 3 3 5 5 5 4 5 1 3 5 1 1 2 4 2 2 3 3 2 0 2 2 4 2 3 1 3 5 4 2 3 2 1 3 3 2 2 3 5 4 4 1 3 3 1 4 4 3 4 4 4 1 4 3 1 3 1 1 3 3 4 4 4 3 4 4 3 4 3 3 4 4 4 4 4 1 1 4 4 4 4 3 3 3 1 4 3 1 3 3 3 4 4 1 4 4 1 4 3 4 4 4 4 3 3 3 1 5 1 1 2 4 4 4 2 1 4 1 1 3 3 1 1 5 4 0 1 5 5 5 5 3 3 5 3 2 3 4 3 1 1 1 3 3 2 5 5 0 0 1 0 0 0 1 0 0 0 1 0 4 4 5 1 5 2 1 3 1 5 1 4 4 5 5 1 1 4 5 5 5 3 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 2 2 5 3 4 1 4 1 2 2 3 3 3 5 3 3 3 3 3 5 0 1 1 2 3 1 3 5 3 5 1 1 3 5 3 3 5 1 3 5 5 3 3 4 1 4 4 4 1 4 3 4 4 3 3 1 1 4 5 3 2 1 4 4 4 1 2 2 2 3 2 2 4 5 1 1 2 0 4 2 4 1 4 1 1 2 2 3 2 5 2 0 2 1 3 1 2 2 3 3 2 1 2 4 3 4 4 5 1 3 2 1 1 3 1 1 5 5 3 1 2 2 2 3 2 3 3 3 5 5 5 4 3 2 4 4 2 5 3 1 1 0 1 1 1 1 4 1 1 1 3 1 2 1 4 1 1 2 1 4 1 2 4 4 2 1 3 1 1 4 1 1 2 4 1 1 1 4 4 1 2 0 4 5 1 1 5 3 4 5 3 2 2 2 4 2 2 5 5 1 2 5 1 1 3 0 4 2 5 3 3 3 3 3 1 3 1 1 3 1 3 3 5 1 2 2 1 2 2 2 5 3 5 1 2 5 1 3 0 4 3 2 3 2 5 1 2 3 3 4 5 5 1 1 1 3 2 2 2 2 1 5 1 4 3 5 5 5 1 5 1 1 0 2 5 1 2 1 1 1 4 3 2 5 3 2 0 1 5 4 5 5 1 1 1 5 3 3 5 4 5 4 2 3 1 5 4 4 4 3 4 5 2 4 4 4 5 4 4 3 3 4 2 1 1 0 1 5 3 3 3 3 3 0 3 4 1 5 3 3 2 4 4 5 5 0 2 4 0 4 5 3 3 5 2 5 3 5 1 1 4 5 2 3 1 5 0 1 3 4 3 1 5 4 3 3 5 1 4 1 4 4 4 4 3 2 4 3 4 4 0 1 3 1 5 5 2 1 4 4 2 3 3 1 4 4 1 1 1 1 1 1 5 5 2 5 5 5 1 1 2 1 4 3 1 5 5 5 1 3 5 5 5 1 5 0 1 4 3 3 1 1 1 3 4 1 3 3 1 1 1 3 2 5 1 3 1 5 3 4 4 5 4 1 1 4 3 0 2 2 3 3 4 2 3 4 3 1 4 4 2 1 2 1 1 1 2 3 1 4 4 3 5 5 5 0 2 4 0 2 3 4 4 1 4 1 1 1 2 1 3 5 2 4 5 5 5 4 3 1 1 1 1 1 4 1 4 1 1 1 1 1 5 4 3 1 1 5 1 5 3 3 0 3 3 3 5 1 4 1 3 1 4 1 4 5 3 5 4 4 2 2 5 2 3 3 5 3 1 3 1 5 3 3 5 4 3 3 2 1 2 5 3 4 3 1 4 4 2 2 3 1 2 2 3 2 2 3 3 3 1 3 3 4 5 4 5 0 1 3 5 3 5 5 2 1 4 4 4 1 4 1 4 4 2 3 2 5 5 3 1 1 1 1 1 1 1 1 2 3 5 1 5 3 2 5 0 1 1 1 3 3 1 3 1 3 3 3 1 1 1 1 5 3 3 3 2 1 5 2 1 3 3 2 3 1 3 3 2 2 1 3 0 3 3 3 1 1 5 3 1 2 5 5 1 2 2 2 2 2 2 3 1 1 2 0 2 3 2 1 2 1 2 3 4 3 4 1 4 1 4 2 1 3 3 5 5 2 3 3 2 4 4 2 3 3 3 3 2 1 2 4 2 5 2 4 0 3 5 5 2 3 2 2 3 5 2 5 0 5 4 3 3 5 5 4 2 4 1 1 2 1 2 5 1 5 5 1 5 1 5 3 5 3 3 3 3 1 4 3 1 3 3 3 3 3 2 2 2 1 4 1 3 3 4 5 5 1 5 1 0 3 3 5 2 1 1 2 5 1 3 3 3 3 5 2 3 5 1 1 5 5 2 2 4 3 3 1 5 5 3 1 2 2 5 4 3 1 3 3 5 5 5 5 5 5 5 5 1 3 5 1 2 5 0 3 1 3 2 2 2 2 3 1 1 1 3 3 4 2 3 1 1 1 1 1 4 1 1 3 3 1 3 1 3 5 5 1 4 4 5 1 1 1 1 5 4 1 5 4 3 1 3 1 1 4 3 3 3 4 4 4 3 1 1 4 1 3 3 5 5 1 2 4 5 5 1 1 5 5 5 5 5 3 1 4 3 1 4 4 3 3 3 3 3 4 1 1 5 5 1 3 1 3 3 3 4 2 1 2 3 2 2 1 1 2 1 2 1 4 1 3 5 3 3 2 1 2 5 2 5 2 5 1 4 2 5 3 3 2 3 5 3 1 1 1 3 1 3 3 1 1 0 3 2 3 2 3 0 1 1 1 1 2 1 3 5 2 0 2 4 4 2 3 3 4 5 5 0 4 5 1 1 1 2 0 4 3 1 1 3 2 2 0 0 2 4 4 4 2 5 1 4 4 3 Correct: 606 out of: 1272\n",
      "Accuracy of the network :  47.64150943396226\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "countCorrect0=0\n",
    "countCorrect1=0\n",
    "count0=0\n",
    "count1=0\n",
    "\n",
    "Y=[]  #target\n",
    "Pred=[]  #predicted\n",
    "\n",
    "with torch.no_grad():\n",
    "    for row in range(len(ValidData)):\n",
    "        outputs = net(ValidData[row,:].float())\n",
    "        result=0\n",
    "        total+=1\n",
    "        if outputs[0]<outputs[1]:result=1\n",
    "        if outputs[result]<outputs[2]:result=2\n",
    "        if outputs[result]<outputs[3]:result=3\n",
    "        if outputs[result]<outputs[4]:result=4\n",
    "        if outputs[result]<outputs[5]:result=5\n",
    "        \n",
    "        if labelsOneHot.iloc[row,result]==1: correct+=1\n",
    "        \n",
    "        Y.append(labels.iloc[row])\n",
    "        Pred.append(result)\n",
    "        \n",
    "        print(result, end=' ')\n",
    "        \n",
    "    \n",
    "print('Correct:', correct, 'out of:', total )\n",
    "print('Accuracy of the network : ',( 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0050,  0.0100,  ..., -0.2527, -0.3252, -0.2666],\n",
       "        [ 0.0000,  0.0050,  0.0100,  ...,  0.1863, -0.6479, -0.5098],\n",
       "        [ 0.0000,  0.0050,  0.0100,  ...,  0.1630, -0.1420, -0.4709],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0050,  ...,  0.8877,  0.2993, -0.2554],\n",
       "        [ 0.0050,  0.0000,  0.0000,  ...,  0.6519,  0.7334,  0.5640],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.7949,  0.3220, -0.1009]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the test data\n",
    "\n",
    "TestData=pd.read_excel('test-clean-Reputation.xlsx' )\n",
    "TestData=TestData.iloc[:,:-1].astype(float)\n",
    "TestData=TestData/200\n",
    "\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Saves/\"\n",
    "TF_Output=pd.read_csv( SavesDirectory+'testOut.tsv', sep='\\t')\n",
    "\n",
    "TestData=pd.concat([TestData,TF_Output], axis=1)\n",
    "\n",
    "\n",
    "TestData=torch.tensor(TestData.values)\n",
    "TestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=pd.read_excel('test-clean-Reputation.xlsx' )\n",
    "\n",
    "labels=labels.iloc[:,-1] \n",
    "labelsOneHot=pd.get_dummies(labels)\n",
    "labelsOneHot\n",
    "\n",
    "TestLables =torch.tensor(labelsOneHot.values)\n",
    "TestLables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3 2 2 4 5 1 5 2 3 5 5 5 3 3 1 1 1 1 2 1 1 2 2 2 4 3 3 0 2 3 2 1 2 1 2 2 3 3 2 3 3 1 0 3 2 1 1 3 1 3 4 1 5 0 5 3 4 1 4 3 4 4 3 3 4 4 4 5 3 4 3 3 3 1 3 1 4 3 4 3 3 3 1 1 3 3 1 4 3 4 3 4 4 4 4 3 4 4 4 3 4 1 4 4 4 1 4 4 3 4 4 4 4 4 2 4 3 3 4 3 3 1 5 1 4 4 4 4 2 2 4 4 4 4 3 2 2 2 3 0 2 4 3 2 1 3 4 4 4 4 5 5 2 5 5 3 2 0 1 0 0 0 2 2 1 0 3 4 3 3 5 3 5 5 1 5 2 5 3 5 3 0 3 1 0 5 2 1 1 1 1 0 4 4 4 4 5 5 0 0 0 0 0 0 0 0 1 0 0 1 0 0 2 5 4 0 3 3 5 5 3 5 3 5 3 5 1 4 1 3 3 1 5 3 3 3 3 3 3 3 3 3 3 3 3 4 3 2 3 3 1 5 0 0 4 3 3 5 5 1 5 1 4 4 3 4 2 0 5 4 2 2 2 0 4 1 5 5 2 2 4 3 1 4 1 1 4 2 5 5 3 2 2 5 1 5 3 2 4 0 5 5 2 1 5 3 1 4 3 3 0 2 2 2 2 2 2 2 5 2 3 3 3 2 2 4 3 5 5 5 1 1 4 2 3 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 1 1 1 1 1 1 2 2 1 2 4 1 1 1 1 1 4 4 4 4 1 4 1 2 1 2 1 5 2 0 2 4 1 1 3 5 3 2 2 4 2 2 2 1 1 3 3 3 3 3 1 2 4 1 3 3 1 3 2 2 1 2 1 3 1 1 3 5 3 4 2 1 5 1 2 5 1 1 2 3 2 4 4 3 4 4 5 1 5 5 4 2 3 5 2 5 5 5 3 1 2 3 3 4 5 5 3 1 1 5 3 1 1 2 1 4 5 3 1 1 3 5 5 2 2 5 5 2 5 5 2 3 3 5 3 3 1 5 1 5 1 1 1 4 4 4 5 4 4 5 4 5 5 3 3 5 4 4 2 4 4 4 3 4 4 5 5 3 3 3 3 4 4 5 4 3 3 2 0 1 4 3 4 5 1 5 3 1 2 5 5 5 5 5 1 2 4 4 5 3 3 3 3 5 4 3 3 5 3 5 2 3 1 1 4 4 1 0 3 4 1 0 1 1 4 4 4 5 3 1 2 1 1 4 4 5 5 3 4 1 1 3 5 5 3 3 5 3 3 5 1 3 5 1 1 1 5 1 1 1 1 1 1 1 1 5 3 1 5 5 5 3 1 5 5 5 5 3 5 1 4 3 3 3 5 3 3 3 3 3 3 3 5 1 3 3 4 3 1 5 3 1 4 3 1 5 1 1 1 1 4 2 3 5 3 4 5 5 5 5 0 2 2 5 2 2 1 1 1 4 0 3 5 2 3 3 4 3 3 5 1 5 0 2 5 1 4 4 2 3 1 3 5 4 2 4 1 1 1 4 1 4 4 3 4 4 4 4 4 3 1 1 5 4 1 2 4 5 1 1 3 3 2 5 5 3 1 5 4 4 5 4 5 4 4 4 5 0 2 4 1 5 0 4 3 1 5 4 3 4 2 3 1 4 1 2 2 3 2 5 4 4 2 3 2 0 3 4 5 3 1 1 4 4 5 1 3 5 5 2 3 3 3 3 3 3 3 3 3 0 4 1 5 2 2 4 3 2 1 3 4 5 4 1 0 5 1 4 2 1 1 1 1 1 1 1 1 2 4 4 1 0 2 0 3 1 2 1 3 3 3 3 3 4 4 1 5 1 1 5 1 5 3 3 3 3 3 3 1 3 1 1 3 5 3 2 2 4 1 4 2 5 5 5 1 2 2 2 2 2 2 2 1 1 1 3 2 3 4 5 3 3 5 2 3 3 4 4 5 4 3 3 1 2 4 1 5 1 3 2 3 4 1 4 3 4 1 3 3 4 5 1 3 2 3 3 3 5 2 2 5 5 3 2 4 4 3 0 5 3 1 1 1 1 0 3 2 3 2 2 1 1 5 2 2 5 3 5 3 3 5 3 1 1 3 1 1 1 3 1 1 2 4 2 3 2 3 3 2 2 2 2 2 3 5 4 5 3 3 1 1 3 1 3 3 1 3 3 5 3 2 2 5 5 5 5 1 1 5 3 1 3 3 5 2 2 1 3 3 3 2 3 3 5 3 4 3 2 3 2 3 3 3 3 1 3 4 1 4 1 3 5 1 4 4 5 5 5 5 5 3 0 4 2 2 3 3 5 3 5 1 2 5 1 1 5 3 1 5 1 1 0 3 3 3 1 1 4 3 5 3 3 3 5 1 1 1 1 1 1 2 1 2 0 4 1 1 1 1 4 3 4 3 4 4 4 3 1 1 4 1 3 3 3 2 1 5 5 5 3 2 2 4 5 5 0 5 5 5 5 5 5 3 2 3 2 1 5 1 1 5 2 1 4 1 1 4 1 1 0 3 1 1 4 4 3 2 5 5 5 3 2 3 1 1 1 3 2 2 2 1 2 1 2 2 2 2 2 2 5 1 1 5 2 4 2 4 2 5 1 3 5 5 3 3 3 4 4 4 1 3 3 1 1 2 1 3 3 1 1 2 1 0 3 3 1 2 0 4 3 4 2 3 0 1 0 2 0 2 2 2 2 1 2 5 2 1 1 4 4 4 4 1 1 1 1 5 2 3 3 Correct: 613 out of: 1255\n",
      "Accuracy of the network :  48.844621513944226\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "Y=[]  #target\n",
    "Pred=[]  #predicted\n",
    "\n",
    "with torch.no_grad():\n",
    "    for row in range(len(TestData)):\n",
    "        outputs = net(TestData[row,:].float())\n",
    "        result=0\n",
    "        total+=1\n",
    "        if outputs[0]<outputs[1]:result=1\n",
    "        if outputs[result]<outputs[2]:result=2\n",
    "        if outputs[result]<outputs[3]:result=3\n",
    "        if outputs[result]<outputs[4]:result=4\n",
    "        if outputs[result]<outputs[5]:result=5\n",
    "        \n",
    "        if labelsOneHot.iloc[row,result]==1: correct+=1\n",
    "        \n",
    "        Y.append(labels.iloc[row])\n",
    "        Pred.append(result)\n",
    "\n",
    "        \n",
    "        \n",
    "        print(result, end=' ')\n",
    "        \n",
    "       \n",
    "print('Correct:', correct, 'out of:', total )\n",
    "print('Accuracy of the network : ',( 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 41  27  10   9   2   2]\n",
      " [  7 135  29  23  21  18]\n",
      " [  6  40  88  50  13  24]\n",
      " [  0  40  22 135  33  25]\n",
      " [  0  23  27  56 109  34]\n",
      " [  3  23  13  32  30 105]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "print(metrics.confusion_matrix(Y,Pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Pants       0.72      0.45      0.55        91\n",
      "       False       0.47      0.58      0.52       233\n",
      " Barely-True       0.47      0.40      0.43       221\n",
      "   Half-True       0.44      0.53      0.48       255\n",
      " Mostly-True       0.52      0.44      0.48       249\n",
      "        True       0.50      0.51      0.51       206\n",
      "\n",
      "    accuracy                           0.49      1255\n",
      "   macro avg       0.52      0.48      0.49      1255\n",
      "weighted avg       0.50      0.49      0.49      1255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Pants', 'False', 'Barely-True','Half-True','Mostly-True','True']\n",
    "\n",
    "print(metrics.classification_report(Y, Pred,target_names =target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the FCNN model\n",
    "\n",
    "stage='NNetwork6WayClass/'\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/\"+stage\n",
    "#PATH = SavesDirectory+'4885.pth'\n",
    "\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# more on saving pytorch networks: https://pytorch.org/docs/stable/notes/serialization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we first do the classification using the transformer This is our first classification task.\n",
    "\n",
    "The output classification vector from the transformer is saved to be used by the FCNN This is our second classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4101, -1.8006,  0.2481, -2.1183]])\n",
      "tensor([[-0.3305,  0.6870,  1.8338, -0.9649]])\n",
      "tensor([0.2243])\n"
     ]
    }
   ],
   "source": [
    "input1 = torch.randn(1,4)\n",
    "input2 = torch.randn(1,4)\n",
    "output = torch.cosine_similarity(input1, input2)\n",
    "print(input1)\n",
    "print(input2)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "Some pre-processing to the dataset has already been done in preparation for various tests, so this processing is not from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedure for getting the data sets and formatting them for the transformer\n",
    " \n",
    "\n",
    "def prepareDataset( filename):\n",
    "     \n",
    "    ReadSet=pd.read_excel(filename )\n",
    "\n",
    "    ReadSet['text']=ReadSet['Statement']\n",
    "    ReadSet['labels']=ReadSet['Label']\n",
    "    \n",
    "    ReadSet=ReadSet.drop(['ID','Label','Statement','Subject','Speaker','Job','From','Affiliation','PantsTotal','NotRealTotal','BarelyTotal','HalfTotal','MostlyTotal','Truths','Context'\n",
    "],axis=1)\n",
    "     \n",
    "    return ReadSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It costs more money to put a person on death r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Charlie Crist \"bizarrely vetoed\" $9.7 million ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Massachusetts, half of the primary care doc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fulton County has successfully reduced the num...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Says he stood up to his own party by voting ag...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10264</th>\n",
       "      <td>Recently Rick Scott closed 30 womens health ca...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10265</th>\n",
       "      <td>Says Charlie Bass supports Paul Ryan plan that...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266</th>\n",
       "      <td>A report by the US General Accountability Offi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10267</th>\n",
       "      <td>In the United States, weve had 12 years in a r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10268</th>\n",
       "      <td>In Virginias Medicaid program, approximately 3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10269 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  labels\n",
       "0      It costs more money to put a person on death r...       0\n",
       "1      Charlie Crist \"bizarrely vetoed\" $9.7 million ...       3\n",
       "2      In Massachusetts, half of the primary care doc...       4\n",
       "3      Fulton County has successfully reduced the num...       4\n",
       "4      Says he stood up to his own party by voting ag...       5\n",
       "...                                                  ...     ...\n",
       "10264  Recently Rick Scott closed 30 womens health ca...       4\n",
       "10265  Says Charlie Bass supports Paul Ryan plan that...       5\n",
       "10266  A report by the US General Accountability Offi...       3\n",
       "10267  In the United States, weve had 12 years in a r...       1\n",
       "10268  In Virginias Medicaid program, approximately 3...       1\n",
       "\n",
       "[10269 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the training dataset\n",
    "train=prepareDataset( 'trainRNDtext.xlsx')\n",
    "# and display for inspecting\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The president is brain-dead.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barack Obama supported keeping troops in Iraq,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He's leading by example, refusing contribution...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm the first person who really took up the is...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I built that border fence in San Diego...and i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>CNN accidentally aired 30 minutes of pornograp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>President Obamas American Recovery and Reinves...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>We (in Illinois) have the fifth-highest tax bu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>Says Donald Trump won more counties than any c...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>A recent study found that cities where Uber op...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1284 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "0                          The president is brain-dead.       0\n",
       "1     Barack Obama supported keeping troops in Iraq,...       3\n",
       "2     He's leading by example, refusing contribution...       3\n",
       "3     I'm the first person who really took up the is...       4\n",
       "4     I built that border fence in San Diego...and i...       4\n",
       "...                                                 ...     ...\n",
       "1279  CNN accidentally aired 30 minutes of pornograp...       1\n",
       "1280  President Obamas American Recovery and Reinves...       2\n",
       "1281  We (in Illinois) have the fifth-highest tax bu...       4\n",
       "1282  Says Donald Trump won more counties than any c...       4\n",
       "1283  A recent study found that cities where Uber op...       3\n",
       "\n",
       "[1284 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the evaluation/validation dataset\n",
    "Eval=prepareDataset('valid.xlsx')\n",
    "# and display for inspecting\n",
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Mexico was 46th in teacher pay (when he wa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barack Obama and Hillary Clinton have changed ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll tell you what I can tell this country: If...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tommy Thompson created the first school choice...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty-six percent decline in overall crime. A ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>We have trade agreements with 20 countries, an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>On Donald Trumps plan to cut federal funding t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>Black Lives Matter, who are attacking law enfo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>Latina who enthusiastically supported Donald T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>Theres been no conclusive or specific report t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1283 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "0     New Mexico was 46th in teacher pay (when he wa...       4\n",
       "1     Barack Obama and Hillary Clinton have changed ...       3\n",
       "2     I'll tell you what I can tell this country: If...       1\n",
       "3     Tommy Thompson created the first school choice...       5\n",
       "4     Fifty-six percent decline in overall crime. A ...       5\n",
       "...                                                 ...     ...\n",
       "1278  We have trade agreements with 20 countries, an...       1\n",
       "1279  On Donald Trumps plan to cut federal funding t...       4\n",
       "1280  Black Lives Matter, who are attacking law enfo...       2\n",
       "1281  Latina who enthusiastically supported Donald T...       0\n",
       "1282  Theres been no conclusive or specific report t...       1\n",
       "\n",
       "[1283 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the test set dataset\n",
    "test=prepareDataset('test.xlsx')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the transformer for fine tuning\n",
    "\n",
    "This is where changes are done to optimise the model\n",
    "\n",
    "The simpletransformers library is the quickest way to do this at the time of writing. \n",
    "For more information on the settings and their default value go here:\n",
    "https://github.com/ThilinaRajapakse/simpletransformers#default-settings \n",
    "\n",
    "###### Please do read that reference before changing any parameters. Don't try to be a hero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model variables were set up: \n"
     ]
    }
   ],
   "source": [
    "#Set the model being used here\n",
    "model_class='albert'  # bert or roberta or albert\n",
    "model_version='albert-large-v2' #bert-base-cased, roberta-base, roberta-large, albert-base-v2 OR albert-large-v2\n",
    "\n",
    "\n",
    "output_folder='./TunedModels/'+model_class+'/'+model_version+\"/\"\n",
    "cache_directory= \"./TunedModels/\"+model_class+\"/\"+model_version+\"/cache/\"\n",
    "labels_count=6  # the number of classification classes\n",
    "\n",
    "print('model variables were set up: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\0 finalThesis\\randomText\n",
      "./TunedModels/albert/albert-large-v2/\n",
      "./TunedModels/albert/albert-large-v2/cache/\n"
     ]
    }
   ],
   "source": [
    "# use this to test if writing to the directories is working\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "print(output_folder)\n",
    "print(cache_directory)\n",
    "\n",
    "testWrite=train.head(30)\n",
    " \n",
    "testWrite.to_csv(output_folder+'DeleteThisToo.tsv', sep='\\t')\n",
    "testWrite.to_csv(cache_directory+'DeleteThisToo.tsv', sep='\\t')\n",
    "\n",
    "del(testWrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "save_every_steps=1285\n",
    "# assuming training batch size of 8\n",
    "# any number above 1284 saves the model only at every epoch\n",
    "# Saving the model mid training very often will consume disk space fast\n",
    "\n",
    "train_args={\n",
    "    \"output_dir\":output_folder,\n",
    "    \"cache_dir\":cache_directory,\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'num_train_epochs': 1,\n",
    "    \"save_steps\": save_every_steps, \n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"train_batch_size\": 64,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"evaluate_during_training_steps\": 312,\n",
    "    \"max_seq_length\": 64,\n",
    "    \"n_gpu\": 1,\n",
    "}\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(model_class, model_version, num_labels=labels_count, args=train_args) \n",
    "\n",
    "# You can set class weights by using the optional weight argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a saved model (based on above args{})\n",
    "\n",
    "If you stopped training you can continue training from a previously saved check point.\n",
    "The next cell allows you to load a model from any checkpoint.\n",
    "The number of epochs in the train_args{} will be done and continue tuning from your checkpoint.\n",
    "\n",
    "###### HOWEVER\n",
    "It will overwrite previous checkpoints!\n",
    "Example:  If you load an epoch-3 checkpoint, the epoch-1 checkpoint will be overwritten by the 4th epoch and it will be equivalent to a 4th epoch even if you have epoch-1 in the name.\n",
    "###### SO BE CAREFUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model, please wait...\n",
      "model in use is : ./TunedModels/albert/albert-large-v2/checkpoint-322-epoch-2\n"
     ]
    }
   ],
   "source": [
    "# loading a previously saved model based on this particular Transformer Class and model_name\n",
    "\n",
    "# loading the checkpoint that gave the best result\n",
    "CheckPoint='checkpoint-322-epoch-2'  #epoch 2\n",
    "\n",
    "\n",
    "preSavedCheckpoint=output_folder+CheckPoint\n",
    "\n",
    "print('Loading model, please wait...')\n",
    "model = ClassificationModel( model_class, preSavedCheckpoint, num_labels=labels_count, args=train_args) \n",
    "print('model in use is :', preSavedCheckpoint )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Transformer\n",
    "\n",
    "Skip the next cell if you want to skip the training and go directly to the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06351d64fd0b4477a59a4c9120cb3d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10269.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9989bdf23b1f476eb7f0aa16e36d1e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71453a166d874af4a6fb48e93b1fe57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=161.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.689072\n",
      "\n",
      "Training of albert model complete. Saved to ./TunedModels/albert/albert-large-v2/.\n",
      "Training time:  0:03:00.334546\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "current_time = datetime.now()\n",
    "model.train_model(train)\n",
    "print(\"Training time: \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded from cache at ./TunedModels/albert/albert-large-v2/cache/cached_dev_albert_64_6_10269\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecd8a4cad614fbf886340ca26e7319b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1284.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': 0.03041130547938583, 'acc': 0.2162820138280261, 'eval_loss': 1.7519530008700779}\n",
      "Features loaded from cache at ./TunedModels/albert/albert-large-v2/cache/cached_dev_albert_64_6_1284\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa060636c194f1b9af5937177e1951b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=161.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': -0.009363452126874507, 'acc': 0.19080996884735202, 'eval_loss': 1.7674117969430012}\n",
      "Features loaded from cache at ./TunedModels/albert/albert-large-v2/cache/cached_dev_albert_64_6_1283\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5c028c7f234fc9a7e0acc2836f4d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=161.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': -0.010929883051276498, 'acc': 0.2034294621979735, 'eval_loss': 1.7521654197147913}\n",
      "Training Result: 0.2162820138280261\n",
      "Eval Result: 0.19080996884735202\n",
      "Test Set Result: 0.2034294621979735\n"
     ]
    }
   ],
   "source": [
    "TrainResult, TrainModel_outputs, wrong_predictions = model.eval_model(train, acc=sklearn.metrics.accuracy_score)\n",
    "\n",
    "EvalResult, EvalModel_outputs, wrong_predictions = model.eval_model(Eval, acc=sklearn.metrics.accuracy_score)\n",
    "\n",
    "TestResult, TestModel_outputs, wrong_predictions = model.eval_model(test, acc=sklearn.metrics.accuracy_score)\n",
    "\n",
    "print('Training Result:', TrainResult['acc'])\n",
    "#print('Model Out:', TrainModel_outputs)\n",
    "\n",
    "print('Eval Result:', EvalResult['acc'])\n",
    "#print('Model Out:', EvalModel_outputs)\n",
    "\n",
    "print('Test Set Result:', TestResult['acc'])\n",
    "#print('Model Out:', TestModel_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.73779297  0.20373535 -0.0227356   0.31225586  0.0958252   0.05310059] 3   4 \n",
      "[-0.68847656  0.2512207   0.01332092  0.2836914   0.10888672  0.02445984] 3   3 Match 1\n",
      "\n",
      "[-0.7558594   0.2421875  -0.01084137  0.3046875   0.0793457   0.01844788] 3   1 \n",
      "[-0.71484375  0.23608398  0.00139999  0.32177734  0.10412598  0.03207397] 3   5 \n",
      "[-0.59521484  0.3383789   0.0607605   0.32617188  0.15124512 -0.00679779] 1   5 \n",
      "[-0.63378906  0.22302246  0.01179504  0.32763672  0.14550781  0.05947876] 3   2 \n",
      "[-0.7651367   0.19580078 -0.0244751   0.34277344  0.08435059  0.04986572] 3   4 \n",
      "[-0.7451172   0.30639648 -0.07012939  0.31298828  0.08636475 -0.04708862] 3   5 \n",
      "[-0.9111328   0.2467041  -0.19567871  0.33276367  0.16955566  0.02609253] 3   4 \n",
      "[-0.78027344  0.21154785 -0.0355835   0.3708496   0.03652954  0.07940674] 3   5 \n",
      "[-0.8076172   0.29003906 -0.11419678  0.37451172  0.02505493 -0.0461731 ] 3   5 \n",
      "[-0.66748047  0.3046875   0.02294922  0.2763672   0.16967773 -0.00335121] 1   3 \n",
      "[-0.72509766  0.2244873  -0.02043152  0.30273438  0.08892822  0.02653503] 3   2 \n",
      "[-0.6796875   0.25610352  0.00817871  0.28710938  0.12927246  0.01654053] 3   1 \n",
      "[-0.6542969   0.34570312  0.00582886  0.32592773  0.21191406 -0.05331421] 1   4 \n",
      "[-0.74316406  0.15112305 -0.04367065  0.35253906  0.05032349  0.06921387] 3   4 \n",
      "[-0.7739258   0.26367188 -0.05795288  0.34448242  0.07525635  0.00972748] 3   4 \n",
      "[-0.6220703   0.32983398  0.01750183  0.2631836   0.17163086 -0.00298309] 1   3 \n",
      "[-0.73095703  0.29370117 -0.03884888  0.31835938  0.09564209  0.01008606] 3   1 \n",
      "[-0.6254883   0.35791016  0.04772949  0.32495117  0.15490723 -0.01080322] 1   1 Match 2\n",
      "\n",
      "[-0.6489258   0.3359375   0.02340698  0.28833008  0.16455078 -0.0034008 ] 1   1 Match 3\n",
      "\n",
      "[-0.58691406  0.3503418   0.00943756  0.29760742  0.19714355  0.01667786] 1   3 \n",
      "[-0.65722656  0.2866211   0.01285553  0.29663086  0.08764648  0.02749634] 3   1 \n",
      "[-0.74365234  0.18066406 -0.01361084  0.3203125   0.10406494  0.06506348] 3   5 \n",
      "[-0.67089844  0.24658203  0.02496338  0.31201172  0.13513184  0.05233765] 3   5 \n",
      "[-0.7792969   0.24121094 -0.0748291   0.32470703  0.07946777 -0.00402832] 3   5 \n",
      "[-0.5883789   0.28735352  0.06286621  0.31420898  0.1998291  -0.00595093] 3   5 \n",
      "[-0.6870117   0.2866211  -0.07196045  0.2770996   0.12780762 -0.02519226] 1   4 \n",
      "[-0.69189453  0.29248047 -0.01558685  0.30981445  0.13708496  0.02438354] 3   5 \n",
      "[-6.9873047e-01  1.9970703e-01  1.1086464e-04  2.8320312e-01\n",
      "  1.1035156e-01  4.4555664e-02] 3   3 Match 4\n",
      "\n",
      "[-0.7392578   0.3173828  -0.03811646  0.32299805  0.14135742 -0.04522705] 3   5 \n",
      "[-0.76123047  0.1973877  -0.05960083  0.34985352  0.05575562  0.03494263] 3   2 \n",
      "[-0.68310547  0.23803711 -0.01828003  0.32250977  0.10668945  0.06530762] 3   0 \n",
      "[-0.5957031   0.3400879   0.07537842  0.27905273  0.17944336  0.04855347] 1   5 \n",
      "[-0.70166016  0.20568848 -0.03033447  0.31713867  0.11834717  0.04992676] 3   5 \n",
      "[-0.5625      0.36767578  0.08044434  0.28344727  0.22180176 -0.01721191] 1   5 \n",
      "[-0.55322266  0.32543945  0.0552063   0.2734375   0.1953125   0.0295105 ] 1   4 \n",
      "[-0.70166016  0.20898438 -0.0305481   0.31054688  0.12902832  0.01818848] 3   3 Match 5\n",
      "\n",
      "[-0.73339844  0.33422852 -0.02905273  0.31347656  0.07904053 -0.06140137] 1   3 \n",
      "[-0.8178711   0.28637695 -0.09521484  0.31054688  0.26293945  0.00297737] 3   5 \n",
      "[-0.6713867   0.2692871  -0.02043152  0.27685547  0.10632324  0.03952026] 3   5 \n",
      "[-0.67041016  0.26220703  0.01708984  0.31176758  0.10089111  0.05020142] 3   4 \n",
      "[-0.7060547   0.23181152  0.01911926  0.31176758  0.05511475  0.02157593] 3   3 Match 6\n",
      "\n",
      "[-0.75439453  0.24829102 -0.05691528  0.3388672   0.08258057 -0.00580978] 3   2 \n",
      "[-0.72509766  0.20825195 -0.03857422  0.2944336   0.09417725  0.04226685] 3   1 \n",
      "[-0.74658203  0.28955078 -0.09991455  0.33032227  0.08129883 -0.03485107] 3   2 \n",
      "[-7.29980469e-01  2.31445312e-01  2.12550163e-04  2.99316406e-01\n",
      "  1.18774414e-01  1.69982910e-02] 3   3 Match 7\n",
      "\n",
      "[-0.7788086   0.19152832 -0.06216431  0.33032227  0.06555176  0.04022217] 3   2 \n",
      "[-0.703125    0.22644043  0.00645828  0.29589844  0.1295166   0.0559082 ] 3   2 \n",
      "[-0.7832031   0.31396484 -0.07781982  0.32592773  0.05917358 -0.06286621] 3   5 \n",
      "[-0.7319336   0.21948242 -0.05743408  0.296875    0.07336426  0.00521088] 3   5 \n",
      "[-0.6621094   0.29736328  0.00788116  0.28808594  0.10638428  0.03121948] 1   2 \n",
      "[-0.7109375   0.22058105 -0.00534821  0.3112793   0.0892334   0.06671143] 3   4 \n",
      "[-0.69677734  0.23754883 -0.01954651  0.30249023  0.0760498   0.03915405] 3   2 \n",
      "[-0.7949219   0.30859375 -0.13208008  0.3725586   0.00386429 -0.0357666 ] 3   1 \n",
      "[-0.69433594  0.45947266 -0.00669098  0.31176758  0.11303711 -0.16052246] 1   5 \n",
      "[-0.6796875   0.27441406 -0.00756073  0.2788086   0.12365723  0.03979492] 3   5 \n",
      "[-0.7084961   0.2783203   0.01933289  0.28515625  0.09814453  0.02102661] 3   2 \n",
      "[-0.7324219   0.24536133 -0.02146912  0.31762695  0.07098389  0.02911377] 3   5 \n",
      "[-0.70947266  0.25268555 -0.03018188  0.3088379   0.09741211  0.00400925] 3   2 \n",
      "[-0.78759766  0.1505127  -0.07434082  0.3322754   0.03192139  0.04879761] 3   1 \n",
      "[-0.7324219   0.21008301 -0.02688599  0.3083496   0.08514404  0.03515625] 3   1 \n",
      "[-0.7636719   0.16088867 -0.0692749   0.35058594  0.04684448  0.0802002 ] 3   3 Match 8\n",
      "\n",
      "[-0.7138672   0.3256836  -0.04977417  0.33862305  0.09960938 -0.05810547] 3   5 \n",
      "[-0.7480469   0.31445312 -0.08721924  0.3154297   0.04171753 -0.07757568] 3   0 \n",
      "[-0.58935547  0.34716797 -0.01651001  0.31201172  0.1227417   0.019104  ] 1   5 \n",
      "[-0.7714844   0.28393555 -0.0982666   0.32250977  0.04083252 -0.0567627 ] 3   3 Match 9\n",
      "\n",
      "[-0.6972656   0.25024414 -0.02806091  0.32348633  0.09008789  0.06112671] 3   4 \n",
      "[-0.7109375   0.32885742 -0.04553223  0.29467773  0.12249756 -0.05627441] 1   3 \n",
      "[-0.78515625  0.35009766 -0.11987305  0.3334961   0.04122925 -0.06951904] 1   1 Match 10\n",
      "\n",
      "[-0.6845703   0.28100586 -0.00696564  0.3161621   0.11810303  0.02217102] 3   5 \n",
      "[-0.5751953   0.41625977  0.04104614  0.29882812  0.19067383 -0.05825806] 1   1 Match 11\n",
      "\n",
      "[-0.7133789   0.2265625  -0.00160408  0.31835938  0.10992432  0.04083252] 3   3 Match 12\n",
      "\n",
      "[-0.6274414   0.35473633  0.0413208   0.34277344  0.09259033 -0.00465775] 1   5 \n",
      "[-0.6738281   0.22033691 -0.05792236  0.32226562  0.06240845  0.04037476] 3   4 \n",
      "[-0.7919922   0.18579102 -0.10137939  0.3190918   0.01133728  0.05297852] 3   1 \n",
      "[-0.6899414   0.26782227 -0.00205231  0.29760742  0.13903809 -0.02688599] 3   4 \n",
      "[-0.7817383   0.20007324 -0.08392334  0.31054688  0.02102661  0.05401611] 3   3 Match 13\n",
      "\n",
      "[-0.7607422   0.31591797 -0.11376953  0.32250977  0.0401001  -0.06585693] 3   1 \n",
      "[-0.7470703   0.3400879  -0.04052734  0.31713867  0.11871338 -0.07836914] 1   3 \n",
      "[-0.6381836   0.24182129 -0.01131439  0.29052734  0.11279297  0.04672241] 3   3 Match 14\n",
      "\n",
      "[-0.73046875  0.18774414 -0.05941772  0.32495117  0.07476807  0.05361938] 3   5 \n",
      "[-0.7558594   0.24816895 -0.07073975  0.30444336  0.0473938   0.00152016] 3   2 \n",
      "[-0.76953125  0.234375   -0.05047607  0.35302734  0.11437988  0.02307129] 3   1 \n",
      "[-0.6977539   0.3708496  -0.0031395   0.32958984  0.1673584  -0.07293701] 1   4 \n",
      "[-0.52197266  0.36767578  0.07550049  0.2770996   0.20202637 -0.02894592] 1   3 \n",
      "[-0.7895508   0.2631836  -0.05764771  0.34350586  0.04901123  0.00479507] 3   3 Match 15\n",
      "\n",
      "[-0.6616211   0.24414062 -0.02146912  0.2915039   0.10284424 -0.00849152] 3   3 Match 16\n",
      "\n",
      "[-0.65234375  0.2734375  -0.10552979  0.34399414  0.2006836  -0.04840088] 3   4 \n",
      "[-0.7421875   0.2121582  -0.01841736  0.31225586  0.1036377   0.0506897 ] 3   5 \n",
      "[-0.64453125  0.33422852  0.04284668  0.30322266  0.17126465  0.01230621] 1   4 \n",
      "[-0.74902344  0.19226074 -0.06243896  0.30200195  0.07476807  0.03967285] 3   3 Match 17\n",
      "\n",
      "[-0.7758789   0.18920898 -0.03890991  0.34326172  0.06280518  0.05563354] 3   0 \n",
      "[-0.74072266  0.296875   -0.00346565  0.32373047  0.08154297  0.02558899] 3   1 \n",
      "[-0.7602539   0.15881348 -0.02841187  0.31567383  0.08215332  0.04702759] 3   1 \n",
      "[-0.72314453  0.2479248  -0.02275085  0.30078125  0.09448242  0.02392578] 3   1 \n",
      "[-0.7832031   0.203125   -0.09436035  0.38012695  0.04937744  0.05548096] 3   5 \n",
      "[-0.7158203   0.23742676 -0.06030273  0.29467773  0.05603027  0.02651978] 3   5 \n",
      "[-0.73583984  0.23327637  0.01166534  0.3347168   0.09124756  0.05395508] 3   3 Match 18\n",
      "\n",
      "[-0.6035156   0.2783203   0.01809692  0.32836914  0.1697998   0.04541016] 3   1 \n",
      "[-0.76904297  0.23864746 -0.04690552  0.33618164  0.08221436  0.04672241] 3   4 \n",
      "[-0.71435547  0.19836426 -0.04403687  0.3334961   0.09228516  0.07189941] 3   0 \n",
      "[-0.7402344   0.24353027 -0.00631714  0.31713867  0.07714844  0.04415894] 3   3 Match 19\n",
      "\n",
      "[-0.7265625   0.30029297 -0.05871582  0.32739258  0.07733154  0.00671768] 3   1 \n",
      "[-0.6069336   0.28393555  0.02606201  0.27197266  0.19311523  0.00843811] 1   2 \n",
      "[-0.74658203  0.2010498  -0.01919556  0.29541016  0.11486816  0.03598022] 3   1 \n",
      "[-0.70410156  0.16296387 -0.03814697  0.31762695  0.1015625   0.05780029] 3   2 \n",
      "[-0.8066406   0.26611328 -0.0970459   0.34204102  0.0904541  -0.00996399] 3   5 \n",
      "[-0.7006836   0.20715332 -0.0328064   0.31030273  0.06726074  0.05911255] 3   5 \n",
      "[-0.7636719   0.18786621 -0.0451355   0.31201172  0.09185791  0.06359863] 3   2 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.68066406  0.26708984  0.01407623  0.28857422  0.1361084  -0.01681519] 3   0 \n",
      "[-0.7421875   0.33520508 -0.08044434  0.3552246   0.11096191 -0.04754639] 3   2 \n",
      "[-0.734375    0.2043457  -0.02694702  0.30664062  0.07592773  0.03540039] 3   5 \n",
      "[-0.7470703   0.18383789 -0.00684357  0.34204102  0.09222412  0.07897949] 3   4 \n",
      "[-0.74658203  0.22314453 -0.05282593  0.30688477  0.05981445  0.05065918] 3   1 \n",
      "[-0.6699219   0.2442627  -0.03092957  0.3203125   0.1352539  -0.05130005] 3   0 \n",
      "[-0.7363281   0.20471191 -0.0218811   0.3059082   0.11791992  0.07196045] 3   5 \n",
      "[-0.76708984  0.29907227 -0.0758667   0.33935547  0.09185791 -0.0447998 ] 3   1 \n",
      "[-0.73046875  0.2536621  -0.03295898  0.31689453  0.11108398  0.00453186] 3   1 \n",
      "[-0.7265625   0.21984863 -0.05303955  0.34521484  0.05908203  0.04354858] 3   3 Match 20\n",
      "\n",
      "[-0.7661133   0.19641113 -0.0329895   0.30566406  0.07092285  0.06951904] 3   2 \n",
      "[-0.7504883   0.19458008 -0.06561279  0.31347656  0.11541748  0.05044556] 3   2 \n",
      "[-0.70996094  0.26220703 -0.03219604  0.31811523  0.07965088  0.05328369] 3   4 \n",
      "[-0.7368164   0.23071289 -0.02464294  0.32080078  0.0769043   0.04464722] 3   4 \n",
      "[-0.7519531   0.25585938 -0.09222412  0.3371582   0.08233643 -0.01565552] 3   5 \n",
      "[-0.7451172   0.21105957 -0.01194     0.31689453  0.09454346  0.05517578] 3   1 \n",
      "[-0.7626953   0.21875    -0.0271759   0.31713867  0.09887695  0.04473877] 3   1 \n",
      "[-0.77441406  0.37182617 -0.08007812  0.32495117  0.06176758 -0.08709717] 1   4 \n",
      "[-0.765625    0.21936035 -0.0280304   0.32836914  0.04928589  0.07727051] 3   5 \n",
      "[-0.73291016  0.20837402 -0.02304077  0.328125    0.08026123  0.03796387] 3   1 \n",
      "[-0.6269531   0.24255371  0.01306915  0.3034668   0.16174316  0.07397461] 3   2 \n",
      "[-0.7446289   0.40698242 -0.14880371  0.3857422   0.12133789 -0.12145996] 1   2 \n",
      "[-0.75878906  0.28735352 -0.05551147  0.34106445  0.07922363 -0.05239868] 3   1 \n",
      "[-0.74560547  0.20092773  0.01446533  0.2680664   0.14208984  0.02104187] 3   2 \n",
      "[-0.7055664   0.22973633 -0.03970337  0.31860352  0.04351807  0.06161499] 3   1 \n",
      "[-0.7036133   0.24987793 -0.0070076   0.29638672  0.12280273  0.01004791] 3   2 \n",
      "[-0.70947266  0.2861328  -0.0305481   0.33325195  0.08734131  0.00902557] 3   5 \n",
      "[-0.79296875  0.2265625  -0.08612061  0.35498047  0.10113525  0.01852417] 3   5 \n",
      "[-0.76464844  0.18713379 -0.04196167  0.34033203  0.07452393  0.05554199] 3   2 \n",
      "[-0.7055664   0.23510742 -0.01626587  0.3071289   0.12341309  0.03839111] 3   3 Match 21\n",
      "\n",
      "[-0.7167969   0.19836426 -0.00504303  0.3269043   0.1206665   0.05511475] 3   1 \n",
      "[-0.7080078   0.27612305 -0.01338196  0.29833984  0.08166504  0.07904053] 3   2 \n",
      "[-0.7246094   0.18127441 -0.01394653  0.3095703   0.105896    0.04849243] 3   1 \n",
      "[-0.73779297  0.19433594 -0.02989197  0.3869629   0.08483887  0.08251953] 3   4 \n",
      "[-0.6928711   0.23291016  0.00327492  0.33569336  0.10900879  0.05181885] 3   3 Match 22\n",
      "\n",
      "[-0.62353516  0.33251953  0.04425049  0.31884766  0.17480469 -0.00498199] 1   4 \n",
      "[-0.77001953  0.2734375  -0.08569336  0.31835938  0.05618286 -0.01361847] 3   3 Match 23\n",
      "\n",
      "[-0.7558594   0.18811035 -0.0291748   0.38891602  0.06646729  0.08746338] 3   3 Match 24\n",
      "\n",
      "[-0.7441406   0.21191406 -0.08990479  0.35595703  0.05493164  0.06884766] 3   5 \n",
      "[-0.6542969   0.25756836 -0.02436829  0.31201172  0.11224365  0.06903076] 3   4 \n",
      "[-0.7626953   0.31201172 -0.13635254  0.34716797  0.02980042 -0.074646  ] 3   1 \n",
      "[-6.9873047e-01  2.2070312e-01 -2.7441978e-04  2.7978516e-01\n",
      "  1.3171387e-01  2.9586792e-02] 3   2 \n",
      "[-0.7397461   0.18164062 -0.03955078  0.32861328  0.08526611  0.05200195] 3   1 \n",
      "[-0.8041992   0.2364502  -0.09020996  0.36791992  0.05212402 -0.00776291] 3   3 Match 25\n",
      "\n",
      "[-0.74658203  0.1899414  -0.07409668  0.33911133  0.04870605  0.00914001] 3   2 \n",
      "[-0.61035156  0.29125977  0.06665039  0.2993164   0.21264648  0.01643372] 3   3 Match 26\n",
      "\n",
      "[-0.7607422   0.25268555 -0.05606079  0.35107422  0.10632324  0.07244873] 3   5 \n",
      "[-0.71777344  0.19311523 -0.02261353  0.32788086  0.05123901  0.03878784] 3   4 \n",
      "[-0.8144531   0.25317383 -0.09747314  0.3774414   0.11901855  0.04986572] 3   4 \n",
      "[-0.7548828   0.20532227 -0.05279541  0.32641602  0.04382324  0.05508423] 3   5 \n",
      "[-0.7973633   0.39233398 -0.12805176  0.35009766  0.02262878 -0.14758301] 1   5 \n",
      "[-0.7314453   0.27490234 -0.0406189   0.30200195  0.16210938 -0.00202942] 3   1 \n",
      "[-0.7753906   0.18041992 -0.02624512  0.30200195  0.12457275  0.0025177 ] 3   4 \n",
      "[-0.76171875  0.17687988 -0.06439209  0.34692383  0.0647583   0.09020996] 3   3 Match 27\n",
      "\n",
      "[-0.69628906  0.2512207  -0.06292725  0.3125      0.11334229  0.06793213] 3   3 Match 28\n",
      "\n",
      "[-0.77734375  0.29101562 -0.11547852  0.31396484  0.10546875 -0.06222534] 3   5 \n",
      "[-0.73583984  0.2401123  -0.02975464  0.28076172  0.08959961  0.03326416] 3   3 Match 29\n",
      "\n",
      "[-0.6669922   0.39086914  0.00210953  0.31689453  0.18432617 -0.09222412] 1   2 \n",
      "[-0.7583008   0.30737305 -0.06622314  0.33862305  0.09387207  0.03004456] 3   3 Match 30\n",
      "\n",
      "[-0.78515625  0.28637695 -0.09289551  0.3244629   0.09875488 -0.04446411] 3   1 \n",
      "[-0.6801758   0.24902344 -0.06005859  0.30493164  0.09069824  0.01438904] 3   3 Match 31\n",
      "\n",
      "[-0.59814453  0.28320312  0.07043457  0.29101562  0.1907959   0.03512573] 3   2 \n",
      "[-0.63916016  0.2927246   0.01487732  0.3046875   0.13537598  0.05889893] 3   5 \n",
      "[-0.75634766  0.23608398 -0.05285645  0.32177734  0.06341553  0.03448486] 3   5 \n",
      "[-0.7939453   0.14807129 -0.0607605   0.34936523  0.04968262  0.06060791] 3   3 Match 32\n",
      "\n",
      "[-0.68896484  0.23034668 -0.01499939  0.30029297  0.10638428  0.05883789] 3   5 \n",
      "[-0.7236328   0.21557617 -0.0559082   0.33813477  0.06719971  0.04528809] 3   3 Match 33\n",
      "\n",
      "[-0.5415039   0.35424805  0.06048584  0.29418945  0.21228027 -0.04574585] 1   4 \n",
      "[-0.72753906  0.26123047 -0.08624268  0.39135742  0.097229    0.08239746] 3   1 \n",
      "[-0.64501953  0.27734375  0.00230408  0.29174805  0.12902832  0.0463562 ] 3   2 \n",
      "[-0.74316406  0.25732422 -0.07897949  0.35058594  0.06933594  0.02372742] 3   5 \n",
      "[-0.7006836   0.25854492 -0.05032349  0.31640625  0.08398438  0.02960205] 3   3 Match 34\n",
      "\n",
      "[-0.7548828   0.24084473 -0.03909302  0.33862305  0.08215332  0.02662659] 3   3 Match 35\n",
      "\n",
      "[-0.58447266  0.31958008  0.02365112  0.29663086  0.16955566  0.06585693] 1   1 Match 36\n",
      "\n",
      "[-0.5649414   0.27978516 -0.06988525  0.33642578  0.17504883  0.03152466] 3   1 \n",
      "[-0.6489258   0.3173828  -0.01608276  0.26586914  0.13476562  0.05459595] 1   0 \n",
      "[-0.7011719   0.34301758 -0.01539612  0.2998047   0.15148926 -0.06921387] 1   0 \n",
      "[-0.6015625   0.26904297  0.01477051  0.29760742  0.15588379  0.07263184] 3   4 \n",
      "[-0.7597656   0.2915039  -0.08911133  0.33642578  0.06567383 -0.00879669] 3   1 \n",
      "[-0.68603516  0.21081543 -0.02697754  0.32128906  0.0703125   0.05459595] 3   5 \n",
      "[-0.6098633   0.2602539   0.05163574  0.29589844  0.19726562 -0.00532913] 3   2 \n",
      "[-0.7182617   0.24780273 -0.0486145   0.36279297  0.05477905  0.05264282] 3   5 \n",
      "[-0.80810547  0.2512207  -0.17089844  0.3930664   0.08001709  0.03411865] 3   5 \n",
      "[-0.77734375  0.18225098 -0.08947754  0.34301758  0.05780029  0.0579834 ] 3   0 \n",
      "[-0.6801758   0.19958496 -0.00117111  0.33520508  0.14025879  0.04101562] 3   1 \n",
      "[-0.7705078   0.18200684 -0.09039307  0.38183594  0.02279663  0.05175781] 3   1 \n",
      "[-0.70166016  0.23266602 -0.04278564  0.33154297  0.07116699  0.06137085] 3   2 \n",
      "[-0.7001953   0.19921875 -0.02412415  0.29956055  0.12792969  0.08148193] 3   3 Match 37\n",
      "\n",
      "[-0.7109375   0.19592285 -0.04562378  0.32421875  0.06915283  0.07061768] 3   3 Match 38\n",
      "\n",
      "[-0.75634766  0.23034668 -0.05477905  0.31079102  0.06585693  0.03305054] 3   1 \n",
      "[-0.7285156   0.21179199 -0.01860046  0.32910156  0.07043457  0.06414795] 3   1 \n",
      "[-0.65625     0.2619629   0.03059387  0.31201172  0.12805176  0.03053284] 3   2 \n",
      "[-0.5341797   0.3798828   0.02909851  0.2668457   0.17272949 -0.04788208] 1   3 \n",
      "[-0.7246094   0.19885254 -0.05297852  0.3408203   0.07678223  0.05749512] 3   3 Match 39\n",
      "\n",
      "[-0.5883789   0.27612305  0.06280518  0.2890625   0.19470215  0.02500916] 3   1 \n",
      "[-0.69433594  0.26220703 -0.02189636  0.3203125   0.07922363  0.04324341] 3   1 \n",
      "[-0.81396484  0.20446777 -0.13989258  0.34838867  0.10357666  0.01885986] 3   4 \n",
      "[-0.7290039   0.21228027 -0.00801086  0.34057617  0.06555176  0.07000732] 3   5 \n",
      "[-0.8432617   0.19580078 -0.10424805  0.35131836  0.02632141 -0.01184082] 3   3 Match 40\n",
      "\n",
      "[-0.69140625  0.23535156  0.01913452  0.32861328  0.14221191  0.03585815] 3   3 Match 41\n",
      "\n",
      "[-0.68896484  0.2163086  -0.02128601  0.32739258  0.09924316  0.06109619] 3   1 \n",
      "[-0.6948242   0.18896484 -0.05963135  0.28686523  0.0635376   0.03948975] 3   3 Match 42\n",
      "\n",
      "[-0.7739258   0.18395996 -0.07299805  0.3371582   0.06585693  0.06573486] 3   3 Match 43\n",
      "\n",
      "[-0.7182617   0.21801758 -0.0133667   0.32128906  0.10369873  0.0491333 ] 3   0 \n",
      "[-0.76953125  0.15637207 -0.05899048  0.31933594  0.08453369  0.05209351] 3   2 \n",
      "[-0.7373047   0.20996094  0.01384735  0.29467773  0.09698486  0.02961731] 3   5 \n",
      "[-0.703125    0.17993164 -0.03924561  0.30639648  0.08294678  0.04956055] 3   2 \n",
      "[-0.7167969   0.2286377  -0.0307312   0.36743164  0.06732178  0.06246948] 3   4 \n",
      "[-0.71875     0.19384766 -0.08239746  0.36669922  0.05938721  0.06774902] 3   3 Match 44\n",
      "\n",
      "[-0.7241211   0.18566895 -0.06616211  0.33007812  0.10296631  0.08105469] 3   4 \n",
      "[-0.71484375  0.21459961 -0.03463745  0.32177734  0.10162354  0.04119873] 3   4 \n",
      "[-0.72558594  0.20690918 -0.02096558  0.31420898  0.07733154  0.0390625 ] 3   1 \n",
      "[-0.74853516  0.18713379 -0.04351807  0.32836914  0.06787109  0.0557251 ] 3   4 \n",
      "[-0.7006836   0.22790527 -0.05758667  0.3388672   0.17614746  0.05789185] 3   2 \n",
      "[-0.7216797   0.19152832 -0.0328064   0.31689453  0.0760498   0.06408691] 3   1 \n",
      "[-0.7529297   0.20703125 -0.04696655  0.30541992  0.08361816  0.03305054] 3   2 \n",
      "[-0.72509766  0.23266602 -0.04483032  0.32104492  0.09100342  0.05950928] 3   3 Match 45\n",
      "\n",
      "[-0.72558594  0.18701172 -0.05914307  0.3173828   0.07409668  0.04693604] 3   1 \n",
      "[-0.67333984  0.2626953   0.01716614  0.30444336  0.11206055  0.04611206] 3   3 Match 46\n",
      "\n",
      "[-0.76416016  0.19213867 -0.02360535  0.30151367  0.09222412  0.05938721] 3   0 \n",
      "[-0.7729492   0.22045898 -0.05874634  0.35620117  0.10107422  0.00447845] 3   3 Match 47\n",
      "\n",
      "[-0.7128906   0.22595215 -0.05212402  0.3046875   0.06530762  0.03143311] 3   1 \n",
      "[-0.6850586   0.2175293  -0.03414917  0.28442383  0.11053467  0.06246948] 3   0 \n",
      "[-0.7182617   0.19360352 -0.01309204  0.32226562  0.1015625   0.04293823] 3   0 \n",
      "[-0.7348633   0.24853516 -0.03500366  0.35253906  0.06982422  0.04769897] 3   1 \n",
      "[-0.6953125   0.1920166  -0.01838684  0.30322266  0.09686279  0.07641602] 3   3 Match 48\n",
      "\n",
      "[-0.73779297  0.17114258 -0.05014038  0.32763672  0.09222412  0.10339355] 3   5 \n",
      "[-0.7597656   0.20666504 -0.03726196  0.34228516  0.09503174  0.06384277] 3   0 \n",
      "[-0.7373047   0.25073242 -0.05175781  0.34277344  0.05361938  0.0423584 ] 3   4 \n",
      "[-0.72265625  0.1772461  -0.03509521  0.3232422   0.09509277  0.06524658] 3   2 \n",
      "[-6.6748047e-01  2.4108887e-01 -2.2757053e-04  2.9443359e-01\n",
      "  1.0107422e-01  4.2175293e-02] 3   2 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7416992   0.22265625 -0.03390503  0.29296875  0.09509277  0.03497314] 3   1 \n",
      "[-0.6040039   0.2800293   0.05691528  0.30371094  0.19091797 -0.00535965] 3   3 Match 49\n",
      "\n",
      "[-0.70458984  0.18395996 -0.05044556  0.32739258  0.06445312  0.05926514] 3   5 \n",
      "[-0.7290039   0.18933105 -0.03117371  0.32666016  0.07318115  0.04653931] 3   4 \n",
      "[-0.67871094  0.2475586  -0.00233078  0.3190918   0.1036377   0.03839111] 3   1 \n",
      "[-0.7265625   0.21765137 -0.05679321  0.32543945  0.0970459   0.0737915 ] 3   0 \n",
      "[-0.8027344   0.14367676 -0.0869751   0.32348633  0.03903198  0.04925537] 3   1 \n",
      "[-0.66503906  0.2590332   0.02285767  0.28735352  0.12414551  0.05374146] 3   3 Match 50\n",
      "\n",
      "[-0.67285156  0.23291016  0.00088406  0.28735352  0.11883545  0.05755615] 3   2 \n",
      "[-0.70214844  0.22473145 -0.05307007  0.30371094  0.09667969  0.01390076] 3   2 \n",
      "[-0.7026367   0.23303223 -0.03442383  0.2939453   0.14892578  0.0296936 ] 3   5 \n",
      "[-0.69140625  0.20410156 -0.05715942  0.3305664   0.08728027  0.0496521 ] 3   3 Match 51\n",
      "\n",
      "[-0.7631836   0.24084473 -0.13574219  0.32666016  0.16796875  0.01416016] 3   4 \n",
      "[-0.70654297  0.20251465 -0.01947021  0.3017578   0.0880127   0.0539856 ] 3   1 \n",
      "[-0.63916016  0.26904297  0.00994873  0.31835938  0.17126465  0.04885864] 3   1 \n",
      "[-0.6557617   0.26489258  0.01011658  0.28881836  0.15075684  0.04162598] 3   1 \n",
      "[-0.7910156   0.22497559 -0.06274414  0.34545898  0.11273193  0.01625061] 3   0 \n",
      "[-0.70947266  0.20397949 -0.00297546  0.29589844  0.11517334  0.05703735] 3   4 \n",
      "[-0.71972656  0.17163086 -0.02693176  0.31982422  0.09741211  0.05236816] 3   1 \n",
      "[-0.69628906  0.2006836  -0.07904053  0.36206055  0.08795166  0.05892944] 3   3 Match 52\n",
      "\n",
      "[-0.71728516  0.17980957 -0.04025269  0.33276367  0.090271    0.0736084 ] 3   1 \n",
      "[-0.6479492   0.3720703   0.01184082  0.3005371   0.16638184 -0.08770752] 1   1 Match 53\n",
      "\n",
      "[-0.6879883   0.20483398  0.01605225  0.31713867  0.12493896  0.05245972] 3   3 Match 54\n",
      "\n",
      "[-0.7441406   0.20385742 -0.05517578  0.3383789   0.08282471  0.06488037] 3   0 \n",
      "[-0.72265625  0.29467773 -0.037323    0.32617188  0.06903076  0.01252747] 3   5 \n",
      "[-0.7319336   0.25952148 -0.07043457  0.32983398  0.01747131  0.03268433] 3   0 \n",
      "[-0.7553711   0.21228027 -0.02616882  0.3569336   0.07391357  0.03973389] 3   3 Match 55\n",
      "\n",
      "[-0.7446289   0.20812988 -0.04138184  0.34716797  0.06878662  0.05368042] 3   2 \n",
      "[-0.6557617   0.21069336 -0.07086182  0.3173828   0.11199951  0.06445312] 3   0 \n",
      "[-0.70654297  0.17614746 -0.03292847  0.2980957   0.10516357  0.06066895] 3   0 \n",
      "[-0.67041016  0.29345703  0.05422974  0.3046875   0.19506836 -0.03674316] 3   4 \n",
      "[-0.74121094  0.2626953  -0.16430664  0.3239746   0.05508423 -0.03903198] 3   1 \n",
      "[-0.7319336   0.25805664 -0.0617981   0.32617188  0.07836914  0.02932739] 3   2 \n",
      "[-0.6557617   0.2322998  -0.02049255  0.30688477  0.12335205  0.07196045] 3   3 Match 56\n",
      "\n",
      "[-0.70947266  0.23718262 -0.0114975   0.28881836  0.08782959  0.0302887 ] 3   2 \n",
      "[-0.6953125   0.21508789 -0.02818298  0.34472656  0.06848145  0.05337524] 3   5 \n",
      "[-0.74365234  0.18591309 -0.09191895  0.34716797  0.06610107  0.07562256] 3   3 Match 57\n",
      "\n",
      "[-0.78271484  0.25146484 -0.06246948  0.34033203  0.11895752  0.01957703] 3   5 \n",
      "[-0.7636719   0.22497559 -0.02830505  0.32958984  0.08538818  0.07336426] 3   4 \n",
      "[-0.68115234  0.22851562  0.00179672  0.32177734  0.09735107  0.05288696] 3   4 \n",
      "[-0.7519531   0.21032715 -0.00253105  0.3449707   0.090271    0.05517578] 3   1 \n",
      "[-0.75341797  0.19580078 -0.03149414  0.29467773  0.07611084  0.04904175] 3   4 \n",
      "[-0.70166016  0.16381836 -0.04110718  0.3461914   0.09545898  0.06378174] 3   3 Match 58\n",
      "\n",
      "[-0.53808594  0.30493164  0.05615234  0.28466797  0.21459961  0.08905029] 1   0 \n",
      "[-0.72998047  0.16418457 -0.05834961  0.32763672  0.0617981   0.06341553] 3   3 Match 59\n",
      "\n",
      "[-0.7265625   0.24047852 -0.00772858  0.32739258  0.07073975  0.06317139] 3   3 Match 60\n",
      "\n",
      "[-0.77197266  0.19470215 -0.08758545  0.36743164  0.02078247  0.03701782] 3   3 Match 61\n",
      "\n",
      "[-0.7446289   0.20776367 -0.03240967  0.32421875  0.0526123   0.06311035] 3   4 \n",
      "[-0.7963867   0.1998291  -0.03887939  0.34179688  0.07849121  0.05169678] 3   0 \n",
      "[-0.7524414   0.23632812 -0.06567383  0.3515625   0.07727051  0.02293396] 3   1 \n",
      "[-0.74121094  0.20092773 -0.05203247  0.3540039   0.05447388  0.0625    ] 3   4 \n",
      "[-0.7114258   0.20959473 -0.04022217  0.3503418   0.0635376   0.0725708 ] 3   1 \n",
      "[-0.7817383   0.17785645 -0.06222534  0.35424805  0.04794312  0.05599976] 3   1 \n",
      "[-0.5917969   0.2919922   0.04321289  0.29223633  0.22558594  0.037323  ] 3   0 \n",
      "[-0.73291016  0.27905273 -0.06216431  0.31689453  0.13757324 -0.00238419] 3   2 \n",
      "[-0.7475586   0.20458984 -0.04644775  0.33032227  0.07452393  0.05963135] 3   4 \n",
      "[-0.72998047  0.17370605 -0.03427124  0.32592773  0.07080078  0.06835938] 3   1 \n",
      "[-0.5961914   0.3083496   0.05447388  0.29125977  0.21704102  0.00251389] 1   2 \n",
      "[-0.73291016  0.26757812 -0.07275391  0.30981445  0.0670166   0.0165863 ] 3   4 \n",
      "[-0.71728516  0.22937012 -0.03665161  0.32250977  0.06726074  0.05926514] 3   1 \n",
      "[-0.7397461   0.16662598 -0.04803467  0.31982422  0.08557129  0.06677246] 3   1 \n",
      "[-0.70410156  0.21386719 -0.00949097  0.30688477  0.11187744  0.05136108] 3   3 Match 62\n",
      "\n",
      "[-0.734375    0.21362305 -0.01409149  0.34814453  0.08660889  0.08978271] 3   5 \n",
      "[-0.6894531   0.2232666  -0.01415253  0.32617188  0.14001465  0.0508728 ] 3   5 \n",
      "[-0.6166992   0.25927734  0.04919434  0.30395508  0.19384766  0.00714493] 3   4 \n",
      "[-0.69189453  0.21899414 -0.03051758  0.29663086  0.11425781  0.03460693] 3   2 \n",
      "[-0.7895508   0.2322998  -0.0668335   0.31396484  0.04876709  0.0011282 ] 3   5 \n",
      "[-0.76464844  0.23339844 -0.05053711  0.31323242  0.08032227  0.03369141] 3   3 Match 63\n",
      "\n",
      "[-7.5537109e-01  2.5537109e-01 -1.4697266e-01  3.0566406e-01\n",
      "  6.0150146e-02 -3.9505959e-04] 3   4 \n",
      "[-0.7294922   0.21899414 -0.0512085   0.33276367  0.10223389  0.0723877 ] 3   1 \n",
      "[-0.69970703  0.24572754 -0.0150528   0.34033203  0.08709717  0.04241943] 3   3 Match 64\n",
      "\n",
      "[-0.73046875  0.16552734 -0.03108215  0.3005371   0.07873535  0.06378174] 3   1 \n",
      "[-0.7192383   0.23242188 -0.02824402  0.29101562  0.08618164  0.03062439] 3   4 \n",
      "[-0.6821289   0.20202637 -0.01131439  0.32421875  0.10302734  0.07232666] 3   0 \n",
      "[-0.60009766  0.2529297   0.01878357  0.27929688  0.17944336  0.06018066] 3   5 \n",
      "[-0.7207031   0.2607422  -0.04473877  0.3635254   0.11749268  0.04037476] 3   5 \n",
      "[-0.73291016  0.23498535 -0.03143311  0.34448242  0.09210205  0.03762817] 3   3 Match 65\n",
      "\n",
      "[-0.7832031   0.13635254 -0.05038452  0.37573242  0.04876709  0.05725098] 3   3 Match 66\n",
      "\n",
      "[-0.7207031   0.24731445 -0.00479889  0.3227539   0.07946777  0.0158844 ] 3   3 Match 67\n",
      "\n",
      "[-0.7167969   0.22546387  0.01994324  0.33129883  0.12927246  0.01905823] 3   4 \n",
      "[-0.7128906   0.20751953 -0.01986694  0.31713867  0.06069946  0.07409668] 3   4 \n",
      "[-0.72021484  0.23608398 -0.00584412  0.33276367  0.08294678  0.06268311] 3   2 \n",
      "[-0.74658203  0.22387695  0.00242996  0.3149414   0.12585449  0.05029297] 3   1 \n",
      "[-0.7651367   0.17492676 -0.0496521   0.34448242  0.08099365  0.06695557] 3   1 \n",
      "[-0.70947266  0.1920166  -0.02046204  0.30810547  0.13696289  0.07244873] 3   0 \n",
      "[-0.69628906  0.22351074 -0.02392578  0.31396484  0.09454346  0.06213379] 3   5 \n",
      "[-0.75439453  0.22668457 -0.05563354  0.31884766  0.09539795  0.03762817] 3   3 Match 68\n",
      "\n",
      "[-0.72802734  0.24511719 -0.02674866  0.30908203  0.08648682  0.06866455] 3   5 \n",
      "[-0.74560547  0.19970703 -0.01982117  0.31030273  0.09326172  0.03961182] 3   1 \n",
      "[-0.72753906  0.20690918 -0.02148438  0.35791016  0.07476807  0.06347656] 3   5 \n",
      "[-0.68359375  0.20581055  0.00508118  0.32739258  0.14831543  0.03146362] 3   4 \n",
      "[-0.73095703  0.2055664  -0.0579834   0.31860352  0.09857178  0.06341553] 3   3 Match 69\n",
      "\n",
      "[-0.78222656  0.21203613 -0.03329468  0.3173828   0.0226593   0.00715637] 3   2 \n",
      "[-0.71240234  0.22155762 -0.04476929  0.33129883  0.08117676  0.05361938] 3   0 \n",
      "[-0.7441406   0.20581055 -0.0609436   0.36523438  0.09820557  0.07446289] 3   1 \n",
      "[-0.7524414   0.26367188 -0.00213051  0.29077148  0.10577393  0.05377197] 3   5 \n",
      "[-0.75634766  0.17211914 -0.08953857  0.37646484  0.05526733  0.07702637] 3   3 Match 70\n",
      "\n",
      "[-0.7207031   0.21130371 -0.01573181  0.33618164  0.06072998  0.05853271] 3   4 \n",
      "[-0.72314453  0.17712402 -0.00788116  0.29492188  0.12121582  0.04681396] 3   1 \n",
      "[-0.74121094  0.22399902 -0.03231812  0.36865234  0.08929443  0.04675293] 3   1 \n",
      "[-0.76464844  0.18798828 -0.06054688  0.33544922  0.0725708   0.07250977] 3   3 Match 71\n",
      "\n",
      "[-0.7363281   0.25195312 -0.08349609  0.33422852  0.06878662  0.03274536] 3   4 \n",
      "[-0.71777344  0.19238281 -0.03161621  0.3190918   0.07415771  0.04187012] 3   1 \n",
      "[-0.72314453  0.19812012 -0.01428986  0.33862305  0.07885742  0.03741455] 3   3 Match 72\n",
      "\n",
      "[-0.7001953   0.2052002   0.0188446   0.29541016  0.13391113  0.06744385] 3   5 \n",
      "[-0.7167969   0.2097168   0.00616455  0.30151367  0.13098145  0.03007507] 3   2 \n",
      "[-0.7246094   0.18066406  0.01795959  0.33422852  0.12213135  0.05697632] 3   3 Match 73\n",
      "\n",
      "[-0.77441406  0.18603516 -0.05780029  0.31054688  0.07397461  0.04742432] 3   3 Match 74\n",
      "\n",
      "[-0.66796875  0.2084961   0.02679443  0.32080078  0.1517334   0.02835083] 3   2 \n",
      "[-0.73046875  0.20739746 -0.03219604  0.3100586   0.08032227  0.03955078] 3   3 Match 75\n",
      "\n",
      "[-6.7578125e-01  2.1154785e-01  4.0745735e-04  3.0371094e-01\n",
      "  1.5466309e-01  4.6966553e-02] 3   0 \n",
      "[-0.70996094  0.22009277 -0.02874756  0.3461914   0.06274414  0.08227539] 3   4 \n",
      "[-0.72265625  0.21032715 -0.01250458  0.31591797  0.11517334  0.04699707] 3   1 \n",
      "[-0.71240234  0.22619629 -0.03314209  0.30273438  0.10375977  0.05737305] 3   3 Match 76\n",
      "\n",
      "[-0.73535156  0.16088867 -0.03713989  0.32666016  0.06518555  0.05792236] 3   3 Match 77\n",
      "\n",
      "[-0.75683594  0.18273926 -0.05764771  0.30541992  0.0579834   0.06033325] 3   5 \n",
      "[-0.7583008   0.16723633 -0.022995    0.37231445  0.07824707  0.04321289] 3   4 \n",
      "[-0.7441406   0.17980957 -0.09216309  0.36401367  0.07611084  0.03903198] 3   1 \n",
      "[-0.765625    0.22351074 -0.04003906  0.3251953   0.0604248   0.03656006] 3   2 \n",
      "[-7.67578125e-01  2.63183594e-01 -4.34570312e-02  3.21777344e-01\n",
      "  1.16882324e-01 -5.07831573e-04] 3   4 \n",
      "[-0.7348633   0.20922852 -0.00161934  0.31567383  0.11303711  0.01571655] 3   1 \n",
      "[-0.65966797  0.3095703   0.04379272  0.28466797  0.22546387  0.01499176] 1   1 Match 78\n",
      "\n",
      "[-0.7729492   0.2644043  -0.11102295  0.30395508  0.01264191 -0.03909302] 3   1 \n",
      "[-0.70214844  0.23132324 -0.00530624  0.31689453  0.14245605  0.03179932] 3   2 \n",
      "[-0.71484375  0.20251465 -0.01067352  0.32714844  0.10882568  0.05737305] 3   0 \n",
      "[-0.73779297  0.19091797 -0.00816345  0.3251953   0.08520508  0.03878784] 3   4 \n",
      "[-0.7470703   0.18933105 -0.04833984  0.328125    0.07794189  0.05059814] 3   1 \n",
      "[-0.69091797  0.2590332  -0.01166534  0.33032227  0.10882568  0.08630371] 3   4 \n",
      "[-0.7807617   0.25878906 -0.05548096  0.31958008  0.08868408  0.02601624] 3   1 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.80615234  0.23620605 -0.0894165   0.34936523  0.05197144  0.00721359] 3   0 \n",
      "[-0.765625    0.24487305 -0.08178711  0.33081055  0.0557251   0.0098877 ] 3   1 \n",
      "[-0.73046875  0.21240234 -0.04821777  0.33081055  0.07977295  0.09130859] 3   5 \n",
      "[-0.74609375  0.22241211 -0.0435791   0.3178711   0.11053467  0.05831909] 3   4 \n",
      "[-0.69921875  0.19372559 -0.02331543  0.3310547   0.09246826  0.07867432] 3   5 \n",
      "[-0.7241211   0.23657227 -0.04452515  0.33276367  0.06536865  0.06341553] 3   1 \n",
      "[-0.7128906   0.34399414 -0.08209229  0.31640625  0.28759766 -0.09570312] 1   3 \n",
      "[-0.75439453  0.19299316 -0.05715942  0.35913086  0.08892822  0.05474854] 3   2 \n",
      "[-0.75683594  0.2175293  -0.01965332  0.33032227  0.06793213  0.03668213] 3   2 \n",
      "[-0.7158203   0.24853516 -0.00390434  0.28198242  0.11743164  0.02684021] 3   1 \n",
      "[-0.7104492   0.20581055  0.02284241  0.31518555  0.1385498   0.04937744] 3   4 \n",
      "[-0.7216797   0.18249512 -0.05578613  0.35595703  0.07012939  0.08666992] 3   1 \n",
      "[-0.7504883   0.20874023 -0.08508301  0.36108398  0.11193848  0.04553223] 3   4 \n",
      "[-0.7446289   0.23327637 -0.05258179  0.30810547  0.06240845  0.0291748 ] 3   3 Match 79\n",
      "\n",
      "[-0.7290039   0.18408203 -0.02693176  0.30493164  0.07891846  0.06567383] 3   3 Match 80\n",
      "\n",
      "[-0.7783203   0.19165039 -0.06130981  0.3486328   0.07067871  0.08612061] 3   3 Match 81\n",
      "\n",
      "[-0.73046875  0.24450684 -0.01071167  0.33520508  0.09179688  0.046875  ] 3   3 Match 82\n",
      "\n",
      "[-0.6791992   0.25878906 -0.01417542  0.28466797  0.11572266  0.01071167] 3   2 \n",
      "[-0.71435547  0.21582031 -0.03979492  0.34375     0.09136963  0.0668335 ] 3   1 \n",
      "[-0.76220703  0.21789551 -0.06463623  0.33569336  0.1206665   0.02992249] 3   3 Match 83\n",
      "\n",
      "[-0.6816406   0.25219727 -0.00909424  0.32373047  0.12316895  0.04379272] 3   3 Match 84\n",
      "\n",
      "[-0.7026367   0.15795898 -0.01791382  0.32861328  0.11846924  0.07519531] 3   1 \n",
      "[-0.78564453  0.17041016 -0.06396484  0.34155273  0.04858398  0.06555176] 3   1 \n",
      "[-0.67041016  0.2298584  -0.00588989  0.29174805  0.12878418  0.0350647 ] 3   1 \n",
      "[-0.6166992   0.27685547  0.00543976  0.29492188  0.17993164  0.02938843] 3   4 \n",
      "[-0.6748047   0.23986816 -0.02749634  0.2998047   0.10858154  0.02674866] 3   3 Match 85\n",
      "\n",
      "[-0.69189453  0.2133789   0.0114975   0.29711914  0.11779785  0.03540039] 3   3 Match 86\n",
      "\n",
      "[-0.7739258   0.1652832  -0.03762817  0.35351562  0.09051514  0.07507324] 3   1 \n",
      "[-6.6210938e-01  2.7783203e-01  3.7765503e-04  3.2617188e-01\n",
      "  1.2927246e-01  6.4697266e-02] 3   4 \n",
      "[-0.75634766  0.22961426 -0.06420898  0.34350586  0.06787109  0.05291748] 3   4 \n",
      "[-0.78271484  0.1796875  -0.03396606  0.28344727  0.09350586  0.01774597] 3   4 \n",
      "[-0.75097656  0.1928711  -0.01153564  0.3239746   0.07781982  0.04815674] 3   4 \n",
      "[-0.62060547  0.22619629  0.00959778  0.27661133  0.14208984  0.07562256] 3   0 \n",
      "[-0.7207031   0.21911621 -0.03897095  0.34277344  0.10437012  0.09637451] 3   1 \n",
      "[-0.75634766  0.18103027 -0.04064941  0.3100586   0.07849121  0.04101562] 3   4 \n",
      "[-0.7163086   0.21899414  0.00693893  0.31054688  0.12731934  0.03005981] 3   5 \n",
      "[-0.77783203  0.20275879 -0.06106567  0.34350586  0.09686279  0.07885742] 3   1 \n",
      "[-0.67871094  0.24682617  0.01609802  0.29882812  0.11737061  0.01593018] 3   3 Match 87\n",
      "\n",
      "[-0.62109375  0.26367188  0.03079224  0.2890625   0.17822266  0.05108643] 3   3 Match 88\n",
      "\n",
      "[-0.8222656   0.29614258 -0.07965088  0.37768555  0.09436035  0.0418396 ] 3   4 \n",
      "[-0.6845703   0.22839355 -0.00509262  0.29541016  0.10406494  0.0256958 ] 3   3 Match 89\n",
      "\n",
      "[-0.65478516  0.27807617 -0.01678467  0.30151367  0.14147949  0.06982422] 3   1 \n",
      "[-0.7055664   0.20422363 -0.04858398  0.3269043   0.0625      0.05560303] 3   4 \n",
      "[-0.7915039   0.26391602 -0.07702637  0.36279297  0.0847168   0.03829956] 3   2 \n",
      "[-0.68603516  0.1928711  -0.01300049  0.29223633  0.09155273  0.02890015] 3   0 \n",
      "[-0.6928711   0.23046875 -0.05062866  0.33154297  0.07940674  0.05056763] 3   1 \n",
      "[-0.7558594   0.20166016 -0.02604675  0.3334961   0.07122803  0.02581787] 3   1 \n",
      "[-0.76708984  0.18884277 -0.04116821  0.34155273  0.04168701  0.08435059] 3   5 \n",
      "[-0.5708008   0.31079102  0.0397644   0.30029297  0.21557617  0.03062439] 1   4 \n",
      "[-0.6660156   0.30737305 -0.04803467  0.3581543   0.07214355  0.02613831] 3   4 \n",
      "[-0.6381836   0.2668457  -0.04849243  0.28222656  0.1307373   0.02429199] 3   0 \n",
      "[-0.70996094  0.2241211  -0.04580688  0.36572266  0.08837891  0.06161499] 3   5 \n",
      "[-0.7084961   0.21398926 -0.01437378  0.30859375  0.12915039  0.04092407] 3   5 \n",
      "[-0.68652344  0.22338867 -0.04055786  0.35424805  0.09155273  0.08917236] 3   1 \n",
      "[-0.64941406  0.21484375 -0.04324341  0.31860352  0.11541748  0.07305908] 3   3 Match 90\n",
      "\n",
      "[-0.7714844   0.21313477 -0.02734375  0.36254883  0.05276489  0.03063965] 3   3 Match 91\n",
      "\n",
      "[-0.68896484  0.24304199 -0.01789856  0.3322754   0.10174561  0.05661011] 3   1 \n",
      "[-0.73291016  0.18566895 -0.02262878  0.34057617  0.11035156  0.05358887] 3   3 Match 92\n",
      "\n",
      "[-0.72314453  0.2319336   0.02078247  0.30981445  0.1138916   0.04205322] 3   4 \n",
      "[-0.62939453  0.29345703  0.02548218  0.3190918   0.17199707 -0.00159645] 3   3 Match 93\n",
      "\n",
      "[-0.7416992   0.18249512 -0.04669189  0.30566406  0.08276367  0.07885742] 3   5 \n",
      "[-0.67041016  0.26293945  0.00997925  0.3347168   0.12683105  0.06896973] 3   1 \n",
      "[-0.7636719   0.2454834  -0.03753662  0.32226562  0.10528564  0.02008057] 3   2 \n",
      "[-0.8105469   0.24487305 -0.08807373  0.3623047   0.0324707   0.03826904] 3   4 \n",
      "[-0.70166016  0.21533203 -0.05371094  0.31225586  0.10882568  0.05355835] 3   3 Match 94\n",
      "\n",
      "[-0.7885742   0.22595215 -0.11096191  0.38476562  0.0541687   0.03038025] 3   2 \n",
      "[-0.7319336   0.21643066 -0.0496521   0.30639648  0.08111572  0.04205322] 3   4 \n",
      "[-0.59228516  0.31201172  0.04241943  0.28979492  0.1743164   0.05917358] 1   3 \n",
      "[-0.68652344  0.23657227 -0.00610352  0.33544922  0.06445312  0.04971313] 3   4 \n",
      "[-0.7338867   0.27856445 -0.09564209  0.36938477  0.12078857 -0.0680542 ] 3   0 \n",
      "[-0.72509766  0.22802734 -0.02267456  0.2775879   0.0847168   0.05758667] 3   5 \n",
      "[-0.7236328   0.20947266 -0.00662231  0.33007812  0.09790039  0.05932617] 3   5 \n",
      "[-0.5996094   0.29345703  0.05645752  0.27368164  0.1817627   0.01411438] 1   2 \n",
      "[-0.7841797   0.26904297 -0.07775879  0.3190918   0.12432861  0.00431061] 3   1 \n",
      "[-0.7348633   0.19116211 -0.03051758  0.3305664   0.08984375  0.05172729] 3   4 \n",
      "[-0.7338867   0.19909668 -0.02496338  0.3461914   0.07202148  0.03417969] 3   2 \n",
      "[-0.57958984  0.27539062  0.0484314   0.29345703  0.1875      0.0266571 ] 3   3 Match 95\n",
      "\n",
      "[-0.6665039   0.22521973 -0.0262146   0.33496094  0.1171875   0.08465576] 3   2 \n",
      "[-0.65283203  0.20935059 -0.01780701  0.2939453   0.13317871  0.0401001 ] 3   1 \n",
      "[-0.6928711   0.265625   -0.00525284  0.31225586  0.08026123  0.03695679] 3   5 \n",
      "[-0.67822266  0.22485352 -0.0150528   0.30078125  0.12316895  0.06469727] 3   1 \n",
      "[-0.67333984  0.22509766 -0.0029068   0.28295898  0.11956787  0.02476501] 3   1 \n",
      "[-0.76660156  0.27563477 -0.07000732  0.30859375  0.19909668 -0.04800415] 3   3 Match 96\n",
      "\n",
      "[-0.7314453   0.21789551 -0.02615356  0.31811523  0.08087158  0.06610107] 3   3 Match 97\n",
      "\n",
      "[-0.71728516  0.18676758 -0.02340698  0.30541992  0.10443115  0.06408691] 3   4 \n",
      "[-0.77734375  0.24047852 -0.06060791  0.34448242  0.07720947  0.02932739] 3   4 \n",
      "[-0.7807617   0.23095703 -0.04452515  0.30664062  0.07629395 -0.01037598] 3   5 \n",
      "[-0.7319336   0.21484375 -0.02192688  0.32763672  0.0715332   0.07458496] 3   3 Match 98\n",
      "\n",
      "[-0.7246094   0.20385742 -0.01183319  0.31274414  0.08947754  0.05471802] 3   1 \n",
      "[-0.6933594   0.18786621 -0.02841187  0.34326172  0.11639404  0.07513428] 3   1 \n",
      "[-0.78222656  0.23986816 -0.10852051  0.4189453   0.10375977  0.08044434] 3   5 \n",
      "[-0.59472656  0.27001953  0.03918457  0.28271484  0.22277832  0.0164032 ] 3   2 \n",
      "[-0.7011719   0.1998291  -0.0553894   0.32055664  0.09869385  0.07086182] 3   3 Match 99\n",
      "\n",
      "[-0.73779297  0.22680664 -0.05322266  0.33642578  0.10076904  0.02565002] 3   4 \n",
      "[-0.59765625  0.30419922  0.0158844   0.28759766  0.1381836   0.02693176] 1   1 Match 100\n",
      "\n",
      "[-0.65185547  0.3251953   0.04821777  0.34033203  0.21679688  0.01325989] 3   3 Match 101\n",
      "\n",
      "[-0.7470703   0.23181152 -0.01496124  0.31982422  0.1229248   0.03933716] 3   1 \n",
      "[-0.7392578   0.20043945 -0.04248047  0.31030273  0.10205078  0.04315186] 3   3 Match 102\n",
      "\n",
      "[-0.69970703  0.1854248  -0.01629639  0.31420898  0.1171875   0.06829834] 3   1 \n",
      "[-0.74316406  0.24060059 -0.02267456  0.29638672  0.12451172  0.02032471] 3   3 Match 103\n",
      "\n",
      "[-0.6777344   0.22497559 -0.01013947  0.29223633  0.12805176  0.0289917 ] 3   2 \n",
      "[-0.6713867   0.21813965 -0.04031372  0.31762695  0.0758667   0.0770874 ] 3   5 \n",
      "[-0.7060547   0.21655273 -0.0043335   0.31274414  0.11724854  0.06628418] 3   1 \n",
      "[-0.5488281   0.31274414  0.06445312  0.32226562  0.2142334   0.09710693] 3   3 Match 104\n",
      "\n",
      "[-0.7470703   0.1706543  -0.00322914  0.3322754   0.11608887  0.04901123] 3   3 Match 105\n",
      "\n",
      "[-0.7446289   0.16796875 -0.03469849  0.33642578  0.09289551  0.05078125] 3   3 Match 106\n",
      "\n",
      "[-0.6152344   0.30004883 -0.00717545  0.27807617  0.140625    0.04043579] 1   3 \n",
      "[-0.74853516  0.23132324 -0.03068542  0.36083984  0.08270264  0.05053711] 3   4 \n",
      "[-0.71484375  0.23461914 -0.05944824  0.3474121   0.09136963  0.06799316] 3   3 Match 107\n",
      "\n",
      "[-0.70458984  0.2109375  -0.02073669  0.34643555  0.11065674  0.0592041 ] 3   3 Match 108\n",
      "\n",
      "[-0.671875    0.2763672  -0.00963593  0.29711914  0.12011719  0.01534271] 3   5 \n",
      "[-0.70458984  0.20568848 -0.04309082  0.3408203   0.06744385  0.06439209] 3   4 \n",
      "[-0.7519531   0.21569824 -0.02687073  0.35058594  0.07727051  0.05065918] 3   5 \n",
      "[-0.70996094  0.23254395 -0.06237793  0.29711914  0.05130005  0.06384277] 3   1 \n",
      "[-0.70654297  0.21057129 -0.04220581  0.32836914  0.10614014  0.07299805] 3   2 \n",
      "[-0.5996094   0.30273438  0.05050659  0.2998047   0.17089844  0.00717163] 1   4 \n",
      "[-0.7211914   0.25048828 -0.05438232  0.31323242  0.08361816  0.05773926] 3   2 \n",
      "[-0.7207031   0.18603516 -0.03845215  0.3203125   0.10626221  0.06079102] 3   2 \n",
      "[-0.6948242   0.2602539  -0.03619385  0.30297852  0.09136963  0.05380249] 3   3 Match 109\n",
      "\n",
      "[-0.76464844  0.25854492 -0.06634521  0.34716797  0.0647583   0.02720642] 3   5 \n",
      "[-0.7211914   0.25073242 -0.03585815  0.35668945  0.11602783  0.03105164] 3   2 \n",
      "[-0.7524414   0.2097168  -0.00208282  0.35253906  0.09490967  0.03890991] 3   0 \n",
      "[-0.64501953  0.29492188  0.00869751  0.3256836   0.12225342  0.05484009] 3   4 \n",
      "[-0.77441406  0.18017578 -0.02172852  0.35009766  0.05722046  0.05291748] 3   1 \n",
      "[-0.69091797  0.19274902 -0.00550461  0.33374023  0.13891602  0.04275513] 3   1 \n",
      "[-0.6845703   0.21716309 -0.05279541  0.32421875  0.10113525  0.07342529] 3   0 \n",
      "[-0.6723633   0.21447754  0.0046196   0.3317871   0.1149292   0.04211426] 3   3 Match 110\n",
      "\n",
      "[-0.7114258   0.22802734  0.00094509  0.32983398  0.08459473  0.0534668 ] 3   4 \n",
      "[-0.63964844  0.23413086  0.01646423  0.28295898  0.15771484  0.01496887] 3   0 \n",
      "[-0.73095703  0.16516113 -0.04141235  0.3503418   0.08868408  0.07836914] 3   4 \n",
      "[-6.6357422e-01  1.9372559e-01 -4.6849251e-04  3.0249023e-01\n",
      "  1.4978027e-01  6.3415527e-02] 3   3 Match 111\n",
      "\n",
      "[-0.72509766  0.22998047 -0.03378296  0.32666016  0.07653809  0.03796387] 3   5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.68603516  0.21984863 -0.01812744  0.31591797  0.11724854  0.05090332] 3   1 \n",
      "[-0.7050781   0.24206543 -0.07946777  0.31982422  0.09405518 -0.04904175] 3   3 Match 112\n",
      "\n",
      "[-0.7036133   0.25952148  0.00407791  0.32861328  0.07220459  0.0322876 ] 3   5 \n",
      "[-0.6816406   0.27124023 -0.01472473  0.27294922  0.10009766  0.01654053] 3   3 Match 113\n",
      "\n",
      "[-0.6220703   0.2298584  -0.02510071  0.28027344  0.13000488  0.07995605] 3   1 \n",
      "[-0.74902344  0.23156738 -0.0758667   0.3581543   0.06762695  0.05776978] 3   4 \n",
      "[-0.59472656  0.33154297  0.01397705  0.29223633  0.16821289  0.05038452] 1   3 \n",
      "[-0.74560547  0.24584961 -0.01348877  0.32641602  0.09698486  0.03890991] 3   3 Match 114\n",
      "\n",
      "[-0.68847656  0.2277832  -0.00590515  0.32666016  0.08514404  0.04934692] 3   3 Match 115\n",
      "\n",
      "[-0.7495117   0.25317383 -0.08862305  0.34521484  0.08050537  0.0032959 ] 3   5 \n",
      "[-0.7866211   0.17248535 -0.07299805  0.35839844  0.04769897  0.05377197] 3   1 \n",
      "[-0.6953125   0.19042969 -0.04199219  0.2939453   0.08648682  0.05749512] 3   1 \n",
      "[-0.70947266  0.2849121   0.004673    0.30493164  0.12609863  0.04962158] 3   4 \n",
      "[-0.75878906  0.18615723 -0.06121826  0.35205078  0.05371094  0.05581665] 3   5 \n",
      "[-0.6972656   0.15368652 -0.1027832   0.35083008  0.07165527  0.08795166] 3   3 Match 116\n",
      "\n",
      "[-0.6875      0.23278809 -0.02140808  0.31835938  0.11169434  0.06726074] 3   5 \n",
      "[-0.57714844  0.296875    0.02224731  0.30908203  0.20349121  0.06216431] 3   1 \n",
      "[-7.5585938e-01  2.7954102e-01 -1.1940002e-02  3.4399414e-01\n",
      "  7.3059082e-02 -1.6713142e-04] 3   5 \n",
      "[-0.68359375  0.24987793 -0.01834106  0.32421875  0.08447266  0.05252075] 3   1 \n",
      "[-0.68896484  0.19787598 -0.04226685  0.28393555  0.10095215  0.0574646 ] 3   3 Match 117\n",
      "\n",
      "[-0.6777344   0.23291016 -0.03016663  0.32006836  0.10174561  0.04483032] 3   5 \n",
      "[-0.74121094  0.1842041  -0.05047607  0.33374023  0.08117676  0.05923462] 3   4 \n",
      "[-0.77441406  0.20666504 -0.02758789  0.31811523  0.1050415   0.03497314] 3   1 \n",
      "[-0.6308594   0.2927246   0.01678467  0.3022461   0.15478516  0.04412842] 3   5 \n",
      "[-0.66064453  0.20166016 -0.01507568  0.30737305  0.11517334  0.05114746] 3   2 \n",
      "[-0.7763672   0.16882324 -0.04290771  0.34545898  0.05621338  0.03634644] 3   3 Match 118\n",
      "\n",
      "[-0.70654297  0.19152832 -0.02249146  0.3256836   0.1105957   0.074646  ] 3   3 Match 119\n",
      "\n",
      "[-0.734375    0.1817627  -0.02603149  0.34228516  0.06628418  0.05844116] 3   5 \n",
      "[-0.74560547  0.2166748   0.01136017  0.3466797   0.08557129  0.03692627] 3   5 \n",
      "[-0.74902344  0.21813965 -0.06137085  0.32885742  0.09863281  0.02392578] 3   2 \n",
      "[-0.7182617   0.22680664 -0.00618744  0.31762695  0.10272217  0.06268311] 3   0 \n",
      "[-0.640625    0.26220703  0.03860474  0.2927246   0.21557617  0.00844574] 3   1 \n",
      "[-0.72998047  0.22009277 -0.03689575  0.2944336   0.11181641  0.02926636] 3   1 \n",
      "[-0.73046875  0.22338867 -0.02049255  0.3137207   0.10375977  0.03033447] 3   3 Match 120\n",
      "\n",
      "[-0.7270508   0.22717285 -0.04574585  0.34326172  0.07574463  0.0916748 ] 3   1 \n",
      "[-0.6845703   0.20056152 -0.01309967  0.32739258  0.12866211  0.07116699] 3   0 \n",
      "[-0.6582031   0.21728516 -0.00231552  0.2836914   0.11791992  0.04516602] 3   2 \n",
      "[-0.6303711   0.30688477  0.0100174   0.27416992  0.14147949  0.04522705] 1   5 \n",
      "[-0.7236328   0.23413086 -0.01291656  0.33398438  0.11004639  0.0425415 ] 3   3 Match 121\n",
      "\n",
      "[-0.74365234  0.2319336  -0.04217529  0.32470703  0.0791626   0.07568359] 3   5 \n",
      "[-0.68408203  0.19873047 -0.01490784  0.29956055  0.08422852  0.05654907] 3   3 Match 122\n",
      "\n",
      "[-0.5961914   0.2565918  -0.00421524  0.27319336  0.17773438  0.06170654] 3   2 \n",
      "[-0.7368164   0.26904297  0.00522995  0.32299805  0.10272217  0.0229187 ] 3   1 \n",
      "[-0.72753906  0.26708984  0.0035038   0.3251953   0.1192627   0.02767944] 3   3 Match 123\n",
      "\n",
      "[-0.72558594  0.22253418 -0.00941467  0.32543945  0.13439941  0.02984619] 3   0 \n",
      "[-0.7080078   0.26611328 -0.05084229  0.33691406  0.11779785  0.0059967 ] 3   5 \n",
      "[-0.7504883   0.22460938 -0.05133057  0.35375977  0.08050537  0.07269287] 3   4 \n",
      "[-0.6616211   0.23779297  0.00559998  0.28295898  0.12646484  0.03074646] 3   2 \n",
      "[-0.77197266  0.18640137 -0.06280518  0.30004883  0.08843994  0.034729  ] 3   1 \n",
      "[-0.75927734  0.26123047 -0.05621338  0.29736328  0.14245605 -0.0044136 ] 3   3 Match 124\n",
      "\n",
      "[-0.7060547   0.19128418 -0.00615311  0.3215332   0.09851074  0.04656982] 3   1 \n",
      "[-0.7573242   0.15075684 -0.06817627  0.328125    0.07507324  0.07659912] 3   1 \n",
      "[-0.7050781   0.29760742 -0.1274414   0.32006836  0.11053467 -0.03485107] 3   2 \n",
      "[-0.7109375   0.24206543  0.00712967  0.32373047  0.0982666   0.01959229] 3   3 Match 125\n",
      "\n",
      "[-0.72509766  0.21289062 -0.0425415   0.35351562  0.08404541  0.02882385] 3   3 Match 126\n",
      "\n",
      "[-0.68066406  0.23815918 -0.01292419  0.28857422  0.11004639  0.04837036] 3   2 \n",
      "[-0.76464844  0.15612793 -0.05441284  0.36743164  0.03930664  0.07189941] 3   2 \n",
      "[-0.68310547  0.22338867 -0.00277138  0.2980957   0.11425781  0.06640625] 3   1 \n",
      "[-0.8046875   0.15100098 -0.05743408  0.38916016  0.08740234  0.07318115] 3   5 \n",
      "[-0.70996094  0.1899414  -0.03701782  0.35375977  0.08117676  0.09197998] 3   5 \n",
      "[-0.7524414   0.21362305 -0.02210999  0.33618164  0.08587646  0.01502991] 3   1 \n",
      "[-0.7294922   0.24865723 -0.04910278  0.31201172  0.09844971  0.05981445] 3   1 \n",
      "[-0.67822266  0.22387695 -0.03369141  0.31762695  0.10266113  0.06726074] 3   0 \n",
      "[-0.72998047  0.21862793 -0.04220581  0.3564453   0.06994629  0.07055664] 3   1 \n",
      "[-0.7265625   0.22229004 -0.03427124  0.32836914  0.10900879  0.03549194] 3   3 Match 127\n",
      "\n",
      "[-0.60546875  0.2915039   0.04888916  0.30395508  0.2019043   0.00939941] 3   4 \n",
      "[-0.7661133   0.24841309 -0.04141235  0.30859375  0.11535645  0.02529907] 3   1 \n",
      "[-0.6791992   0.21923828 -0.01670837  0.32641602  0.11352539  0.05047607] 3   3 Match 128\n",
      "\n",
      "[-0.5854492   0.29663086  0.06585693  0.30810547  0.22888184 -0.00219536] 3   5 \n",
      "[-0.63378906  0.29248047  0.01565552  0.27734375  0.12359619  0.04537964] 1   3 \n",
      "[-0.58447266  0.28149414  0.06256104  0.2944336   0.18725586  0.06256104] 3   3 Match 129\n",
      "\n",
      "[-0.5810547   0.33496094  0.08007812  0.29101562  0.16625977  0.00075817] 1   0 \n",
      "[-0.6933594   0.2890625  -0.08227539  0.32836914  0.1463623  -0.00281334] 3   1 \n",
      "[-0.65966797  0.27856445  0.00873566  0.31274414  0.19677734  0.01276398] 3   2 \n",
      "[-0.7729492   0.22607422 -0.08227539  0.3647461   0.08685303  0.06088257] 3   2 \n",
      "[-0.5986328   0.33374023  0.04330444  0.28076172  0.17175293  0.02677917] 1   4 \n",
      "[-0.68652344  0.21716309 -0.02775574  0.30371094  0.12365723  0.05966187] 3   3 Match 130\n",
      "\n",
      "[-0.64404297  0.28735352  0.0317688   0.31323242  0.20275879  0.02894592] 3   4 \n",
      "[-0.72998047  0.17163086 -0.04833984  0.35717773  0.07757568  0.07196045] 3   2 \n",
      "[-0.52441406  0.35083008  0.07281494  0.29174805  0.22875977 -0.03271484] 1   2 \n",
      "[-0.6767578   0.19824219 -0.00770187  0.3251953   0.09661865  0.04629517] 3   1 \n",
      "[-0.71972656  0.17993164 -0.05032349  0.29638672  0.04644775  0.06945801] 3   0 \n",
      "[-0.71728516  0.19921875 -0.02180481  0.31445312  0.08648682  0.05435181] 3   1 \n",
      "[-0.73095703  0.21276855 -0.03985596  0.35205078  0.07537842  0.06604004] 3   1 \n",
      "[-0.71972656  0.15270996 -0.05258179  0.34204102  0.08752441  0.09387207] 3   2 \n",
      "[-0.7060547   0.22753906 -0.00098133  0.29711914  0.11437988  0.01451874] 3   5 \n",
      "[-0.74560547  0.20629883 -0.0411377   0.36499023  0.06390381  0.04641724] 3   3 Match 131\n",
      "\n",
      "[-0.71972656  0.21081543 -0.0236969   0.31835938  0.10705566  0.04321289] 3   2 \n",
      "[-0.62109375  0.2836914   0.05688477  0.29663086  0.18554688 -0.0006876 ] 3   2 \n",
      "[-0.6982422   0.22814941 -0.05361938  0.32177734  0.1451416   0.03088379] 3   0 \n",
      "[-0.7680664   0.21350098 -0.06982422  0.35839844  0.06005859  0.06866455] 3   3 Match 132\n",
      "\n",
      "[-0.75097656  0.16894531 -0.0317688   0.3022461   0.09112549  0.04763794] 3   2 \n",
      "[-0.6660156   0.20483398  0.00079107  0.3046875   0.12658691  0.0484314 ] 3   3 Match 133\n",
      "\n",
      "[-0.77734375  0.2722168  -0.06298828  0.31079102  0.15197754 -0.03421021] 3   0 \n",
      "[-0.7416992   0.1977539  -0.03384399  0.33813477  0.08508301  0.07818604] 3   1 \n",
      "[-0.68066406  0.23791504 -0.00812531  0.28198242  0.10784912  0.02590942] 3   4 \n",
      "[-0.74902344  0.3022461  -0.03860474  0.31176758  0.14111328  0.00992584] 3   3 Match 134\n",
      "\n",
      "[-0.5942383   0.27368164  0.05718994  0.3112793   0.20141602 -0.00492096] 3   1 \n",
      "[-0.7651367   0.17285156 -0.03726196  0.30737305  0.11169434  0.04754639] 3   2 \n",
      "[-0.71875     0.2915039  -0.09375     0.3334961   0.13879395  0.01488495] 3   5 \n",
      "[-0.71777344  0.18054199 -0.03909302  0.30151367  0.10565186  0.04483032] 3   2 \n",
      "[-0.59814453  0.32421875  0.013237    0.2788086   0.15759277  0.06872559] 1   4 \n",
      "[-0.70947266  0.2454834   0.01081085  0.30810547  0.13256836 -0.01687622] 3   3 Match 135\n",
      "\n",
      "[-0.6245117   0.29541016  0.056427    0.28686523  0.1850586   0.02519226] 1   3 \n",
      "[-0.69677734  0.2866211  -0.0065918   0.30981445  0.11694336 -0.00966644] 3   1 \n",
      "[-0.61083984  0.2902832   0.04315186  0.2866211   0.16125488  0.02268982] 1   3 \n",
      "[-0.7446289   0.17089844 -0.05007935  0.33081055  0.08135986  0.0715332 ] 3   3 Match 136\n",
      "\n",
      "[-0.7998047   0.2919922  -0.09381104  0.29248047  0.08538818 -0.00683594] 3   4 \n",
      "[-0.70996094  0.17883301  0.01887512  0.31713867  0.13342285  0.0249939 ] 3   1 \n",
      "[-0.6904297   0.16308594 -0.01096344  0.30419922  0.13024902  0.04501343] 3   5 \n",
      "[-0.7011719   0.21887207  0.01421356  0.2944336   0.09375     0.03613281] 3   3 Match 137\n",
      "\n",
      "[-0.7631836   0.25048828 -0.06292725  0.34375     0.08630371  0.03268433] 3   1 \n",
      "[-0.6948242   0.21936035 -0.0098114   0.3022461   0.11950684  0.03182983] 3   4 \n",
      "[-0.7241211   0.22277832 -0.04086304  0.31860352  0.07159424  0.04711914] 3   5 \n",
      "[-0.61621094  0.29956055  0.03552246  0.3334961   0.14489746 -0.003479  ] 3   5 \n",
      "[-0.7324219   0.1940918  -0.04397583  0.30981445  0.06933594  0.03726196] 3   3 Match 138\n",
      "\n",
      "[-0.74609375  0.18591309 -0.03631592  0.3581543   0.06488037  0.08093262] 3   3 Match 139\n",
      "\n",
      "[-0.6777344   0.24902344  0.00272179  0.27368164  0.14135742  0.01456451] 3   3 Match 140\n",
      "\n",
      "[-0.6894531   0.23291016 -0.04095459  0.31884766  0.06463623  0.07281494] 3   2 \n",
      "[-0.7558594   0.19995117 -0.04907227  0.36865234  0.0614624   0.0836792 ] 3   4 \n",
      "[-0.66064453  0.25463867  0.01113129  0.31176758  0.12548828  0.00670624] 3   5 \n",
      "[-0.72753906  0.21740723 -0.01531219  0.3215332   0.09320068  0.03265381] 3   1 \n",
      "[-0.6015625   0.29541016  0.01686096  0.2854004   0.1842041   0.01286316] 1   2 \n",
      "[-0.65625     0.2631836  -0.02398682  0.3083496   0.0927124   0.03659058] 3   4 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.68603516  0.3166504  -0.00830841  0.32983398  0.07495117  0.01232147] 3   3 Match 141\n",
      "\n",
      "[-0.7006836   0.1850586  -0.0196991   0.33911133  0.10296631  0.07366943] 3   5 \n",
      "[-0.7792969   0.20178223 -0.05514526  0.33081055  0.09356689  0.05377197] 3   0 \n",
      "[-0.6118164   0.2529297   0.01647949  0.2746582   0.13269043  0.05386353] 3   0 \n",
      "[-0.66503906  0.2861328  -0.0524292   0.28173828  0.14624023  0.02146912] 1   1 Match 142\n",
      "\n",
      "[-0.7265625   0.16247559 -0.05838013  0.30688477  0.09362793  0.02978516] 3   1 \n",
      "[-0.69970703  0.2121582  -0.03555298  0.32495117  0.09381104  0.05419922] 3   4 \n",
      "[-0.76416016  0.29370117 -0.04815674  0.32910156  0.09838867  0.00563049] 3   5 \n",
      "[-0.73291016  0.2084961  -0.04769897  0.32128906  0.10516357  0.05783081] 3   0 \n",
      "[-6.7333984e-01  2.0654297e-01  8.6724758e-05  2.9248047e-01\n",
      "  1.3464355e-01  4.8370361e-02] 3   4 \n",
      "[-0.7363281   0.21289062 -0.04675293  0.32763672  0.0635376   0.07110596] 3   2 \n",
      "[-0.50927734  0.3388672   0.04592896  0.26782227  0.23413086  0.01759338] 1   4 \n",
      "[-0.68310547  0.19995117 -0.03778076  0.32080078  0.1116333   0.06585693] 3   2 \n",
      "[-0.5883789   0.27294922  0.03778076  0.3005371   0.18908691  0.00592804] 3   2 \n",
      "[-0.60791016  0.31689453  0.04238892  0.32226562  0.1763916   0.06219482] 3   2 \n",
      "[-0.7001953   0.22241211 -0.00146294  0.30078125  0.14245605 -0.00152016] 3   0 \n",
      "[-0.609375    0.23388672 -0.01550293  0.2783203   0.15625     0.05819702] 3   2 \n",
      "[-0.73291016  0.22290039 -0.04428101  0.3515625   0.08270264  0.06872559] 3   2 \n",
      "[-0.7294922   0.20654297  0.00624466  0.29663086  0.11273193  0.02500916] 3   1 \n",
      "[-0.609375    0.30517578  0.05181885  0.30151367  0.20532227  0.08062744] 1   4 \n",
      "[-0.5839844   0.2824707   0.04995728  0.31152344  0.20324707  0.01194763] 3   2 \n",
      "[-0.72802734  0.23791504 -0.03848267  0.33740234  0.10992432  0.050354  ] 3   3 Match 143\n",
      "\n",
      "[-0.75        0.19421387 -0.03921509  0.30688477  0.09289551  0.03219604] 3   3 Match 144\n",
      "\n",
      "[-0.6196289   0.29785156  0.0418396   0.29956055  0.2019043   0.00209236] 3   4 \n",
      "[-6.9140625e-01  2.4194336e-01 -3.0398369e-04  2.9736328e-01\n",
      "  9.2712402e-02  1.4076233e-02] 3   2 \n",
      "[-0.703125    0.22717285 -0.00786591  0.30688477  0.13903809  0.07104492] 3   2 \n",
      "[-0.73876953  0.17407227 -0.0385437   0.34545898  0.07440186  0.08843994] 3   3 Match 145\n",
      "\n",
      "[-0.7011719   0.18066406 -0.03671265  0.3359375   0.0871582   0.07037354] 3   2 \n",
      "[-0.7163086   0.21777344 -0.08770752  0.3178711   0.08758545  0.02494812] 3   4 \n",
      "[-0.71777344  0.203125   -0.02793884  0.32543945  0.10736084  0.05117798] 3   3 Match 146\n",
      "\n",
      "[-0.7001953   0.17675781 -0.02043152  0.31347656  0.09240723  0.07061768] 3   1 \n",
      "[-0.74658203  0.24401855 -0.04522705  0.36108398  0.08978271  0.05377197] 3   5 \n",
      "[-0.7705078   0.21569824 -0.07507324  0.32739258  0.07946777  0.00519943] 3   3 Match 147\n",
      "\n",
      "[-0.72314453  0.20703125 -0.05099487  0.3581543   0.06811523  0.08325195] 3   4 \n",
      "[-0.6430664   0.28686523  0.01870728  0.30493164  0.17468262  0.05813599] 3   5 \n",
      "[-0.7104492   0.21228027 -0.0214386   0.3100586   0.10247803  0.01754761] 3   2 \n",
      "[-0.77734375  0.24633789 -0.07977295  0.30786133  0.07098389  0.00560379] 3   3 Match 148\n",
      "\n",
      "[-0.7026367   0.2244873  -0.05981445  0.32080078  0.1307373   0.00704575] 3   5 \n",
      "[-0.6508789   0.2626953  -0.03314209  0.31103516  0.12225342  0.06939697] 3   2 \n",
      "[-0.7416992   0.16003418 -0.04815674  0.34765625  0.07354736  0.06896973] 3   5 \n",
      "[-0.76708984  0.23352051 -0.01873779  0.31933594  0.09350586  0.0223999 ] 3   4 \n",
      "[-0.7973633   0.24987793 -0.0848999   0.3425293   0.07891846  0.02981567] 3   2 \n",
      "[-0.73291016  0.16772461 -0.06182861  0.36083984  0.06439209  0.06420898] 3   3 Match 149\n",
      "\n",
      "[-0.6777344   0.23120117  0.0269928   0.31982422  0.13696289  0.01980591] 3   0 \n",
      "[-0.7470703   0.20959473 -0.0211792   0.3088379   0.08319092  0.06451416] 3   2 \n",
      "[-0.74853516  0.22802734 -0.03787231  0.32104492  0.07348633  0.03286743] 3   5 \n",
      "[-0.73828125  0.25195312 -0.03009033  0.3059082   0.07281494  0.05264282] 3   5 \n",
      "[-0.72021484  0.19372559 -0.03933716  0.33764648  0.07000732  0.06204224] 3   3 Match 150\n",
      "\n",
      "[-0.5756836   0.3408203   0.06298828  0.31689453  0.21655273 -0.01042938] 1   2 \n",
      "[-0.7060547   0.2590332  -0.01465607  0.3178711   0.13793945  0.04266357] 3   4 \n",
      "[-0.7441406   0.1743164  -0.04135132  0.33374023  0.06768799  0.0597229 ] 3   1 \n",
      "[-0.73876953  0.3232422  -0.12005615  0.33032227  0.05541992  0.02438354] 3   4 \n",
      "[-0.7260742   0.20141602 -0.01468658  0.31713867  0.06567383  0.07037354] 3   3 Match 151\n",
      "\n",
      "[-0.5644531   0.29370117  0.05133057  0.2878418   0.2130127   0.01702881] 1   3 \n",
      "[-0.7651367   0.2553711  -0.04110718  0.33129883  0.0927124  -0.00763702] 3   1 \n",
      "[-0.70166016  0.2064209  -0.04190063  0.3630371   0.07525635  0.10571289] 3   4 \n",
      "[-0.7548828   0.2097168   0.00191975  0.3203125   0.08374023  0.07086182] 3   5 \n",
      "[-0.73535156  0.19604492 -0.08197021  0.33007812  0.07287598  0.08575439] 3   2 \n",
      "[-0.69189453  0.21826172 -0.03286743  0.30859375  0.11621094  0.00804901] 3   0 \n",
      "[-0.69091797  0.20605469 -0.03399658  0.30200195  0.13513184  0.0609436 ] 3   1 \n",
      "[-0.66308594  0.30859375  0.04937744  0.30273438  0.17956543  0.00560379] 1   3 \n",
      "[-0.7519531   0.20532227 -0.0531311   0.3461914   0.06750488  0.0218811 ] 3   3 Match 152\n",
      "\n",
      "[-0.7338867   0.2199707  -0.05499268  0.3466797   0.08233643  0.0657959 ] 3   2 \n",
      "[-0.6323242   0.30395508  0.04318237  0.28173828  0.19055176  0.00844574] 1   5 \n",
      "[-0.77734375  0.1920166  -0.040802    0.32421875  0.07598877  0.07806396] 3   2 \n",
      "[-0.7373047   0.20031738 -0.04992676  0.3635254   0.09661865  0.04394531] 3   5 \n",
      "[-0.71728516  0.23242188 -0.04290771  0.30932617  0.09686279  0.05905151] 3   3 Match 153\n",
      "\n",
      "[-0.73876953  0.20996094 -0.09954834  0.32348633  0.10003662  0.04147339] 3   4 \n",
      "[-0.78271484  0.22155762 -0.07836914  0.33642578  0.05310059  0.03747559] 3   2 \n",
      "[-0.7011719   0.21875    -0.00666809  0.34936523  0.07537842  0.05715942] 3   4 \n",
      "[-0.60058594  0.34594727 -0.00634003  0.26782227  0.16674805 -0.02433777] 1   2 \n",
      "[-0.7080078   0.1730957  -0.05227661  0.33374023  0.07226562  0.08123779] 3   4 \n",
      "[-0.75634766  0.19628906 -0.05001831  0.33496094  0.06280518  0.05810547] 3   4 \n",
      "[-0.76708984  0.19458008 -0.00507355  0.35742188  0.1105957   0.06658936] 3   0 \n",
      "[-0.76660156  0.22424316 -0.01454163  0.32202148  0.08636475  0.05105591] 3   2 \n",
      "[-0.7128906   0.23425293 -0.0266571   0.29760742  0.08502197 -0.02467346] 3   5 \n",
      "[-0.7182617   0.22888184 -0.01625061  0.3400879   0.07019043  0.0647583 ] 3   5 \n",
      "[-0.7548828   0.26538086 -0.09735107  0.35302734  0.0869751  -0.04382324] 3   2 \n",
      "[-0.7631836   0.18786621 -0.04351807  0.3371582   0.10229492  0.07550049] 3   3 Match 154\n",
      "\n",
      "[-0.7348633   0.2298584  -0.0581665   0.33447266  0.05548096  0.04019165] 3   1 \n",
      "[-0.70214844  0.17712402 -0.02279663  0.33544922  0.11627197  0.0758667 ] 3   3 Match 155\n",
      "\n",
      "[-0.7314453   0.21740723 -0.01371765  0.3112793   0.10925293  0.06738281] 3   3 Match 156\n",
      "\n",
      "[-0.75390625  0.28808594 -0.11816406  0.36914062  0.08673096 -0.00788116] 3   0 \n",
      "[-0.71972656  0.21594238 -0.0091629   0.3178711   0.07269287  0.05142212] 3   2 \n",
      "[-0.7675781   0.16894531 -0.03042603  0.3215332   0.09851074  0.05703735] 3   4 \n",
      "[-0.7553711   0.27685547  0.0035305   0.33398438  0.06750488  0.00437546] 3   4 \n",
      "[-0.72753906  0.22888184 -0.04196167  0.34399414  0.07556152  0.02532959] 3   4 \n",
      "[-0.73828125  0.26489258 -0.074646    0.3022461   0.09851074  0.02587891] 3   4 \n",
      "[-0.7104492   0.22106934 -0.01637268  0.3227539   0.13232422  0.0322876 ] 3   3 Match 157\n",
      "\n",
      "[-0.7421875   0.21118164 -0.02680969  0.33813477  0.09747314  0.00475311] 3   0 \n",
      "[-0.7758789   0.21801758 -0.097229    0.3334961   0.0401001   0.02708435] 3   5 \n",
      "[-0.7861328   0.2746582  -0.06329346  0.3010254   0.09399414  0.01387787] 3   2 \n",
      "[-0.7885742   0.3395996  -0.04638672  0.30786133  0.13549805  0.03808594] 1   2 \n",
      "[-0.7495117   0.2088623  -0.01985168  0.34570312  0.11657715  0.03237915] 3   4 \n",
      "[-0.74121094  0.19689941 -0.03149414  0.3232422   0.11132812  0.02593994] 3   1 \n",
      "[-0.7685547   0.2064209  -0.03189087  0.31518555  0.07727051  0.03884888] 3   4 \n",
      "[-0.7128906   0.25634766 -0.08349609  0.2980957   0.07061768 -0.00592041] 3   3 Match 158\n",
      "\n",
      "[-0.6772461   0.19702148 -0.0369873   0.34301758  0.08947754  0.07092285] 3   4 \n",
      "[-0.67626953  0.26782227 -0.03118896  0.29589844  0.13330078  0.04452515] 3   5 \n",
      "[-0.74316406  0.20678711 -0.04260254  0.3317871   0.08184814  0.06280518] 3   2 \n",
      "[-0.6621094   0.2232666   0.00078678  0.2854004   0.13623047  0.05480957] 3   5 \n",
      "[-0.6958008   0.27246094 -0.07275391  0.33374023  0.10003662  0.0748291 ] 3   3 Match 159\n",
      "\n",
      "[-0.7363281   0.26367188 -0.03707886  0.32861328  0.12133789  0.0451355 ] 3   3 Match 160\n",
      "\n",
      "[-0.7441406   0.18395996 -0.04663086  0.31103516  0.08703613  0.07598877] 3   1 \n",
      "[-0.7241211   0.21728516 -0.01959229  0.34057617  0.10845947  0.06445312] 3   4 \n",
      "[-0.7480469   0.2109375  -0.00333405  0.31347656  0.11480713  0.06854248] 3   2 \n",
      "[-0.7729492   0.21264648 -0.06396484  0.35253906  0.03424072  0.03323364] 3   1 \n",
      "[-0.7055664   0.21044922 -0.04837036  0.32910156  0.08258057  0.03829956] 3   4 \n",
      "[-0.7758789   0.21350098 -0.09753418  0.34765625  0.03912354  0.03546143] 3   4 \n",
      "[-0.7163086   0.2088623  -0.027771    0.34155273  0.08563232  0.06088257] 3   3 Match 161\n",
      "\n",
      "[-0.70703125  0.23632812 -0.06317139  0.35668945  0.09814453  0.05908203] 3   3 Match 162\n",
      "\n",
      "[-0.6694336   0.2163086  -0.00896454  0.30151367  0.08728027  0.03022766] 3   2 \n",
      "[-0.62353516  0.31152344  0.01088715  0.34350586  0.1459961   0.01771545] 3   0 \n",
      "[-0.72753906  0.21643066 -0.02903748  0.29492188  0.09484863  0.05221558] 3   1 \n",
      "[-0.73828125  0.20654297 -0.05715942  0.34399414  0.06616211  0.05633545] 3   4 \n",
      "[-0.6791992   0.18115234 -0.01500702  0.29760742  0.09936523  0.08813477] 3   4 \n",
      "[-0.765625    0.24768066 -0.06903076  0.36108398  0.05612183  0.01846313] 3   4 \n",
      "[-0.7324219   0.20690918 -0.05780029  0.3400879   0.0892334   0.04611206] 3   5 \n",
      "[-0.5756836   0.32104492  0.0871582   0.3244629   0.20153809 -0.02349854] 3   3 Match 163\n",
      "\n",
      "[-0.72558594  0.21716309 -0.0029335   0.359375    0.08190918  0.0496521 ] 3   2 \n",
      "[-0.88134766  0.32788086 -0.15686035  0.38793945  0.06195068 -0.01308441] 3   5 \n",
      "[-0.7739258   0.2927246  -0.11419678  0.37670898  0.04663086 -0.01032257] 3   0 \n",
      "[-0.5493164   0.30004883  0.04644775  0.3125      0.24145508  0.03030396] 3   3 Match 164\n",
      "\n",
      "[-0.71191406  0.27954102 -0.05685425  0.31420898  0.16064453  0.0098877 ] 3   0 \n",
      "[-0.7290039   0.20080566 -0.03530884  0.3461914   0.08843994  0.08209229] 3   4 \n",
      "[-0.6279297   0.31225586  0.0670166   0.31030273  0.18774414  0.03662109] 1   4 \n",
      "[-0.7270508   0.20202637  0.00759125  0.31933594  0.11419678  0.05743408] 3   4 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.77197266  0.22485352 -0.03982544  0.34423828  0.11114502  0.06262207] 3   3 Match 165\n",
      "\n",
      "[-0.71728516  0.19311523 -0.03353882  0.3232422   0.08319092  0.03884888] 3   2 \n",
      "[-0.73291016  0.21850586 -0.04812622  0.31640625  0.1204834   0.01899719] 3   5 \n",
      "[-0.7788086   0.18945312 -0.06082153  0.32739258  0.0880127   0.02964783] 3   5 \n",
      "[-0.72753906  0.24829102 -0.07855225  0.32177734  0.10101318 -0.00466156] 3   1 \n",
      "[-0.76416016  0.25219727 -0.06756592  0.30639648  0.08410645  0.00565338] 3   1 \n",
      "[-0.75146484  0.27319336 -0.05010986  0.30395508  0.10137939  0.04742432] 3   3 Match 166\n",
      "\n",
      "[-0.68310547  0.23510742 -0.01167297  0.35375977  0.07397461  0.05114746] 3   2 \n",
      "[-0.7089844   0.20605469 -0.05126953  0.32373047  0.09576416  0.07867432] 3   5 \n",
      "[-0.6723633   0.23815918  0.01197815  0.33007812  0.08953857  0.04541016] 3   2 \n",
      "[-0.7451172   0.22570801 -0.00251198  0.31567383  0.1373291   0.0567627 ] 3   1 \n",
      "[-0.7651367   0.17956543 -0.11010742  0.3852539   0.07061768  0.09356689] 3   3 Match 167\n",
      "\n",
      "[-0.5961914   0.28173828  0.06866455  0.3137207   0.20080566  0.01332855] 3   3 Match 168\n",
      "\n",
      "[-0.7363281   0.19445801 -0.04986572  0.32714844  0.07330322  0.05392456] 3   1 \n",
      "[-0.7480469   0.2541504  -0.007061    0.32177734  0.08892822  0.03137207] 3   3 Match 169\n",
      "\n",
      "[-0.69677734  0.22912598  0.00738525  0.31958008  0.12854004  0.09118652] 3   4 \n",
      "[-0.7036133   0.18530273 -0.05432129  0.2919922   0.08483887  0.0489502 ] 3   5 \n",
      "[-0.58691406  0.30297852  0.05569458  0.29541016  0.1743164   0.04168701] 1   4 \n",
      "[-0.7246094   0.19995117 -0.03686523  0.36132812  0.10076904  0.05688477] 3   1 \n",
      "[-0.70458984  0.25195312 -0.00335693  0.33862305  0.09106445  0.03704834] 3   3 Match 170\n",
      "\n",
      "[-0.7451172   0.2130127  -0.03111267  0.3310547   0.05471802  0.02934265] 3   4 \n",
      "[-0.6923828   0.24072266  0.01262665  0.31176758  0.12768555  0.050354  ] 3   0 \n",
      "[-0.6699219   0.23303223 -0.03445435  0.32861328  0.11260986  0.04620361] 3   2 \n",
      "[-0.7885742   0.22814941 -0.07928467  0.30444336  0.05697632  0.03509521] 3   1 \n",
      "[-0.72216797  0.25146484 -0.00931549  0.28564453  0.11810303  0.00392151] 3   5 \n",
      "[-0.7841797   0.2290039  -0.08215332  0.32836914  0.08734131 -0.00826263] 3   2 \n",
      "[-0.7446289   0.18664551 -0.02337646  0.3540039   0.08721924  0.04058838] 3   4 \n",
      "[-0.625       0.2836914   0.03619385  0.29736328  0.15783691  0.05929565] 3   3 Match 171\n",
      "\n",
      "[-0.7392578   0.21362305 -0.00095797  0.31445312  0.09674072  0.04388428] 3   4 \n",
      "[-0.7182617   0.20812988 -0.00273132  0.32299805  0.11364746  0.05300903] 3   5 \n",
      "[-0.7026367   0.26123047 -0.03057861  0.3322754   0.10888672 -0.0072403 ] 3   1 \n",
      "[-0.71875     0.23913574 -0.04632568  0.32910156  0.07049561  0.0637207 ] 3   0 \n",
      "[-0.74072266  0.1986084  -0.06896973  0.3088379   0.1472168   0.04641724] 3   2 \n",
      "[-0.57177734  0.29760742  0.06951904  0.30371094  0.2211914   0.0071106 ] 3   2 \n",
      "[-0.671875    0.24768066 -0.03108215  0.31054688  0.08435059  0.04156494] 3   1 \n",
      "[-0.72509766  0.17614746 -0.03405762  0.3383789   0.0569458   0.03445435] 3   1 \n",
      "[-0.7114258   0.20397949 -0.02282715  0.3293457   0.07537842  0.0690918 ] 3   4 \n",
      "[-0.69091797  0.21655273 -0.01060486  0.29077148  0.09161377  0.04959106] 3   2 \n",
      "[-0.87890625  0.26879883 -0.22131348  0.35009766  0.15808105 -0.10638428] 3   1 \n",
      "[-0.7895508   0.25952148 -0.08026123  0.35058594  0.05773926 -0.0042038 ] 3   5 \n",
      "[-0.77197266  0.24035645 -0.0480957   0.31079102  0.08428955  0.02746582] 3   4 \n",
      "[-0.64501953  0.25390625  0.03363037  0.2836914   0.17773438  0.00735855] 3   5 \n",
      "[-0.6713867   0.2211914  -0.00160217  0.28955078  0.13989258  0.0567627 ] 3   3 Match 172\n",
      "\n",
      "[-0.71728516  0.23852539 -0.03213501  0.31420898  0.11346436  0.03466797] 3   1 \n",
      "[-0.7919922   0.23388672 -0.07879639  0.33081055  0.09735107  0.03448486] 3   2 \n",
      "[-0.71484375  0.25024414 -0.02180481  0.32128906  0.12261963  0.01413727] 3   2 \n",
      "[-0.76953125  0.20605469 -0.03295898  0.3371582   0.03601074  0.04653931] 3   3 Match 173\n",
      "\n",
      "[-0.82910156  0.23156738 -0.15527344  0.38354492  0.15356445 -0.03497314] 3   5 \n",
      "[-0.76123047  0.21813965 -0.05606079  0.34521484  0.0758667   0.04455566] 3   1 \n",
      "[-0.7265625   0.30908203 -0.00933838  0.30151367  0.09881592 -0.06243896] 1   3 \n",
      "[-0.69433594  0.2088623  -0.03610229  0.32348633  0.08746338  0.06292725] 3   3 Match 174\n",
      "\n",
      "[-0.7919922   0.17956543 -0.09686279  0.38427734  0.04428101  0.07592773] 3   5 \n",
      "[-0.68066406  0.30078125 -0.01768494  0.3083496   0.10656738  0.01558685] 3   2 \n",
      "[-0.71875     0.2421875   0.02993774  0.2993164   0.1430664   0.01678467] 3   1 \n",
      "[-0.7714844   0.2421875  -0.00531006  0.33740234  0.08959961  0.01261902] 3   3 Match 175\n",
      "\n",
      "[-0.7949219   0.2626953  -0.09710693  0.29174805  0.11523438 -0.01103973] 3   3 Match 176\n",
      "\n",
      "[-0.6513672   0.20092773  0.0118866   0.29248047  0.14819336  0.03869629] 3   5 \n",
      "[-0.7480469   0.28930664  0.00515366  0.3317871   0.10125732  0.02545166] 3   5 \n",
      "[-0.6616211   0.2211914  -0.03213501  0.26538086  0.07745361  0.04840088] 3   2 \n",
      "[-0.6582031   0.27416992 -0.02876282  0.32788086  0.15234375  0.02209473] 3   5 \n",
      "[-0.69140625  0.27368164  0.02403259  0.32348633  0.16003418  0.04827881] 3   5 \n",
      "[-0.7084961   0.22912598 -0.08862305  0.31274414  0.07873535  0.08496094] 3   3 Match 177\n",
      "\n",
      "[-0.5961914   0.29956055  0.04879761  0.31225586  0.14672852  0.05072021] 3   1 \n",
      "[-0.7807617   0.2097168  -0.02279663  0.3635254   0.06903076  0.0715332 ] 3   4 \n",
      "[-0.79541016  0.17468262 -0.03314209  0.36157227  0.08288574  0.07653809] 3   3 Match 178\n",
      "\n",
      "[-0.7480469   0.24450684 -0.06744385  0.31445312  0.10595703  0.0091095 ] 3   1 \n",
      "[-0.7475586   0.19787598 -0.08795166  0.36621094  0.06756592  0.05874634] 3   4 \n",
      "[-0.74902344  0.21899414 -0.07702637  0.33740234  0.09082031  0.06021118] 3   1 \n",
      "[-0.72998047  0.23303223 -0.06109619  0.34716797  0.0586853   0.03384399] 3   5 \n",
      "[-0.71240234  0.20788574  0.01204681  0.2927246   0.10369873  0.0602417 ] 3   3 Match 179\n",
      "\n",
      "[-0.73828125  0.17883301 -0.02578735  0.34423828  0.08538818  0.07727051] 3   4 \n",
      "[-0.7246094   0.2685547  -0.05361938  0.31396484  0.08288574  0.03741455] 3   3 Match 180\n",
      "\n",
      "[-0.69677734  0.20446777 -0.03857422  0.3400879   0.11114502  0.07452393] 3   1 \n",
      "[-0.7675781   0.18408203 -0.01971436  0.33984375  0.08746338  0.09777832] 3   3 Match 181\n",
      "\n",
      "[-0.57177734  0.3466797   0.05349731  0.2993164   0.17602539  0.06292725] 1   4 \n",
      "[-0.7597656   0.28466797 -0.0254364   0.33154297  0.09191895  0.01776123] 3   4 \n",
      "[-0.77001953  0.21728516 -0.05108643  0.33398438  0.08648682  0.03509521] 3   5 \n",
      "[-0.70458984  0.22070312 -0.05584717  0.3244629   0.06506348  0.0539856 ] 3   4 \n",
      "[-0.66748047  0.24743652 -0.0473938   0.32910156  0.09216309  0.04876709] 3   4 \n",
      "[-0.7294922   0.19128418 -0.06130981  0.32592773  0.07843018  0.0368042 ] 3   5 \n",
      "[-0.6899414   0.23937988  0.00427628  0.30615234  0.11383057  0.04315186] 3   5 \n",
      "[-0.62890625  0.29101562  0.04760742  0.29345703  0.1586914   0.00444794] 3   4 \n",
      "[-0.75683594  0.24682617 -0.08129883  0.3227539   0.07104492  0.00698853] 3   5 \n",
      "[-0.7026367   0.24853516 -0.01387024  0.34838867  0.08837891  0.05172729] 3   4 \n",
      "[-0.71533203  0.19641113 -0.02326965  0.32836914  0.06304932  0.05429077] 3   3 Match 182\n",
      "\n",
      "[-0.69628906  0.2211914  -0.02444458  0.3046875   0.09466553  0.04431152] 3   0 \n",
      "[-0.7416992   0.23291016 -0.05984497  0.2878418   0.11804199 -0.01966858] 3   2 \n",
      "[-0.7441406   0.24853516 -0.05731201  0.3076172   0.06341553  0.05987549] 3   4 \n",
      "[-0.7138672   0.20727539 -0.03640747  0.3569336   0.059021    0.05819702] 3   4 \n",
      "[-0.70947266  0.26342773  0.01611328  0.32763672  0.12133789  0.05056763] 3   2 \n",
      "[-0.7578125   0.20935059 -0.02659607  0.3125      0.06738281  0.074646  ] 3   3 Match 183\n",
      "\n",
      "[-0.7529297   0.22424316 -0.05697632  0.35351562  0.05438232  0.04797363] 3   5 \n",
      "[-0.71728516  0.19030762 -0.02314758  0.31396484  0.08270264  0.03689575] 3   4 \n",
      "[-0.74072266  0.21411133 -0.0163269   0.3605957   0.08654785  0.07214355] 3   4 \n",
      "[-0.68603516  0.20422363 -0.00492477  0.2932129   0.13708496  0.03424072] 3   0 \n",
      "[-0.7211914   0.22045898 -0.03744507  0.30541992  0.10113525  0.05847168] 3   4 \n",
      "[-0.74902344  0.20959473 -0.01713562  0.33569336  0.0814209   0.03729248] 3   5 \n",
      "[-0.68066406  0.27856445 -0.02963257  0.35375977  0.12792969  0.03033447] 3   1 \n",
      "[-0.70410156  0.20288086 -0.01321411  0.3408203   0.08575439  0.06311035] 3   2 \n",
      "[-0.77197266  0.17993164 -0.03735352  0.328125    0.06378174  0.05392456] 3   2 \n",
      "[-0.6645508   0.2626953   0.05526733  0.28735352  0.15026855  0.00257301] 3   0 \n",
      "[-0.74365234  0.20654297 -0.01939392  0.35766602  0.06427002  0.0703125 ] 3   4 \n",
      "[-0.70996094  0.21606445 -0.02868652  0.33740234  0.08337402  0.04647827] 3   3 Match 184\n",
      "\n",
      "[-0.6279297   0.23400879 -0.00113583  0.3125      0.13452148  0.07513428] 3   5 \n",
      "[-0.7290039   0.23547363 -0.01051331  0.3293457   0.08551025  0.02912903] 3   2 \n",
      "[-0.58691406  0.32226562  0.02752686  0.35107422  0.1862793   0.0802002 ] 3   1 \n",
      "[-0.7426758   0.23022461 -0.01628113  0.32250977  0.12176514  0.03930664] 3   2 \n",
      "[-0.73779297  0.19470215 -0.02047729  0.31860352  0.10675049  0.0690918 ] 3   4 \n",
      "[-0.7368164   0.18798828 -0.02984619  0.35791016  0.06549072  0.09039307] 3   5 \n",
      "[-0.6821289   0.20922852 -0.03164673  0.31103516  0.09863281  0.0574646 ] 3   5 \n",
      "[-0.72802734  0.19580078 -0.01487732  0.30688477  0.10443115  0.04821777] 3   1 \n",
      "[-0.7363281   0.19470215 -0.01921082  0.3112793   0.09637451  0.04934692] 3   3 Match 185\n",
      "\n",
      "[-0.7324219   0.19494629 -0.02601624  0.3046875   0.12176514  0.0451355 ] 3   1 \n",
      "[-0.7207031   0.24902344 -0.03787231  0.32666016  0.06835938  0.00989532] 3   2 \n",
      "[-0.6040039   0.27490234  0.02148438  0.3239746   0.19006348  0.01705933] 3   3 Match 186\n",
      "\n",
      "[-0.58935547  0.30078125  0.05667114  0.33911133  0.18859863  0.02345276] 3   1 \n",
      "[-0.67089844  0.20532227  0.02006531  0.28198242  0.13903809  0.0597229 ] 3   0 \n",
      "[-0.72021484  0.22790527 -0.01702881  0.3293457   0.10089111  0.04867554] 3   5 \n",
      "[-0.7524414   0.22924805 -0.00276947  0.33935547  0.08532715  0.05508423] 3   5 \n",
      "[-0.7055664   0.28125    -0.02581787  0.31958008  0.0949707   0.05084229] 3   1 \n",
      "[-0.74365234  0.23828125 -0.10864258  0.3137207   0.07751465  0.05154419] 3   1 \n",
      "[-0.7314453   0.22192383 -0.11791992  0.33422852  0.13085938  0.04171753] 3   0 \n",
      "[-0.67285156  0.23327637  0.01081085  0.33374023  0.12451172  0.03790283] 3   1 \n",
      "[-0.7128906   0.21960449 -0.04071045  0.34106445  0.10467529  0.05007935] 3   3 Match 187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-0.7241211   0.18066406 -0.04470825  0.3125      0.06512451  0.06115723] 3   0 \n",
      "[-0.73583984  0.2211914  -0.00443649  0.32128906  0.09625244  0.01519012] 3   4 \n",
      "[-0.69970703  0.28149414 -0.02464294  0.32910156  0.07366943  0.04260254] 3   4 \n",
      "[-0.73779297  0.2084961  -0.07055664  0.3215332   0.0836792   0.04406738] 3   5 \n",
      "[-0.75878906  0.18688965 -0.0191803   0.3166504   0.0748291   0.03549194] 3   3 Match 188\n",
      "\n",
      "[-0.6767578   0.27612305 -0.01283264  0.29174805  0.13012695  0.0304718 ] 3   2 \n",
      "[-0.7519531   0.2097168  -0.0164032   0.30639648  0.10040283 -0.01178741] 3   0 \n",
      "[-0.66064453  0.26831055 -0.01013947  0.28051758  0.10498047  0.03421021] 3   4 \n",
      "[-0.7866211   0.26635742 -0.08520508  0.32495117  0.07415771  0.06451416] 3   2 \n",
      "[-0.6503906   0.25732422 -0.02902222  0.31567383  0.16137695  0.02125549] 3   2 \n",
      "[-0.73779297  0.24035645 -0.07122803  0.3256836   0.06542969  0.0345459 ] 3   2 \n",
      "[-0.7265625   0.25805664 -0.07470703  0.3310547   0.09545898  0.05111694] 3   4 \n",
      "[-6.8554688e-01  1.8835449e-01  1.4400482e-04  3.1884766e-01\n",
      "  1.2878418e-01  4.6691895e-02] 3   5 \n",
      "[-0.75341797  0.19226074 -0.01062012  0.30371094  0.0871582   0.00350189] 3   4 \n",
      "[-0.7026367   0.24914551 -0.0513916   0.3251953   0.13598633  0.0259552 ] 3   2 \n",
      "[-0.6801758   0.2434082  -0.01768494  0.3076172   0.11224365  0.02711487] 3   4 \n",
      "[-0.70996094  0.24560547  0.01104736  0.33642578  0.08807373  0.01137543] 3   4 \n",
      "[-0.70166016  0.23132324 -0.00655365  0.36914062  0.06799316  0.05221558] 3   2 \n",
      "[-0.7192383   0.2166748  -0.02534485  0.30273438  0.0970459   0.04553223] 3   3 Match 189\n",
      "\n",
      "[-0.71875     0.22875977 -0.00545502  0.31762695  0.10693359  0.02452087] 3   2 \n",
      "[-0.6899414   0.23242188  0.02734375  0.3203125   0.13330078  0.04730225] 3   4 \n",
      "[-0.6621094   0.25195312  0.00602722  0.31640625  0.18884277 -0.00434494] 3   3 Match 190\n",
      "\n",
      "[-0.7324219   0.2902832  -0.04421997  0.35058594  0.08551025  0.05749512] 3   2 \n",
      "[-0.68896484  0.2541504   0.03405762  0.30493164  0.14233398  0.03527832] 3   1 \n",
      "[-0.71191406  0.23620605 -0.03022766  0.34985352  0.0814209   0.05117798] 3   4 \n",
      "[-0.7285156   0.19812012 -0.05441284  0.3149414   0.06256104  0.04849243] 3   0 \n",
      "[-0.71533203  0.2692871  -0.02095032  0.32836914  0.10491943  0.04705811] 3   2 \n",
      "[-0.68896484  0.22351074 -0.01791382  0.31835938  0.09606934  0.02310181] 3   3 Match 191\n",
      "\n",
      "[-0.67333984  0.27441406 -0.00379372  0.30664062  0.13061523  0.03016663] 3   0 \n",
      "[-7.0654297e-01  2.0532227e-01  2.3834229e-02  2.9345703e-01\n",
      "  1.5625000e-01 -4.6873093e-04] 3   2 \n",
      "[-0.67041016  0.23425293 -0.02149963  0.31982422  0.12780762  0.06494141] 3   1 \n",
      "[-0.7114258   0.25268555 -0.02455139  0.3605957   0.07958984  0.07012939] 3   4 \n",
      "[-0.734375    0.21203613 -0.03164673  0.36206055  0.09594727  0.05331421] 3   1 \n",
      "[-0.58447266  0.2442627   0.03503418  0.27539062  0.17382812  0.07348633] 3   3 Match 192\n",
      "\n",
      "[-0.7236328   0.22460938  0.00766754  0.3232422   0.11602783  0.0574646 ] 3   0 \n",
      "[-0.7026367   0.23144531 -0.01792908  0.2939453   0.10717773  0.04364014] 3   1 \n",
      "[-0.6616211   0.25976562 -0.00684357  0.2824707   0.11553955  0.01774597] 3   3 Match 193\n",
      "\n",
      "[-0.7392578   0.23339844 -0.046875    0.34985352  0.06964111  0.06204224] 3   5 \n",
      "[-0.74072266  0.22143555 -0.02601624  0.31982422  0.05947876  0.03863525] 3   4 \n",
      "[-0.7631836   0.22766113 -0.02131653  0.34765625  0.08685303  0.03329468] 3   4 \n",
      "[-0.7324219   0.18640137 -0.01366425  0.30664062  0.07104492  0.04690552] 3   0 \n",
      "[-0.75390625  0.2467041  -0.00869751  0.32202148  0.10223389  0.05703735] 3   0 \n",
      "[-0.6435547   0.21484375 -0.00238037  0.26611328  0.1373291   0.04800415] 3   2 \n",
      "[-0.7553711   0.24060059 -0.04522705  0.32617188  0.08062744  0.04846191] 3   0 \n",
      "[-5.9423828e-01  3.7841797e-01 -4.5466423e-04  2.9858398e-01\n",
      "  2.3901367e-01  1.3595581e-02] 1   2 \n",
      "[-0.77001953  0.22814941 -0.01654053  0.3251953   0.08203125  0.07049561] 3   1 \n",
      "[-0.78466797  0.19128418 -0.03872681  0.35913086  0.05300903  0.06100464] 3   3 Match 194\n",
      "\n",
      "[-0.67529297  0.22460938 -0.01159668  0.28222656  0.11486816  0.03939819] 3   3 Match 195\n",
      "\n",
      "[-0.6953125   0.2154541  -0.03335571  0.31933594  0.0880127   0.05465698] 3   5 \n",
      "[-0.7138672   0.1854248  -0.0368042   0.31762695  0.10211182  0.07452393] 3   3 Match 196\n",
      "\n",
      "[-0.7548828   0.19311523 -0.02050781  0.32617188  0.05502319  0.02442932] 3   1 \n",
      "[-0.70166016  0.20800781  0.01863098  0.27246094  0.12585449  0.03613281] 3   0 \n",
      "[-0.74365234  0.22033691 -0.09899902  0.37573242  0.01220703  0.04650879] 3   2 \n",
      "[-0.60546875  0.33569336 -0.08172607  0.29833984  0.13427734  0.06140137] 1   2 \n",
      "[-0.6381836   0.27441406  0.05029297  0.31079102  0.13378906  0.03601074] 3   1 \n",
      "[-0.70703125  0.21350098 -0.01551056  0.3154297   0.10339355  0.0484314 ] 3   5 \n",
      "[-0.7504883   0.21472168 -0.07293701  0.35302734  0.07952881  0.06719971] 3   2 \n",
      "[-0.71191406  0.21582031 -0.00601959  0.31274414  0.13208008  0.06033325] 3   1 \n",
      "[-0.7524414   0.21313477 -0.00620651  0.34643555  0.10253906  0.05444336] 3   4 \n",
      "[-0.73535156  0.26098633 -0.04220581  0.3605957   0.07904053  0.04608154] 3   1 \n",
      "[-0.6899414   0.2722168  -0.01480865  0.29125977  0.10577393 -0.00479507] 3   1 \n",
      "[-0.7001953   0.17980957 -0.02914429  0.3076172   0.09783936  0.06027222] 3   4 \n",
      "[-0.6899414   0.2121582  -0.00130558  0.27148438  0.13964844  0.03726196] 3   3 Match 197\n",
      "\n",
      "[-0.7216797   0.24230957 -0.0703125   0.30126953  0.12683105  0.00963593] 3   4 \n",
      "[-0.70458984  0.20129395  0.00275993  0.31567383  0.13024902  0.04299927] 3   4 \n",
      "[-0.6816406   0.26245117 -0.00873566  0.2770996   0.10498047  0.06225586] 3   2 \n",
      "[-0.6933594   0.21679688 -0.06280518  0.35009766  0.08880615  0.05480957] 3   4 \n",
      "[-0.70703125  0.18115234 -0.02189636  0.31298828  0.10070801  0.06652832] 3   4 \n",
      "[-0.7133789   0.21081543 -0.00996399  0.31176758  0.11053467  0.059021  ] 3   4 \n",
      "[-0.73535156  0.2536621  -0.0451355   0.30932617  0.10601807  0.02224731] 3   5 \n",
      "[-0.63427734  0.35620117  0.01216888  0.27954102  0.1451416   0.02064514] 1   5 \n",
      "[-0.80566406  0.19592285 -0.0161438   0.36816406  0.08294678  0.07763672] 3   0 \n",
      "[-0.7207031   0.22106934 -0.05899048  0.34448242  0.07092285  0.06396484] 3   5 \n",
      "[-0.73779297  0.22937012 -0.01342773  0.37109375  0.06085205  0.04476929] 3   3 Match 198\n",
      "\n",
      "[-0.73339844  0.19152832  0.01211548  0.33251953  0.13684082  0.03933716] 3   4 \n",
      "[-0.72753906  0.21533203 -0.06118774  0.3215332   0.07458496  0.03839111] 3   3 Match 199\n",
      "\n",
      "[-0.69433594  0.25610352 -0.02206421  0.28198242  0.13757324 -0.0093689 ] 3   3 Match 200\n",
      "\n",
      "[-0.72216797  0.22070312 -0.0035305   0.30786133  0.09069824  0.0395813 ] 3   4 \n",
      "[-0.7207031   0.19677734 -0.02377319  0.31152344  0.08825684  0.00385857] 3   3 Match 201\n",
      "\n",
      "[-0.6948242   0.19995117 -0.07592773  0.32836914  0.15979004  0.03634644] 3   1 \n",
      "[-0.5786133   0.25976562  0.00901031  0.28588867  0.16357422  0.09320068] 3   1 \n",
      "[-0.7294922   0.26367188 -0.05761719  0.3408203   0.05825806  0.06210327] 3   1 \n",
      "[-0.71728516  0.22167969 -0.00522232  0.3359375   0.0824585   0.06143188] 3   5 \n",
      "[-0.7114258   0.1652832  -0.0501709   0.30395508  0.06970215  0.05474854] 3   0 \n",
      "[-0.5727539   0.28833008  0.07354736  0.2944336   0.1875      0.01138306] 3   1 \n",
      "[-0.67822266  0.21557617  0.00431442  0.29882812  0.11102295  0.04269409] 3   3 Match 202\n",
      "\n",
      "[-0.7133789   0.19934082 -0.02679443  0.32470703  0.10009766  0.04199219] 3   1 \n",
      "[-0.7836914   0.1730957  -0.03442383  0.35058594  0.08465576  0.06854248] 3   1 \n",
      "[-0.7246094   0.2232666  -0.05422974  0.31054688  0.05685425  0.03445435] 3   5 \n",
      "[-0.74560547  0.265625   -0.02531433  0.3203125   0.10083008  0.03372192] 3   4 \n",
      "[-0.69433594  0.21105957 -0.02111816  0.31567383  0.13891602  0.05490112] 3   2 \n",
      "[-0.75146484  0.23950195 -0.0597229   0.34765625  0.08306885  0.0604248 ] 3   3 Match 203\n",
      "\n",
      "[-0.7324219   0.23876953 -0.06842041  0.33764648  0.09460449  0.05072021] 3   1 \n",
      "[-0.6816406   0.2133789  -0.01844788  0.2919922   0.1361084   0.07843018] 3   2 \n",
      "[-0.5810547   0.28833008  0.05102539  0.28710938  0.18347168  0.04055786] 1   1 Match 204\n",
      "\n",
      "[-0.63964844  0.25585938  0.00691605  0.27807617  0.12878418  0.07531738] 3   2 \n",
      "[-0.7167969   0.23925781 -0.03004456  0.32104492  0.1182251   0.050354  ] 3   4 \n",
      "[-0.77490234  0.21728516 -0.03860474  0.3330078   0.05871582  0.03649902] 3   0 \n",
      "[-0.60253906  0.33276367  0.06439209  0.28808594  0.1895752   0.00875092] 1   4 \n",
      "[-0.7192383   0.20446777 -0.02690125  0.2919922   0.1003418   0.05252075] 3   4 \n",
      "[-0.7260742   0.24499512 -0.02348328  0.29467773  0.18200684  0.02639771] 3   4 \n",
      "[-0.72314453  0.2512207  -0.00704575  0.34326172  0.1081543   0.0178833 ] 3   4 \n",
      "[-0.7104492   0.27197266 -0.04351807  0.37573242  0.05050659  0.02220154] 3   5 \n",
      "[-0.7675781   0.2800293  -0.05264282  0.33081055  0.1381836   0.04656982] 3   5 \n",
      "[-0.69677734  0.20605469 -0.0259552   0.3088379   0.09649658  0.0619812 ] 3   2 \n",
      "[-0.7495117   0.18737793 -0.04464722  0.3774414   0.05273438  0.08990479] 3   1 \n",
      "[-0.76953125  0.18359375 -0.07562256  0.38500977  0.03082275  0.06634521] 3   4 \n",
      "[-0.6567383   0.2541504  -0.02507019  0.3215332   0.08319092  0.04690552] 3   4 \n",
      "[-0.7265625   0.23962402 -0.02366638  0.3503418   0.08477783  0.0736084 ] 3   3 Match 205\n",
      "\n",
      "[-0.6459961   0.26953125  0.04821777  0.31860352  0.19604492  0.02276611] 3   3 Match 206\n",
      "\n",
      "[-0.77197266  0.21789551 -0.05029297  0.31469727  0.11938477  0.01278687] 3   3 Match 207\n",
      "\n",
      "[-0.6953125   0.24707031  0.01164246  0.29711914  0.11730957  0.06323242] 3   2 \n",
      "[-0.7573242   0.28125    -0.08203125  0.30322266  0.1083374   0.04202271] 3   3 Match 208\n",
      "\n",
      "[-0.7285156   0.21240234 -0.02542114  0.32470703  0.07519531  0.03808594] 3   3 Match 209\n",
      "\n",
      "[-0.8330078   0.23425293 -0.18652344  0.32910156  0.09375    -0.05053711] 3   4 \n",
      "[-0.60546875  0.31152344  0.01216888  0.3017578   0.20996094  0.01763916] 1   1 Match 210\n",
      "\n",
      "[-0.7163086   0.18249512 -0.05291748  0.32470703  0.08880615  0.07025146] 3   4 \n",
      "[-0.74609375  0.21411133 -0.01477814  0.3359375   0.06390381  0.04473877] 3   3 Match 211\n",
      "\n",
      "[-0.7480469   0.23168945 -0.06164551  0.3383789   0.09680176  0.03805542] 3   5 \n",
      "[-0.73046875  0.12060547 -0.04483032  0.36987305  0.07073975  0.12078857] 3   1 \n",
      "[-0.6948242   0.25610352 -0.00144672  0.30541992  0.11938477  0.0302887 ] 3   1 \n",
      "[-0.703125    0.20593262 -0.01928711  0.32128906  0.10638428  0.07659912] 3   5 \n",
      "[-0.7207031   0.16784668 -0.04205322  0.3293457   0.1060791   0.10516357] 3   3 Match 212\n",
      "\n",
      "[-0.74365234  0.23962402 -0.01093292  0.3395996   0.07958984  0.04144287] 3   1 \n",
      "[-6.93847656e-01  2.36083984e-01 -6.46972656e-02  3.18603516e-01\n",
      "  1.01135254e-01 -5.60760498e-04] 3   2 \n",
      "[-0.78027344  0.2980957  -0.06610107  0.3581543   0.09118652  0.03015137] 3   4 \n",
      "[-0.66552734  0.27270508 -0.01470184  0.27661133  0.1161499   0.03955078] 3   1 \n",
      "[-0.74316406  0.21154785 -0.02536011  0.34716797  0.06292725  0.05514526] 3   1 \n",
      "[-0.6977539   0.17224121 -0.02581787  0.30932617  0.11761475  0.06317139] 3   3 Match 213\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6743164   0.24560547 -0.0380249   0.33911133  0.10021973  0.03970337] 3   0 \n",
      "[-0.75341797  0.22924805 -0.03111267  0.3239746   0.07208252  0.06634521] 3   1 \n",
      "[-0.73095703  0.21826172 -0.05102539  0.35498047  0.09069824  0.07434082] 3   5 \n",
      "[-0.64404297  0.3244629   0.01525879  0.25073242  0.1685791   0.0383606 ] 1   2 \n",
      "[-0.7753906   0.19018555 -0.04928589  0.39111328  0.07336426  0.09204102] 3   3 Match 214\n",
      "\n",
      "[-0.56689453  0.3540039   0.04141235  0.28710938  0.19421387 -0.00075054] 1   4 \n",
      "[-0.7373047   0.18457031 -0.05386353  0.3317871   0.04321289  0.05407715] 3   2 \n",
      "[-0.77197266  0.18896484 -0.10827637  0.33422852  0.02577209  0.00584412] 3   1 \n",
      "[-0.7290039   0.23352051 -0.04464722  0.3347168   0.08129883  0.0340271 ] 3   3 Match 215\n",
      "\n",
      "[-0.6660156   0.27954102  0.0013237   0.26831055  0.14672852  0.02420044] 1   4 \n",
      "[-0.77001953  0.1862793  -0.01501465  0.31689453  0.09265137  0.06192017] 3   1 \n",
      "[-0.68310547  0.2607422  -0.01012421  0.29833984  0.08587646 -0.0076828 ] 3   3 Match 216\n",
      "\n",
      "[-0.74316406  0.2220459  -0.08642578  0.30517578  0.0552063   0.03311157] 3   5 \n",
      "[-0.70654297  0.27905273 -0.03305054  0.29663086  0.09692383  0.01148224] 3   1 \n",
      "[-0.7529297   0.22570801 -0.02920532  0.3232422   0.0869751   0.06234741] 3   1 \n",
      "[-0.54785156  0.29858398  0.08068848  0.29663086  0.19812012  0.04489136] 1   0 \n",
      "[-0.71777344  0.30566406 -0.06622314  0.35888672  0.07873535 -0.04022217] 3   5 \n",
      "[-0.71435547  0.20446777 -0.03326416  0.32299805  0.08917236  0.07928467] 3   3 Match 217\n",
      "\n",
      "[-0.64404297  0.30078125 -0.01050568  0.3173828   0.17285156  0.00209427] 3   4 \n",
      "[-0.7573242   0.25195312 -0.04040527  0.3515625   0.0308075   0.05413818] 3   1 \n",
      "[-0.69921875  0.22058105 -0.01556396  0.30737305  0.1050415   0.01953125] 3   4 \n",
      "[-0.7319336   0.16955566 -0.05273438  0.35327148  0.06652832  0.05783081] 3   5 \n",
      "[-0.70654297  0.2244873  -0.00872803  0.30541992  0.11767578  0.03793335] 3   2 \n",
      "[-0.68408203  0.18151855 -0.00721741  0.3359375   0.12902832  0.04806519] 3   0 \n",
      "[-0.75097656  0.18652344 -0.10437012  0.38916016  0.0553894   0.07830811] 3   1 \n",
      "[-0.7290039   0.21057129 -0.01293182  0.30664062  0.12841797  0.0668335 ] 3   4 \n",
      "[-0.6953125   0.29418945 -0.02999878  0.27026367  0.20239258 -0.01585388] 1   5 \n",
      "[-0.7192383   0.23071289 -0.03167725  0.32763672  0.12219238  0.04003906] 3   1 \n",
      "[-0.59277344  0.28857422  0.03915405  0.32080078  0.21350098  0.02078247] 3   5 \n",
      "[-0.65283203  0.23364258  0.00288963  0.30786133  0.13842773  0.03964233] 3   3 Match 218\n",
      "\n",
      "[-7.2949219e-01  2.2900391e-01  5.7172775e-04  3.8305664e-01\n",
      "  7.3608398e-02  3.6895752e-02] 3   5 \n",
      "[-0.74316406  0.24487305 -0.06130981  0.35229492  0.04962158  0.05523682] 3   1 \n",
      "[-0.79541016  0.29589844 -0.02030945  0.34423828  0.11706543  0.01548004] 3   5 \n",
      "[-0.73779297  0.24694824 -0.00690842  0.3269043   0.07855225  0.021698  ] 3   1 \n",
      "[-0.7158203   0.19970703 -0.0300293   0.33520508  0.09851074  0.07281494] 3   4 \n",
      "[-0.7709961   0.20751953 -0.04663086  0.37768555  0.06646729  0.07867432] 3   2 \n",
      "[-0.68408203  0.22253418 -0.01061249  0.33862305  0.09857178  0.05172729] 3   5 \n",
      "[-0.69921875  0.20812988 -0.05664062  0.32885742  0.08551025  0.03363037] 3   1 \n",
      "[-0.7558594   0.30932617 -0.04171753  0.30444336  0.08691406 -0.01615906] 1   5 \n",
      "[-0.73339844  0.22485352 -0.01280212  0.29370117  0.09460449  0.03038025] 3   4 \n",
      "[-0.76464844  0.18066406 -0.02766418  0.34936523  0.05117798  0.06933594] 3   5 \n",
      "[-0.6933594   0.20739746 -0.04345703  0.31274414  0.07702637  0.04232788] 3   4 \n",
      "[-0.69677734  0.21350098 -0.00271797  0.31884766  0.10668945  0.0585022 ] 3   1 \n",
      "[-0.7973633   0.19836426 -0.06054688  0.34472656  0.06719971  0.01142883] 3   2 \n",
      "[-0.6713867   0.26538086 -0.00933838  0.32885742  0.07702637  0.05175781] 3   5 \n",
      "[-0.73876953  0.19812012 -0.01261139  0.31396484  0.08312988  0.06500244] 3   1 \n",
      "[-0.6591797   0.29418945  0.02645874  0.29345703  0.11187744  0.02998352] 1   4 \n",
      "[-0.77490234  0.18725586 -0.04483032  0.3540039   0.07440186  0.02600098] 3   4 \n",
      "[-0.7128906   0.28686523 -0.04302979  0.32104492  0.10461426  0.05001831] 3   4 \n",
      "[-0.74902344  0.21435547 -0.01979065  0.34936523  0.08001709  0.03890991] 3   5 \n",
      "[-0.7709961   0.2154541  -0.10113525  0.3178711   0.10247803 -0.03295898] 3   3 Match 219\n",
      "\n",
      "[-0.7011719   0.2355957   0.01867676  0.31469727  0.10430908  0.04336548] 3   5 \n",
      "[-0.6748047   0.27197266 -0.00324821  0.2932129   0.14135742  0.01681519] 3   4 \n",
      "[-0.6870117   0.25683594 -0.03262329  0.30419922  0.1239624   0.03692627] 3   4 \n",
      "[-0.73535156  0.22570801 -0.03062439  0.31469727  0.08068848  0.05587769] 3   1 \n",
      "[-0.7133789   0.2548828  -0.04867554  0.33154297  0.06433105  0.04934692] 3   0 \n",
      "[-0.7578125   0.20507812 -0.04901123  0.32714844  0.05886841  0.06671143] 3   4 \n",
      "[-0.6826172   0.24768066 -0.02700806  0.3076172   0.09899902  0.06036377] 3   3 Match 220\n",
      "\n",
      "[-0.70458984  0.20495605 -0.01901245  0.28344727  0.09552002  0.02763367] 3   3 Match 221\n",
      "\n",
      "[-0.6904297   0.2590332  -0.00949097  0.33203125  0.11102295  0.01383972] 3   1 \n",
      "[-0.7128906   0.20739746 -0.02313232  0.3269043   0.09869385  0.05163574] 3   4 \n",
      "[-0.73779297  0.27807617 -0.0668335   0.33032227  0.05130005  0.00995636] 3   2 \n",
      "[-0.77490234  0.22033691 -0.04815674  0.3149414   0.06469727  0.02844238] 3   3 Match 222\n",
      "\n",
      "[-0.71533203  0.20495605 -0.03060913  0.3317871   0.09991455  0.07971191] 3   4 \n",
      "[-0.72558594  0.19750977 -0.01745605  0.32006836  0.07849121  0.04946899] 3   4 \n",
      "[-0.8154297   0.21643066 -0.15991211  0.34448242  0.06304932  0.040802  ] 3   4 \n",
      "[-0.7026367   0.24291992 -0.02780151  0.29711914  0.12524414  0.02674866] 3   3 Match 223\n",
      "\n",
      "[-0.7236328   0.1821289  -0.01702881  0.34350586  0.11737061  0.0769043 ] 3   4 \n",
      "[-0.66259766  0.27270508  0.06677246  0.2932129   0.15551758  0.00386047] 3   4 \n",
      "[-0.6665039   0.26660156  0.0116806   0.31274414  0.10888672  0.00996399] 3   2 \n",
      "[-0.72998047  0.18688965 -0.04421997  0.35766602  0.07739258  0.06781006] 3   5 \n",
      "[-0.59277344  0.3022461   0.03826904  0.2944336   0.17175293  0.00169182] 1   4 \n",
      "[-0.6064453   0.24475098  0.02529907  0.27539062  0.16210938  0.05480957] 3   1 \n",
      "[-0.60058594  0.31347656  0.05578613  0.27954102  0.18395996  0.02342224] 1   1 Match 224\n",
      "\n",
      "[-0.6845703   0.2705078   0.00270653  0.32836914  0.08056641  0.02192688] 3   2 \n",
      "[-0.7104492   0.17871094 -0.03271484  0.31958008  0.10772705  0.08001709] 3   5 \n",
      "[-0.7792969   0.20092773  0.03509521  0.35913086  0.10040283  0.0536499 ] 3   3 Match 225\n",
      "\n",
      "[-0.73339844  0.22888184 -0.06243896  0.34545898  0.06762695  0.08013916] 3   4 \n",
      "[-0.77001953  0.2088623  -0.05349731  0.328125    0.06854248  0.03979492] 3   3 Match 226\n",
      "\n",
      "[-0.7001953   0.18139648 -0.02067566  0.3112793   0.12561035  0.06378174] 3   4 \n",
      "[-0.7705078   0.19030762 -0.03448486  0.34594727  0.0682373   0.04104614] 3   0 \n",
      "[-0.74072266  0.2541504  -0.06530762  0.35131836  0.04919434  0.06738281] 3   3 Match 227\n",
      "\n",
      "[-0.74560547  0.27783203 -0.05432129  0.3527832   0.11334229  0.02008057] 3   1 \n",
      "[-0.67041016  0.2746582  -0.00792694  0.28833008  0.15686035  0.05673218] 3   4 \n",
      "[-0.6459961   0.2631836  -0.10205078  0.35717773  0.17529297  0.07470703] 3   4 \n",
      "[-0.74658203  0.22094727 -0.05020142  0.3227539   0.11700439  0.05969238] 3   4 \n",
      "[-0.6269531   0.27929688  0.06240845  0.27124023  0.20141602  0.00546646] 1   4 \n",
      "[-0.7182617   0.2088623  -0.02986145  0.36743164  0.06622314  0.06750488] 3   1 \n",
      "[-0.7109375   0.21130371 -0.0451355   0.32641602  0.09771729  0.09118652] 3   2 \n",
      "[-0.73046875  0.21484375  0.00135136  0.31811523  0.09454346  0.05023193] 3   1 \n",
      "[-0.7265625   0.22009277 -0.05648804  0.31103516  0.06420898  0.04299927] 3   4 \n",
      "[-0.6645508   0.20593262 -0.02720642  0.30493164  0.12329102  0.02740479] 3   1 \n",
      "[-0.7392578   0.26953125 -0.10449219  0.3486328   0.1484375   0.00227737] 3   2 \n",
      "[-0.7573242   0.29858398 -0.07843018  0.32202148  0.125      -0.01869202] 3   5 \n",
      "[-0.6176758   0.27978516  0.0506897   0.3017578   0.17333984  0.00791168] 3   2 \n",
      "[-0.7182617   0.27490234 -0.02113342  0.30566406  0.08398438 -0.00849152] 3   3 Match 228\n",
      "\n",
      "[-0.70996094  0.25024414 -0.04483032  0.33544922  0.07940674  0.0345459 ] 3   4 \n",
      "[-0.7084961   0.20483398 -0.0585022   0.30664062  0.09259033  0.04568481] 3   5 \n",
      "[-0.7211914   0.23034668 -0.02513123  0.35058594  0.09674072  0.08538818] 3   0 \n",
      "[-0.7241211   0.25610352 -0.02180481  0.35107422  0.04223633  0.02677917] 3   3 Match 229\n",
      "\n",
      "[-0.7651367   0.19213867 -0.02531433  0.3400879   0.04556274  0.06561279] 3   5 \n",
      "[-0.68847656  0.22399902 -0.04788208  0.3149414   0.05981445  0.06222534] 3   2 \n",
      "[-0.59814453  0.28686523  0.02246094  0.328125    0.22937012  0.04959106] 3   3 Match 230\n",
      "\n",
      "[-0.69970703  0.27172852 -0.01211548  0.29760742  0.09851074  0.01663208] 3   4 \n",
      "[-0.63378906  0.30395508  0.06628418  0.30273438  0.14990234  0.03024292] 1   5 \n",
      "[-0.7011719   0.2442627   0.02688599  0.28955078  0.13049316  0.02185059] 3   3 Match 231\n",
      "\n",
      "[-0.76123047  0.22961426 -0.08392334  0.34179688  0.02838135  0.00546646] 3   0 \n",
      "[-0.66308594  0.25146484 -0.01737976  0.29003906  0.11169434  0.03363037] 3   2 \n",
      "[-0.70751953  0.27270508 -0.0480957   0.31298828  0.1194458  -0.04394531] 3   1 \n",
      "[-0.6875      0.27612305 -0.0134201   0.29467773  0.16259766  0.00461197] 3   2 \n",
      "[-6.8603516e-01  2.2070312e-01 -6.4516068e-04  3.0981445e-01\n",
      "  1.2402344e-01  4.2480469e-02] 3   1 \n",
      "[-0.5698242   0.3022461   0.06323242  0.25390625  0.19299316  0.06286621] 1   5 \n",
      "[-0.77734375  0.22729492 -0.06121826  0.3413086   0.0925293   0.02960205] 3   2 \n",
      "[-0.7558594   0.24719238 -0.06817627  0.29296875  0.07965088 -0.01042938] 3   0 \n",
      "[-0.72314453  0.22192383 -0.08905029  0.32470703  0.10430908  0.04248047] 3   2 \n",
      "[-0.80126953  0.29345703 -0.03234863  0.31347656  0.1616211  -0.02297974] 3   5 \n",
      "[-0.7607422   0.2993164  -0.05322266  0.3413086   0.08483887 -0.05151367] 3   3 Match 232\n",
      "\n",
      "[-0.74072266  0.22839355  0.01687622  0.3552246   0.07189941  0.04418945] 3   5 \n",
      "[-0.74365234  0.28735352 -0.07287598  0.3449707   0.09594727  0.00870514] 3   1 \n",
      "[-0.6870117   0.2631836   0.01251984  0.29760742  0.11846924  0.0297699 ] 3   1 \n",
      "[-0.79052734  0.29711914 -0.04315186  0.33691406  0.08007812  0.00645828] 3   5 \n",
      "[-0.6660156   0.31030273  0.0352478   0.28833008  0.15515137  0.01155853] 1   1 Match 233\n",
      "\n",
      "[-0.74072266  0.16345215 -0.08178711  0.40698242  0.0625      0.06329346] 3   1 \n",
      "[-0.7495117   0.24841309 -0.03329468  0.32202148  0.08184814  0.05245972] 3   3 Match 234\n",
      "\n",
      "[-0.7368164   0.22033691 -0.0453186   0.33374023  0.0916748   0.04492188] 3   4 \n",
      "[-0.77246094  0.15856934 -0.02714539  0.4140625   0.04977417  0.09588623] 3   3 Match 235\n",
      "\n",
      "[-0.70751953  0.24829102 -0.06903076  0.2919922   0.10876465  0.04956055] 3   3 Match 236\n",
      "\n",
      "[-0.72216797  0.23791504 -0.00894928  0.29907227  0.08636475  0.04467773] 3   4 \n",
      "[-0.74072266  0.16967773 -0.06112671  0.32885742  0.05844116  0.07299805] 3   5 \n",
      "[-0.7211914   0.22961426 -0.03668213  0.30810547  0.09210205  0.07214355] 3   5 \n",
      "[-0.68603516  0.21325684 -0.0148468   0.30566406  0.13781738  0.02404785] 3   5 \n",
      "[-0.7758789   0.24133301 -0.0491333   0.32421875  0.09375     0.0350647 ] 3   2 \n",
      "[-0.73876953  0.21716309 -0.00453568  0.34033203  0.09484863  0.04244995] 3   4 \n",
      "[-0.62841797  0.2800293  -0.00447845  0.31054688  0.12512207  0.06890869] 3   2 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6044922   0.2956543   0.05236816  0.28125     0.18688965  0.01425171] 1   4 \n",
      "[-0.7246094   0.22143555 -0.02952576  0.32592773  0.07403564  0.06469727] 3   4 \n",
      "[-0.72509766  0.15710449 -0.11071777  0.36132812  0.06884766  0.13452148] 3   2 \n",
      "[-0.75        0.16149902 -0.05700684  0.33496094  0.06982422  0.05307007] 3   2 \n",
      "[-0.7114258   0.21447754 -0.00648117  0.32641602  0.1038208   0.0380249 ] 3   2 \n",
      "[-0.71240234  0.19689941 -0.02619934  0.33984375  0.11260986  0.07275391] 3   2 \n",
      "[-0.7792969   0.2076416  -0.08239746  0.29492188  0.02375793 -0.01380157] 3   3 Match 237\n",
      "\n",
      "[-0.7324219   0.2541504  -0.01544952  0.3149414   0.08380127  0.01794434] 3   4 \n",
      "[-0.75        0.21826172 -0.05493164  0.34350586  0.04837036  0.06268311] 3   0 \n",
      "[-0.6791992   0.21740723 -0.04846191  0.30200195  0.08581543  0.06945801] 3   3 Match 238\n",
      "\n",
      "[-0.7866211   0.2619629  -0.10186768  0.35498047  0.10565186 -0.02290344] 3   5 \n",
      "[-0.67089844  0.31420898  0.01834106  0.35083008  0.13354492  0.00780869] 3   3 Match 239\n",
      "\n",
      "[-0.7441406   0.23535156 -0.13391113  0.30688477  0.08642578 -0.01429749] 3   1 \n",
      "[-0.6879883   0.2722168  -0.01290131  0.3005371   0.1463623   0.05126953] 3   4 \n",
      "[-0.7167969   0.2644043  -0.00783539  0.32543945  0.1348877  -0.00401306] 3   4 \n",
      "[-0.7373047   0.21350098  0.00773239  0.33154297  0.09558105  0.06384277] 3   3 Match 240\n",
      "\n",
      "[-0.72998047  0.20336914 -0.02519226  0.31201172  0.10028076  0.01708984] 3   5 \n",
      "[-0.6166992   0.25048828  0.02583313  0.28857422  0.15197754  0.01278687] 3   4 \n",
      "[-0.67089844  0.24511719  0.01236725  0.2758789   0.16625977 -0.01212311] 3   4 \n",
      "[-0.7573242   0.19641113 -0.03039551  0.33813477  0.0692749   0.06396484] 3   4 \n",
      "[-0.7001953   0.19555664 -0.05612183  0.31933594  0.11853027  0.04547119] 3   2 \n",
      "[-0.6894531   0.19458008 -0.03009033  0.31176758  0.08850098  0.03973389] 3   4 \n",
      "[-0.81347656  0.20581055 -0.17907715  0.328125    0.14624023 -0.03433228] 3   2 \n",
      "[-0.7421875   0.21398926 -0.04443359  0.33740234  0.06658936  0.06561279] 3   5 \n",
      "[-0.6616211   0.25927734  0.02867126  0.29663086  0.19128418  0.02731323] 3   2 \n",
      "[-0.73291016  0.2397461  -0.01856995  0.30249023  0.1050415   0.00735092] 3   4 \n",
      "[-0.70214844  0.22705078 -0.03530884  0.33544922  0.09423828  0.04345703] 3   2 \n",
      "[-0.76953125  0.23962402 -0.10345459  0.36279297  0.09240723  0.05447388] 3   5 \n",
      "[-0.7138672   0.2166748  -0.0305481   0.30810547  0.09735107  0.02113342] 3   5 \n",
      "[-0.6635742   0.2208252   0.00290298  0.29516602  0.11346436  0.02679443] 3   2 \n",
      "[-0.7290039   0.19091797 -0.03390503  0.32006836  0.08319092  0.04714966] 3   0 \n",
      "[-0.7529297   0.2241211  -0.06549072  0.33935547  0.06738281 -0.03027344] 3   1 \n",
      "[-0.70947266  0.23571777 -0.00744247  0.3083496   0.10662842  0.0489502 ] 3   4 \n",
      "[-0.7163086   0.19006348 -0.04544067  0.33862305  0.07220459  0.06591797] 3   2 \n",
      "[-0.6894531   0.17114258 -0.04949951  0.35205078  0.08111572  0.08758545] 3   4 \n",
      "[-0.75878906  0.22790527 -0.08587646  0.3232422   0.09436035  0.0109787 ] 3   2 \n",
      "[-0.71435547  0.22741699 -0.02767944  0.33520508  0.09783936  0.06323242] 3   5 \n",
      "[-0.71875     0.27392578 -0.05514526  0.2919922   0.10345459  0.03111267] 3   2 \n",
      "[-0.7705078   0.21508789 -0.03207397  0.35595703  0.0604248   0.04901123] 3   2 \n",
      "[-0.7089844   0.21472168 -0.04019165  0.33032227  0.11590576  0.03933716] 3   1 \n",
      "[-7.2070312e-01  2.3059082e-01 -1.2707710e-04  3.5864258e-01\n",
      "  9.7778320e-02 -5.3071976e-04] 3   5 \n",
      "[-0.5883789   0.3371582   0.05752563  0.2927246   0.16271973  0.00061607] 1   4 \n",
      "[-0.70214844  0.24328613 -0.05673218  0.32104492  0.07611084  0.03677368] 3   1 \n",
      "[-0.6923828   0.20581055 -0.08648682  0.28442383  0.13244629  0.0086441 ] 3   2 \n",
      "[-0.7114258   0.25463867  0.00618744  0.2998047   0.10620117  0.01454163] 3   3 Match 241\n",
      "\n",
      "[-0.6738281   0.2097168  -0.02372742  0.32104492  0.12017822  0.04193115] 3   1 \n",
      "[-0.7470703   0.25439453 -0.10479736  0.30419922  0.08520508  0.00839233] 3   4 \n",
      "[-0.71191406  0.20837402 -0.03123474  0.31640625  0.08807373  0.05059814] 3   2 \n",
      "[-0.7705078   0.22033691 -0.03173828  0.31030273  0.08929443  0.03372192] 3   5 \n",
      "[-0.7519531   0.18005371 -0.04266357  0.3149414   0.08966064  0.04653931] 3   5 \n",
      "[-0.68066406  0.24279785  0.01171112  0.29516602  0.13903809  0.03173828] 3   2 \n",
      "[-0.62841797  0.2944336   0.03167725  0.2932129   0.12084961 -0.00746918] 1   2 \n",
      "[-0.7758789   0.23608398 -0.06130981  0.35913086  0.04348755  0.02787781] 3   4 \n",
      "[-0.7182617   0.20910645 -0.03582764  0.33569336  0.08520508  0.08355713] 3   4 \n",
      "[-0.7788086   0.15991211 -0.03799438  0.36083984  0.046875    0.02545166] 3   1 \n",
      "[-0.7089844   0.24804688 -0.01421356  0.32299805  0.11090088  0.04327393] 3   0 \n",
      "[-0.7441406   0.25024414 -0.03692627  0.3540039   0.0453186   0.04745483] 3   4 \n",
      "[-0.58984375  0.29492188  0.04327393  0.29077148  0.1920166   0.05099487] 1   3 \n",
      "[-0.7348633   0.18359375 -0.06286621  0.359375    0.08135986  0.08575439] 3   1 \n",
      "[-0.7294922   0.25878906 -0.08239746  0.2878418   0.11395264 -0.00830841] 3   2 \n",
      "[-0.6958008   0.25805664 -0.01858521  0.34375     0.09979248  0.0581665 ] 3   3 Match 242\n",
      "\n",
      "[-0.6074219   0.2841797   0.02444458  0.32885742  0.19799805  0.02233887] 3   3 Match 243\n",
      "\n",
      "[-0.73828125  0.18310547 -0.02960205  0.31982422  0.08435059  0.06274414] 3   4 \n",
      "[-0.6855469   0.21813965 -0.02987671  0.32177734  0.06542969  0.03945923] 3   5 \n",
      "[-0.6430664   0.3034668   0.02047729  0.31347656  0.12866211  0.01530457] 3   3 Match 244\n",
      "\n",
      "[-0.625       0.2878418   0.03186035  0.27612305  0.17272949  0.04769897] 1   2 \n",
      "[-0.6875      0.18579102 -0.02406311  0.31103516  0.11364746  0.05435181] 3   2 \n",
      "[-0.7626953   0.20507812 -0.08959961  0.36254883  0.05111694  0.05886841] 3   4 \n",
      "[-0.765625    0.15185547 -0.01786804  0.35107422  0.0645752   0.06811523] 3   1 \n",
      "[-0.73828125  0.23486328 -0.07434082  0.31713867  0.02406311  0.04165649] 3   1 \n",
      "[-0.5883789   0.32104492  0.04217529  0.2697754   0.17810059  0.02075195] 1   5 \n",
      "[-0.60058594  0.28637695  0.06561279  0.32617188  0.21777344  0.01667786] 3   5 \n",
      "[-0.70458984  0.26733398 -0.05426025  0.34106445  0.13317871  0.01657104] 3   3 Match 245\n",
      "\n",
      "[-0.71484375  0.25463867 -0.01394653  0.34448242  0.09692383  0.03573608] 3   2 \n",
      "[-0.6738281   0.27416992  0.04299927  0.29956055  0.13989258  0.01933289] 3   0 \n",
      "[-0.69677734  0.19592285 -0.00747299  0.32055664  0.11010742  0.02383423] 3   3 Match 246\n",
      "\n",
      "[-0.7236328   0.19177246 -0.05895996  0.31811523  0.07891846  0.05050659] 3   5 \n",
      "[-0.69628906  0.17749023 -0.02462769  0.32348633  0.11029053  0.02058411] 3   4 \n",
      "[-0.6958008   0.2319336  -0.01308441  0.3100586   0.09588623  0.03009033] 3   5 \n",
      "[-0.7128906   0.20495605 -0.02001953  0.32910156  0.09503174  0.05847168] 3   3 Match 247\n",
      "\n",
      "[-0.5917969   0.30151367  0.06640625  0.296875    0.18103027  0.02696228] 1   2 \n",
      "[-0.69433594  0.2064209  -0.00567245  0.3334961   0.09912109  0.06408691] 3   2 \n",
      "[-0.7597656   0.18530273 -0.03216553  0.33276367  0.1038208   0.02857971] 3   1 \n",
      "[-0.7109375   0.20617676 -0.03356934  0.31079102  0.08361816  0.0803833 ] 3   1 \n",
      "[-0.71875     0.1887207  -0.0275116   0.30273438  0.09246826  0.05474854] 3   3 Match 248\n",
      "\n",
      "[-0.7402344   0.20080566 -0.03344727  0.3046875   0.09039307  0.03744507] 3   5 \n",
      "[-0.70751953  0.23535156  0.00506973  0.32080078  0.14025879  0.03378296] 3   4 \n",
      "[-0.6401367   0.31884766  0.03515625  0.31958008  0.16760254  0.02336121] 3   0 \n",
      "[-0.6791992   0.20776367 -0.00813293  0.29736328  0.1262207   0.0322876 ] 3   2 \n",
      "[-0.7397461   0.1973877  -0.01381683  0.33251953  0.10986328  0.05725098] 3   5 \n",
      "[-0.70947266  0.20861816 -0.01282501  0.3232422   0.09820557  0.03744507] 3   4 \n",
      "[-0.6767578   0.2614746  -0.040802    0.31640625  0.10620117  0.05584717] 3   2 \n",
      "[-0.69677734  0.21191406 -0.03942871  0.33618164  0.08673096  0.06988525] 3   3 Match 249\n",
      "\n",
      "[-0.7363281   0.15844727 -0.04129028  0.32666016  0.0803833   0.05923462] 3   2 \n",
      "[-0.61083984  0.2680664   0.02658081  0.32421875  0.23510742  0.03268433] 3   3 Match 250\n",
      "\n",
      "[-0.73876953  0.24487305 -0.04504395  0.36083984  0.07794189  0.02763367] 3   1 \n",
      "[-0.7192383   0.2241211  -0.00865936  0.2878418   0.10424805  0.04257202] 3   2 \n",
      "[-0.6879883   0.2199707  -0.02734375  0.3203125   0.1060791   0.04556274] 3   5 \n",
      "[-0.71435547  0.23828125 -0.02865601  0.31469727  0.10626221  0.04272461] 3   5 \n",
      "[-0.70751953  0.23181152 -0.02830505  0.3095703   0.09143066  0.06378174] 3   3 Match 251\n",
      "\n",
      "[-0.7006836   0.20214844 -0.00553513  0.29760742  0.10980225  0.05776978] 3   3 Match 252\n",
      "\n",
      "[-0.63720703  0.2939453   0.03860474  0.25952148  0.17785645  0.03601074] 1   2 \n",
      "[-0.7133789   0.26660156 -0.01622009  0.28198242  0.109375   -0.00730133] 3   4 \n",
      "[-0.5678711   0.30029297  0.09674072  0.30029297  0.22485352 -0.00850677] 1   4 \n",
      "[-0.7792969   0.2927246  -0.10089111  0.30737305  0.13012695 -0.00537872] 3   3 Match 253\n",
      "\n",
      "[-0.6743164   0.22436523 -0.02754211  0.31347656  0.11053467  0.0586853 ] 3   1 \n",
      "[-0.6069336   0.27270508  0.01087952  0.3059082   0.18664551  0.03131104] 3   3 Match 254\n",
      "\n",
      "[-0.72998047  0.17602539 -0.06445312  0.32641602  0.07342529  0.05523682] 3   4 \n",
      "[-0.6645508   0.20593262  0.00149155  0.31762695  0.14648438  0.05618286] 3   3 Match 255\n",
      "\n",
      "[-0.6928711   0.20373535 -0.00915527  0.2758789   0.12756348  0.03796387] 3   2 \n",
      "[-0.7285156   0.24804688  0.02308655  0.31298828  0.08392334  0.04031372] 3   1 \n",
      "[-0.6923828   0.21411133 -0.04043579  0.30249023  0.08898926  0.05215454] 3   4 \n",
      "[-0.69140625  0.21118164 -0.00739288  0.30566406  0.1081543   0.04776001] 3   3 Match 256\n",
      "\n",
      "[-0.7285156   0.20117188 -0.05004883  0.35083008  0.1151123   0.07519531] 3   4 \n",
      "[-0.6982422   0.18347168 -0.01753235  0.3137207   0.08227539  0.03726196] 3   1 \n",
      "[-0.71777344  0.24047852 -0.03866577  0.3137207   0.05117798  0.03634644] 3   2 \n",
      "[-0.72216797  0.22167969 -0.02050781  0.32763672  0.1005249   0.03118896] 3   2 \n",
      "[-0.7192383   0.24438477 -0.0145874   0.28930664  0.09136963  0.01002502] 3   0 \n",
      "[-0.7089844   0.19714355 -0.01529694  0.30322266  0.10247803  0.03117371] 3   2 \n",
      "[-0.6958008   0.25390625 -0.03848267  0.30517578  0.09466553 -0.00568771] 3   2 \n",
      "[-0.6845703   0.21325684 -0.02668762  0.30151367  0.07501221  0.06439209] 3   3 Match 257\n",
      "\n",
      "[-0.70751953  0.18811035 -0.11810303  0.34765625  0.05303955  0.08721924] 3   5 \n",
      "[-0.7368164   0.23620605 -0.00913239  0.30493164  0.07995605  0.04455566] 3   4 \n",
      "[-0.7216797   0.15637207 -0.04367065  0.33325195  0.08239746  0.0619812 ] 3   2 \n",
      "[-0.72216797  0.24328613 -0.03533936  0.3256836   0.09771729  0.02975464] 3   3 Match 258\n",
      "\n",
      "[-0.7241211   0.23754883 -0.04052734  0.33691406  0.0892334   0.0534668 ] 3   3 Match 259\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.66552734  0.22253418 -0.03692627  0.29785156  0.09625244  0.05340576] 3   4 \n",
      "[-0.7114258   0.28710938 -0.04708862  0.3413086   0.09912109  0.00397491] 3   4 \n",
      "[-0.68652344  0.19470215  0.00685501  0.31054688  0.13269043  0.05456543] 3   2 \n",
      "[-0.71875     0.18017578 -0.01980591  0.36108398  0.08129883  0.05566406] 3   3 Match 260\n",
      "\n",
      "[-0.8017578   0.22631836 -0.03050232  0.3371582   0.07177734  0.03579712] 3   2 \n",
      "[-0.68847656  0.20153809 -0.03921509  0.29492188  0.10144043  0.08050537] 3   5 \n",
      "[-0.7548828   0.19543457 -0.06228638  0.3251953   0.08026123  0.05081177] 3   1 \n",
      "[-0.69921875  0.19042969 -0.0134964   0.30908203  0.09936523  0.05618286] 3   2 \n",
      "[-0.6660156   0.22619629 -0.02914429  0.34985352  0.08618164  0.08868408] 3   0 \n",
      "[-0.7192383   0.23620605 -0.01470947  0.3071289   0.06707764  0.04785156] 3   0 \n",
      "[-0.7089844   0.20629883 -0.08679199  0.32666016  0.0647583   0.06591797] 3   3 Match 261\n",
      "\n",
      "[-0.7680664   0.17553711 -0.03842163  0.3564453   0.08831787  0.01091003] 3   2 \n",
      "[-0.7553711   0.23864746 -0.05062866  0.34350586  0.10150146  0.07635498] 3   1 \n",
      "[-0.6274414   0.2800293   0.04327393  0.28295898  0.20812988 -0.00940704] 3   4 \n",
      "[-0.7207031   0.15270996 -0.02363586  0.3256836   0.12261963  0.0760498 ] 3   2 \n",
      "[-0.7133789   0.22851562 -0.08703613  0.33129883  0.11407471  0.00542831] 3   0 \n",
      "[-0.7104492   0.31860352 -0.07696533  0.3449707   0.12084961 -0.01104736] 3   1 \n",
      "261\n"
     ]
    }
   ],
   "source": [
    "Pred=[]\n",
    "\n",
    "countCorrect=0\n",
    "\n",
    "for row in range(TestModel_outputs.shape[0]):\n",
    "    outputs=TestModel_outputs[row]\n",
    "    #print(test.iloc[row,0])\n",
    "    print(outputs, end=' ')\n",
    "    \n",
    "    result=0\n",
    "    if outputs[0]<outputs[1]:result=1\n",
    "    if outputs[result]<outputs[2]:result=2\n",
    "    if outputs[result]<outputs[3]:result=3\n",
    "    if outputs[result]<outputs[4]:result=4\n",
    "    if outputs[result]<outputs[5]:result=5\n",
    "    Pred.append(result)\n",
    "    print(result, ' ',test.iloc[row,1], end=' ')\n",
    "    if result==test.iloc[row,1]:\n",
    "        countCorrect+=1\n",
    "        print('Match',countCorrect)\n",
    "    print('')\n",
    "\n",
    "print(countCorrect)\n",
    "#Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   5   0  87   0   0]\n",
      " [  0  13   0 237   0   0]\n",
      " [  0  18   0 196   0   0]\n",
      " [  0  19   0 248   0   0]\n",
      " [  0  26   0 223   0   0]\n",
      " [  0  15   0 196   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    " \n",
    "print(metrics.confusion_matrix(test['labels'],Pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Pants       0.00      0.00      0.00        92\n",
      "       False       0.14      0.05      0.08       250\n",
      " Barely-True       0.00      0.00      0.00       214\n",
      "   Half-True       0.21      0.93      0.34       267\n",
      " Mostly-True       0.00      0.00      0.00       249\n",
      "        True       0.00      0.00      0.00       211\n",
      "\n",
      "    accuracy                           0.20      1283\n",
      "   macro avg       0.06      0.16      0.07      1283\n",
      "weighted avg       0.07      0.20      0.09      1283\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Pants', 'False', 'Barely-True','Half-True','Mostly-True','True']\n",
    "\n",
    "print(metrics.classification_report(test['labels'], Pred,target_names =target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Saving Complete on 2020-03-30 18:53:43.668869 in: ./TunedModels/albert/albert-large-v2/Saves/\n"
     ]
    }
   ],
   "source": [
    "# saving the output of the models to CSVs\n",
    "#these are 1X6 classification vectors\n",
    "\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Saves/\"\n",
    "print('Saving...')\n",
    "trainOut = pd.DataFrame(data= TrainModel_outputs )\n",
    "trainOut.to_csv(SavesDirectory+'trainOut.tsv', sep='\\t',  index=False)\n",
    "\n",
    "evalOut = pd.DataFrame(data= EvalModel_outputs )\n",
    "evalOut.to_csv(SavesDirectory+'evalOut.tsv', sep='\\t',  index=False)\n",
    "\n",
    "testOut = pd.DataFrame(data= TestModel_outputs )\n",
    "testOut.to_csv(SavesDirectory+'testOut.tsv', sep='\\t',  index=False)\n",
    "\n",
    "print('Saving Complete on',datetime.now() ,'in:', SavesDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)\n",
    "#del(train,Eval,test)\n",
    "del(trainOut,evalOut,testOut)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Adding the reputation vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section takes the output results from the transformer used above and uses it together with the speaker's reputation to enhance the classification.\n",
    "\n",
    "Before running this section it is suggested that you halt the program and start running it again from this cell. The neural net will likely have an error caused by some unreleased variable used by thr simple transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PantsTotal</th>\n",
       "      <th>NotRealTotal</th>\n",
       "      <th>BarelyTotal</th>\n",
       "      <th>HalfTotal</th>\n",
       "      <th>MostlyTotal</th>\n",
       "      <th>Truths</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.723145</td>\n",
       "      <td>0.213379</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.322021</td>\n",
       "      <td>0.105225</td>\n",
       "      <td>0.046814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.657227</td>\n",
       "      <td>0.261475</td>\n",
       "      <td>-0.007530</td>\n",
       "      <td>0.341064</td>\n",
       "      <td>0.132202</td>\n",
       "      <td>-0.009216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.711426</td>\n",
       "      <td>0.196411</td>\n",
       "      <td>-0.025452</td>\n",
       "      <td>0.342285</td>\n",
       "      <td>0.091919</td>\n",
       "      <td>0.047363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.744629</td>\n",
       "      <td>0.279541</td>\n",
       "      <td>-0.048859</td>\n",
       "      <td>0.376221</td>\n",
       "      <td>0.030746</td>\n",
       "      <td>-0.011841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.687012</td>\n",
       "      <td>0.213623</td>\n",
       "      <td>-0.060669</td>\n",
       "      <td>0.288818</td>\n",
       "      <td>0.093445</td>\n",
       "      <td>0.036285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10264</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.701172</td>\n",
       "      <td>0.222290</td>\n",
       "      <td>-0.032501</td>\n",
       "      <td>0.326172</td>\n",
       "      <td>0.062439</td>\n",
       "      <td>0.044952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10265</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.699707</td>\n",
       "      <td>0.165649</td>\n",
       "      <td>-0.015297</td>\n",
       "      <td>0.312988</td>\n",
       "      <td>0.124878</td>\n",
       "      <td>0.049164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.642578</td>\n",
       "      <td>0.189941</td>\n",
       "      <td>-0.032532</td>\n",
       "      <td>0.315674</td>\n",
       "      <td>0.144043</td>\n",
       "      <td>0.040405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10267</th>\n",
       "      <td>0.305</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.728516</td>\n",
       "      <td>0.233398</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.306885</td>\n",
       "      <td>0.088257</td>\n",
       "      <td>-0.003422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10268</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.763672</td>\n",
       "      <td>0.226074</td>\n",
       "      <td>-0.055267</td>\n",
       "      <td>0.343994</td>\n",
       "      <td>0.086304</td>\n",
       "      <td>0.020523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10269 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PantsTotal  NotRealTotal  BarelyTotal  HalfTotal  MostlyTotal  Truths  \\\n",
       "0           0.005         0.000        0.000      0.000        0.000   0.000   \n",
       "1           0.095         0.160        0.170      0.290        0.165   0.165   \n",
       "2           0.005         0.010        0.005      0.015        0.040   0.010   \n",
       "3           0.005         0.010        0.005      0.015        0.040   0.010   \n",
       "4           0.035         0.145        0.200      0.345        0.380   0.365   \n",
       "...           ...           ...          ...        ...          ...     ...   \n",
       "10264       0.005         0.030        0.070      0.050        0.050   0.020   \n",
       "10265       0.055         0.075        0.080      0.100        0.050   0.035   \n",
       "10266       0.035         0.115        0.140      0.190        0.170   0.075   \n",
       "10267       0.305         0.570        0.315      0.255        0.185   0.070   \n",
       "10268       0.000         0.005        0.000      0.000        0.000   0.000   \n",
       "\n",
       "              0         1         2         3         4         5  \n",
       "0     -0.723145  0.213379  0.000787  0.322021  0.105225  0.046814  \n",
       "1     -0.657227  0.261475 -0.007530  0.341064  0.132202 -0.009216  \n",
       "2     -0.711426  0.196411 -0.025452  0.342285  0.091919  0.047363  \n",
       "3     -0.744629  0.279541 -0.048859  0.376221  0.030746 -0.011841  \n",
       "4     -0.687012  0.213623 -0.060669  0.288818  0.093445  0.036285  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "10264 -0.701172  0.222290 -0.032501  0.326172  0.062439  0.044952  \n",
       "10265 -0.699707  0.165649 -0.015297  0.312988  0.124878  0.049164  \n",
       "10266 -0.642578  0.189941 -0.032532  0.315674  0.144043  0.040405  \n",
       "10267 -0.728516  0.233398  0.000734  0.306885  0.088257 -0.003422  \n",
       "10268 -0.763672  0.226074 -0.055267  0.343994  0.086304  0.020523  \n",
       "\n",
       "[10269 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train=pd.read_excel('trainReputation.xlsx' )\n",
    "train=train.iloc[:,:-2].astype(float)\n",
    "train=train/200  #for scaling\n",
    "#train\n",
    "\n",
    "model_class='albert'  # bert or roberta or albert\n",
    "model_version='albert-large-v2' #bert-base-cased, roberta-base, roberta-large, albert-base-v2 OR albert-large-v2\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Saves/\"\n",
    "TF_Output=pd.read_csv( SavesDirectory+'trainOut.tsv', sep='\\t')\n",
    "\n",
    "train=pd.concat([train,TF_Output], axis=1)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10264</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10265</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10267</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10268</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10269 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4  5\n",
       "0      1  0  0  0  0  0\n",
       "1      0  0  0  1  0  0\n",
       "2      0  0  0  0  1  0\n",
       "3      0  0  0  0  1  0\n",
       "4      0  0  0  0  0  1\n",
       "...   .. .. .. .. .. ..\n",
       "10264  0  0  0  0  1  0\n",
       "10265  0  0  0  0  0  1\n",
       "10266  0  0  0  1  0  0\n",
       "10267  0  1  0  0  0  0\n",
       "10268  0  1  0  0  0  0\n",
       "\n",
       "[10269 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLables=pd.read_excel('trainReputation.xlsx' )\n",
    "TrainLables=TrainLables.iloc[:,-1] \n",
    "\n",
    "TrainLables=pd.get_dummies(TrainLables)\n",
    "TrainLables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0050,  0.0000,  0.0000,  ...,  0.3220,  0.1052,  0.0468],\n",
       "        [ 0.0950,  0.1600,  0.1700,  ...,  0.3411,  0.1322, -0.0092],\n",
       "        [ 0.0050,  0.0100,  0.0050,  ...,  0.3423,  0.0919,  0.0474],\n",
       "        ...,\n",
       "        [ 0.0350,  0.1150,  0.1400,  ...,  0.3157,  0.1440,  0.0404],\n",
       "        [ 0.3050,  0.5700,  0.3150,  ...,  0.3069,  0.0883, -0.0034],\n",
       "        [ 0.0000,  0.0050,  0.0000,  ...,  0.3440,  0.0863,  0.0205]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.tensor(train.values)\n",
    "del(train)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets=torch.tensor(TrainLables.astype(float).values)\n",
    "del(TrainLables)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: 12\n",
      "output size: 6\n"
     ]
    }
   ],
   "source": [
    " \n",
    "size= torch.tensor(input[0].size())\n",
    "InputSize=size.item()\n",
    "\n",
    "OutputSize=torch.tensor(targets[0].size()).item()\n",
    "\n",
    "print('input size:', InputSize)\n",
    "print('output size:', OutputSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "         \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(InputSize, 24)  # input size \n",
    "        self.fc2 = nn.Linear(24,12)\n",
    "        self.fc3 = nn.Linear(12, OutputSize)  #classifies 'outputsize' different classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x)) \n",
    "        x = torch.tanh(self.fc3(x)).double()\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "#now we use it\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we  setup the neural network parameters\n",
    "# pick an optimizer (Simple Gradient Descent)\n",
    "\n",
    "learning_rate = 10e-4\n",
    "criterion = nn.MSELoss()  #computes the loss Function\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# creating optimizer\n",
    "#optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1515, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 0\n",
      "Loss: tensor(0.1502, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1\n",
      "Loss: tensor(0.1490, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2\n",
      "Loss: tensor(0.1479, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 3\n",
      "Loss: tensor(0.1468, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 4\n",
      "Loss: tensor(0.1459, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 5\n",
      "Loss: tensor(0.1451, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 6\n",
      "Loss: tensor(0.1443, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 7\n",
      "Loss: tensor(0.1436, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 8\n",
      "Loss: tensor(0.1429, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 9\n",
      "Loss: tensor(0.1423, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 10\n",
      "Loss: tensor(0.1417, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 11\n",
      "Loss: tensor(0.1411, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 12\n",
      "Loss: tensor(0.1406, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 13\n",
      "Loss: tensor(0.1401, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 14\n",
      "Loss: tensor(0.1397, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 15\n",
      "Loss: tensor(0.1392, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 16\n",
      "Loss: tensor(0.1388, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 17\n",
      "Loss: tensor(0.1385, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 18\n",
      "Loss: tensor(0.1382, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 19\n",
      "Loss: tensor(0.1379, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 20\n",
      "Loss: tensor(0.1376, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 21\n",
      "Loss: tensor(0.1374, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 22\n",
      "Loss: tensor(0.1372, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 23\n",
      "Loss: tensor(0.1371, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 24\n",
      "Loss: tensor(0.1370, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 25\n",
      "Loss: tensor(0.1369, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 26\n",
      "Loss: tensor(0.1368, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 27\n",
      "Loss: tensor(0.1368, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 28\n",
      "Loss: tensor(0.1368, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 29\n",
      "Loss: tensor(0.1368, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 30\n",
      "Loss: tensor(0.1368, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 31\n",
      "Loss: tensor(0.1368, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 32\n",
      "Loss: tensor(0.1368, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 33\n",
      "Loss: tensor(0.1368, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 34\n",
      "Loss: tensor(0.1368, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 35\n",
      "Loss: tensor(0.1368, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 36\n",
      "Loss: tensor(0.1368, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 37\n",
      "Loss: tensor(0.1367, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 38\n",
      "Loss: tensor(0.1367, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 39\n",
      "Loss: tensor(0.1367, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 40\n",
      "Loss: tensor(0.1367, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 41\n",
      "Loss: tensor(0.1367, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 42\n",
      "Loss: tensor(0.1366, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 43\n",
      "Loss: tensor(0.1366, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 44\n",
      "Loss: tensor(0.1366, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 45\n",
      "Loss: tensor(0.1365, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 46\n",
      "Loss: tensor(0.1365, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 47\n",
      "Loss: tensor(0.1365, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 48\n",
      "Loss: tensor(0.1364, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 49\n",
      "Loss: tensor(0.1364, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 50\n",
      "Loss: tensor(0.1364, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 51\n",
      "Loss: tensor(0.1364, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 52\n",
      "Loss: tensor(0.1363, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 53\n",
      "Loss: tensor(0.1363, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 54\n",
      "Loss: tensor(0.1363, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 55\n",
      "Loss: tensor(0.1363, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 56\n",
      "Loss: tensor(0.1363, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 57\n",
      "Loss: tensor(0.1362, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 58\n",
      "Loss: tensor(0.1362, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 59\n",
      "Loss: tensor(0.1362, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 60\n",
      "Loss: tensor(0.1362, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 61\n",
      "Loss: tensor(0.1362, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 62\n",
      "Loss: tensor(0.1362, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 63\n",
      "Loss: tensor(0.1362, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 64\n",
      "Loss: tensor(0.1362, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 65\n",
      "Loss: tensor(0.1361, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 66\n",
      "Loss: tensor(0.1361, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 67\n",
      "Loss: tensor(0.1361, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 68\n",
      "Loss: tensor(0.1361, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 69\n",
      "Loss: tensor(0.1361, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 70\n",
      "Loss: tensor(0.1361, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 71\n",
      "Loss: tensor(0.1361, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 72\n",
      "Loss: tensor(0.1360, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 73\n",
      "Loss: tensor(0.1360, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 74\n",
      "Loss: tensor(0.1360, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 75\n",
      "Loss: tensor(0.1360, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 76\n",
      "Loss: tensor(0.1360, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 77\n",
      "Loss: tensor(0.1360, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 78\n",
      "Loss: tensor(0.1360, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 79\n",
      "Loss: tensor(0.1359, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 80\n",
      "Loss: tensor(0.1359, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 81\n",
      "Loss: tensor(0.1359, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 82\n",
      "Loss: tensor(0.1359, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 83\n",
      "Loss: tensor(0.1359, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 84\n",
      "Loss: tensor(0.1359, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 85\n",
      "Loss: tensor(0.1359, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 86\n",
      "Loss: tensor(0.1359, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 87\n",
      "Loss: tensor(0.1359, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 88\n",
      "Loss: tensor(0.1358, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 89\n",
      "Loss: tensor(0.1358, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 90\n",
      "Loss: tensor(0.1358, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 91\n",
      "Loss: tensor(0.1358, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 92\n",
      "Loss: tensor(0.1358, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 93\n",
      "Loss: tensor(0.1358, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 94\n",
      "Loss: tensor(0.1358, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 95\n",
      "Loss: tensor(0.1358, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 96\n",
      "Loss: tensor(0.1358, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 97\n",
      "Loss: tensor(0.1358, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 98\n",
      "Loss: tensor(0.1358, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 100\n",
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 101\n",
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 102\n",
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 103\n",
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 104\n",
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 105\n",
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 106\n",
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 107\n",
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 108\n",
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 109\n",
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 110\n",
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 111\n",
      "Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 112\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 113\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 114\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 115\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 116\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 117\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 118\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 119\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 120\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 121\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 122\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 123\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 124\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 125\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 126\n",
      "Loss: tensor(0.1356, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 127\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 128\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 129\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 130\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 131\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 132\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 133\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 134\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 135\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 136\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 137\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 138\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 139\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 140\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 141\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 142\n",
      "Loss: tensor(0.1355, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 143\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 144\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 145\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 146\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 147\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 148\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 149\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 150\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 151\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 152\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 153\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 154\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 155\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 156\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 157\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 158\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 159\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 160\n",
      "Loss: tensor(0.1354, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 161\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 162\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 163\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 164\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 165\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 166\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 167\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 168\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 169\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 170\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 171\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 172\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 173\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 174\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 175\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 176\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 177\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 178\n",
      "Loss: tensor(0.1353, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 179\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 180\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 181\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 182\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 183\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 184\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 185\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 186\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 187\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 188\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 189\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 190\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 191\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 192\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 193\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 194\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 195\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 196\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 197\n",
      "Loss: tensor(0.1352, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 198\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 199\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 200\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 201\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 202\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 203\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 204\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 206\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 207\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 208\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 209\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 210\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 211\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 212\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 213\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 214\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 215\n",
      "Loss: tensor(0.1351, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 216\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 217\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 218\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 219\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 220\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 221\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 222\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 223\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 224\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 225\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 226\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 227\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 228\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 229\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 230\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 231\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 232\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 233\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 234\n",
      "Loss: tensor(0.1350, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 235\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 236\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 237\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 238\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 239\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 240\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 241\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 242\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 243\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 244\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 245\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 246\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 247\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 248\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 249\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 250\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 251\n",
      "Loss: tensor(0.1349, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 252\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 253\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 254\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 255\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 256\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 257\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 258\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 259\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 260\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 261\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 262\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 263\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 264\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 265\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 266\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 267\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 268\n",
      "Loss: tensor(0.1348, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 269\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 270\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 271\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 272\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 273\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 274\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 275\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 276\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 277\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 278\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 279\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 280\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 281\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 282\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 283\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 284\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 285\n",
      "Loss: tensor(0.1347, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 286\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 287\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 288\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 289\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 290\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 291\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 292\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 293\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 294\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 295\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 296\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 297\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 298\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 299\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 300\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 301\n",
      "Loss: tensor(0.1346, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 302\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 303\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 304\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 305\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 306\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 307\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 308\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 309\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 311\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 312\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 313\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 314\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 315\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 316\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 317\n",
      "Loss: tensor(0.1345, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 318\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 319\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 320\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 321\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 322\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 323\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 324\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 325\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 326\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 327\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 328\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 329\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 330\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 331\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 332\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 333\n",
      "Loss: tensor(0.1344, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 334\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 335\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 336\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 337\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 338\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 339\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 340\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 341\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 342\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 343\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 344\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 345\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 346\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 347\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 348\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 349\n",
      "Loss: tensor(0.1343, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 350\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 351\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 352\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 353\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 354\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 355\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 356\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 357\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 358\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 359\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 360\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 361\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 362\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 363\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 364\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 365\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 366\n",
      "Loss: tensor(0.1342, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 367\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 368\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 369\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 370\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 371\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 372\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 373\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 374\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 375\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 376\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 377\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 378\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 379\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 380\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 381\n",
      "Loss: tensor(0.1341, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 382\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 383\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 384\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 385\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 386\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 387\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 388\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 389\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 390\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 391\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 392\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 393\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 394\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 395\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 396\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 397\n",
      "Loss: tensor(0.1340, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 398\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 399\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 400\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 401\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 402\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 403\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 404\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 405\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 406\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 407\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 408\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 409\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 410\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 411\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 412\n",
      "Loss: tensor(0.1339, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 413\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 414\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 416\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 417\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 418\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 419\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 420\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 421\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 422\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 423\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 424\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 425\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 426\n",
      "Loss: tensor(0.1338, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 427\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 428\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 429\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 430\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 431\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 432\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 433\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 434\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 435\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 436\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 437\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 438\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 439\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 440\n",
      "Loss: tensor(0.1337, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 441\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 442\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 443\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 444\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 445\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 446\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 447\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 448\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 449\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 450\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 451\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 452\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 453\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 454\n",
      "Loss: tensor(0.1336, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 455\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 456\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 457\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 458\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 459\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 460\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 461\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 462\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 463\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 464\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 465\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 466\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 467\n",
      "Loss: tensor(0.1335, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 468\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 469\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 470\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 471\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 472\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 473\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 474\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 475\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 476\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 477\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 478\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 479\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 480\n",
      "Loss: tensor(0.1334, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 481\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 482\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 483\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 484\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 485\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 486\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 487\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 488\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 489\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 490\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 491\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 492\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 493\n",
      "Loss: tensor(0.1333, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 494\n",
      "Loss: tensor(0.1332, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 495\n",
      "Loss: tensor(0.1332, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 496\n",
      "Loss: tensor(0.1332, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 497\n",
      "Loss: tensor(0.1332, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 498\n",
      "Loss: tensor(0.1332, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 499\n",
      "Loss: tensor(0.1332, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 500\n",
      "Loss: tensor(0.1332, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 501\n",
      "Loss: tensor(0.1332, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 502\n",
      "Loss: tensor(0.1332, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 503\n",
      "Loss: tensor(0.1332, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 504\n",
      "Loss: tensor(0.1332, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 505\n",
      "Loss: tensor(0.1332, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 506\n",
      "Loss: tensor(0.1331, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 507\n",
      "Loss: tensor(0.1331, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 508\n",
      "Loss: tensor(0.1331, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 509\n",
      "Loss: tensor(0.1331, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 510\n",
      "Loss: tensor(0.1331, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 511\n",
      "Loss: tensor(0.1331, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 512\n",
      "Loss: tensor(0.1331, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 513\n",
      "Loss: tensor(0.1331, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 514\n",
      "Loss: tensor(0.1331, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 515\n",
      "Loss: tensor(0.1331, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 516\n",
      "Loss: tensor(0.1331, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 517\n",
      "Loss: tensor(0.1331, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 518\n",
      "Loss: tensor(0.1330, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 519\n",
      "Loss: tensor(0.1330, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 520\n",
      "Loss: tensor(0.1330, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 521\n",
      "Loss: tensor(0.1330, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 522\n",
      "Loss: tensor(0.1330, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 523\n",
      "Loss: tensor(0.1330, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 524\n",
      "Loss: tensor(0.1330, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 525\n",
      "Loss: tensor(0.1330, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 526\n",
      "Loss: tensor(0.1330, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 527\n",
      "Loss: tensor(0.1330, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 528\n",
      "Loss: tensor(0.1330, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 529\n",
      "Loss: tensor(0.1330, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 530\n",
      "Loss: tensor(0.1329, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 531\n",
      "Loss: tensor(0.1329, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 532\n",
      "Loss: tensor(0.1329, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 533\n",
      "Loss: tensor(0.1329, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1329, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 535\n",
      "Loss: tensor(0.1329, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 536\n",
      "Loss: tensor(0.1329, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 537\n",
      "Loss: tensor(0.1329, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 538\n",
      "Loss: tensor(0.1329, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 539\n",
      "Loss: tensor(0.1329, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 540\n",
      "Loss: tensor(0.1329, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 541\n",
      "Loss: tensor(0.1329, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 542\n",
      "Loss: tensor(0.1328, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 543\n",
      "Loss: tensor(0.1328, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 544\n",
      "Loss: tensor(0.1328, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 545\n",
      "Loss: tensor(0.1328, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 546\n",
      "Loss: tensor(0.1328, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 547\n",
      "Loss: tensor(0.1328, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 548\n",
      "Loss: tensor(0.1328, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 549\n",
      "Loss: tensor(0.1328, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 550\n",
      "Loss: tensor(0.1328, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 551\n",
      "Loss: tensor(0.1328, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 552\n",
      "Loss: tensor(0.1328, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 553\n",
      "Loss: tensor(0.1328, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 554\n",
      "Loss: tensor(0.1327, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 555\n",
      "Loss: tensor(0.1327, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 556\n",
      "Loss: tensor(0.1327, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 557\n",
      "Loss: tensor(0.1327, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 558\n",
      "Loss: tensor(0.1327, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 559\n",
      "Loss: tensor(0.1327, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 560\n",
      "Loss: tensor(0.1327, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 561\n",
      "Loss: tensor(0.1327, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 562\n",
      "Loss: tensor(0.1327, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 563\n",
      "Loss: tensor(0.1327, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 564\n",
      "Loss: tensor(0.1327, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 565\n",
      "Loss: tensor(0.1327, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 566\n",
      "Loss: tensor(0.1326, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 567\n",
      "Loss: tensor(0.1326, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 568\n",
      "Loss: tensor(0.1326, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 569\n",
      "Loss: tensor(0.1326, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 570\n",
      "Loss: tensor(0.1326, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 571\n",
      "Loss: tensor(0.1326, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 572\n",
      "Loss: tensor(0.1326, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 573\n",
      "Loss: tensor(0.1326, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 574\n",
      "Loss: tensor(0.1326, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 575\n",
      "Loss: tensor(0.1326, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 576\n",
      "Loss: tensor(0.1326, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 577\n",
      "Loss: tensor(0.1326, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 578\n",
      "Loss: tensor(0.1325, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 579\n",
      "Loss: tensor(0.1325, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 580\n",
      "Loss: tensor(0.1325, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 581\n",
      "Loss: tensor(0.1325, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 582\n",
      "Loss: tensor(0.1325, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 583\n",
      "Loss: tensor(0.1325, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 584\n",
      "Loss: tensor(0.1325, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 585\n",
      "Loss: tensor(0.1325, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 586\n",
      "Loss: tensor(0.1325, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 587\n",
      "Loss: tensor(0.1325, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 588\n",
      "Loss: tensor(0.1325, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 589\n",
      "Loss: tensor(0.1325, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 590\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 591\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 592\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 593\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 594\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 595\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 596\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 597\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 598\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 599\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 600\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 601\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 602\n",
      "Loss: tensor(0.1324, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 603\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 604\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 605\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 606\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 607\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 608\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 609\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 610\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 611\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 612\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 613\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 614\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 615\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 616\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 617\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 618\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 619\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 620\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 621\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 622\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 623\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 624\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 625\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 626\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 627\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 628\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 629\n",
      "Loss: tensor(0.1322, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 630\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 631\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 632\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 633\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 634\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 635\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 636\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 637\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 638\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 639\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 640\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 642\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 643\n",
      "Loss: tensor(0.1321, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 644\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 645\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 646\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 647\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 648\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 649\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 650\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 651\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 652\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 653\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 654\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 655\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 656\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 657\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 658\n",
      "Loss: tensor(0.1320, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 659\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 660\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 661\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 662\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 663\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 664\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 665\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 666\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 667\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 668\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 669\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 670\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 671\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 672\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 673\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 674\n",
      "Loss: tensor(0.1319, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 675\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 676\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 677\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 678\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 679\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 680\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 681\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 682\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 683\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 684\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 685\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 686\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 687\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 688\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 689\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 690\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 691\n",
      "Loss: tensor(0.1318, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 692\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 693\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 694\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 695\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 696\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 697\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 698\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 699\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 700\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 701\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 702\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 703\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 704\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 705\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 706\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 707\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 708\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 709\n",
      "Loss: tensor(0.1317, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 710\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 711\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 712\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 713\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 714\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 715\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 716\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 717\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 718\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 719\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 720\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 721\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 722\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 723\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 724\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 725\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 726\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 727\n",
      "Loss: tensor(0.1316, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 728\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 729\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 730\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 731\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 732\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 733\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 734\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 735\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 736\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 737\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 738\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 739\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 740\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 741\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 742\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 743\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 744\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 745\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 746\n",
      "Loss: tensor(0.1315, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 747\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 748\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 749\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 750\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 751\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 752\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 753\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 754\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 755\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 757\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 758\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 759\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 760\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 761\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 762\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 763\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 764\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 765\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 766\n",
      "Loss: tensor(0.1314, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 767\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 768\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 769\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 770\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 771\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 772\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 773\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 774\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 775\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 776\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 777\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 778\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 779\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 780\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 781\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 782\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 783\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 784\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 785\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 786\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 787\n",
      "Loss: tensor(0.1313, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 788\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 789\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 790\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 791\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 792\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 793\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 794\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 795\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 796\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 797\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 798\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 799\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 800\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 801\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 802\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 803\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 804\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 805\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 806\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 807\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 808\n",
      "Loss: tensor(0.1312, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 809\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 810\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 811\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 812\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 813\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 814\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 815\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 816\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 817\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 818\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 819\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 820\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 821\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 822\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 823\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 824\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 825\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 826\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 827\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 828\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 829\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 830\n",
      "Loss: tensor(0.1311, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 831\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 832\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 833\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 834\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 835\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 836\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 837\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 838\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 839\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 840\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 841\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 842\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 843\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 844\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 845\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 846\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 847\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 848\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 849\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 850\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 851\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 852\n",
      "Loss: tensor(0.1310, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 853\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 854\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 855\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 856\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 857\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 858\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 859\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 860\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 861\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 862\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 863\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 864\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 865\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 866\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 867\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 868\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 869\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 870\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 871\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 872\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 873\n",
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1309, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 875\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 876\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 877\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 878\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 879\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 880\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 881\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 882\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 883\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 884\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 885\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 886\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 887\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 888\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 889\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 890\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 891\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 892\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 893\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 894\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 895\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 896\n",
      "Loss: tensor(0.1308, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 897\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 898\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 899\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 900\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 901\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 902\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 903\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 904\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 905\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 906\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 907\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 908\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 909\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 910\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 911\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 912\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 913\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 914\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 915\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 916\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 917\n",
      "Loss: tensor(0.1307, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 918\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 919\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 920\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 921\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 922\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 923\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 924\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 925\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 926\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 927\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 928\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 929\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 930\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 931\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 932\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 933\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 934\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 935\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 936\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 937\n",
      "Loss: tensor(0.1306, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 938\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 939\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 940\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 941\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 942\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 943\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 944\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 945\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 946\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 947\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 948\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 949\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 950\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 951\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 952\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 953\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 954\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 955\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 956\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 957\n",
      "Loss: tensor(0.1305, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 958\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 959\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 960\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 961\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 962\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 963\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 964\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 965\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 966\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 967\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 968\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 969\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 970\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 971\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 972\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 973\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 974\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 975\n",
      "Loss: tensor(0.1304, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 976\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 977\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 978\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 979\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 980\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 981\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 982\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 983\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 984\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 985\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 986\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 987\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 988\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 989\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 990\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 991\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 992\n",
      "Loss: tensor(0.1303, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 994\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 995\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 996\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 997\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 998\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 999\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1000\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1001\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1002\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1003\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1004\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1005\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1006\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1007\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1008\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1009\n",
      "Loss: tensor(0.1302, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1010\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1011\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1012\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1013\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1014\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1015\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1016\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1017\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1018\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1019\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1020\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1021\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1022\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1023\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1024\n",
      "Loss: tensor(0.1301, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1025\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1026\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1027\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1028\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1029\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1030\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1031\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1032\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1033\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1034\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1035\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1036\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1037\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1038\n",
      "Loss: tensor(0.1300, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1039\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1040\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1041\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1042\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1043\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1044\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1045\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1046\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1047\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1048\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1049\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1050\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1051\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1052\n",
      "Loss: tensor(0.1299, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1053\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1054\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1055\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1056\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1057\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1058\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1059\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1060\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1061\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1062\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1063\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1064\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1065\n",
      "Loss: tensor(0.1298, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1066\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1067\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1068\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1069\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1070\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1071\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1072\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1073\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1074\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1075\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1076\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1077\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1078\n",
      "Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1079\n",
      "Loss: tensor(0.1296, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1080\n",
      "Loss: tensor(0.1296, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1081\n",
      "Loss: tensor(0.1296, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1082\n",
      "Loss: tensor(0.1296, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1083\n",
      "Loss: tensor(0.1296, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1084\n",
      "Loss: tensor(0.1296, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1085\n",
      "Loss: tensor(0.1296, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1086\n",
      "Loss: tensor(0.1296, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1087\n",
      "Loss: tensor(0.1296, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1088\n",
      "Loss: tensor(0.1296, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1089\n",
      "Loss: tensor(0.1296, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1090\n",
      "Loss: tensor(0.1296, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1091\n",
      "Loss: tensor(0.1295, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1092\n",
      "Loss: tensor(0.1295, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1093\n",
      "Loss: tensor(0.1295, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1094\n",
      "Loss: tensor(0.1295, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1095\n",
      "Loss: tensor(0.1295, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1096\n",
      "Loss: tensor(0.1295, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1097\n",
      "Loss: tensor(0.1295, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1098\n",
      "Loss: tensor(0.1295, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1099\n",
      "Loss: tensor(0.1295, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1100\n",
      "Loss: tensor(0.1295, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1101\n",
      "Loss: tensor(0.1295, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1102\n",
      "Loss: tensor(0.1294, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1103\n",
      "Loss: tensor(0.1294, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1104\n",
      "Loss: tensor(0.1294, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1105\n",
      "Loss: tensor(0.1294, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1106\n",
      "Loss: tensor(0.1294, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1107\n",
      "Loss: tensor(0.1294, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1108\n",
      "Loss: tensor(0.1294, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1109\n",
      "Loss: tensor(0.1294, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1294, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1111\n",
      "Loss: tensor(0.1294, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1112\n",
      "Loss: tensor(0.1294, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1113\n",
      "Loss: tensor(0.1293, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1114\n",
      "Loss: tensor(0.1293, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1115\n",
      "Loss: tensor(0.1293, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1116\n",
      "Loss: tensor(0.1293, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1117\n",
      "Loss: tensor(0.1293, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1118\n",
      "Loss: tensor(0.1293, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1119\n",
      "Loss: tensor(0.1293, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1120\n",
      "Loss: tensor(0.1293, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1121\n",
      "Loss: tensor(0.1293, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1122\n",
      "Loss: tensor(0.1293, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1123\n",
      "Loss: tensor(0.1292, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1124\n",
      "Loss: tensor(0.1292, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1125\n",
      "Loss: tensor(0.1292, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1126\n",
      "Loss: tensor(0.1292, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1127\n",
      "Loss: tensor(0.1292, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1128\n",
      "Loss: tensor(0.1292, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1129\n",
      "Loss: tensor(0.1292, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1130\n",
      "Loss: tensor(0.1292, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1131\n",
      "Loss: tensor(0.1292, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1132\n",
      "Loss: tensor(0.1292, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1133\n",
      "Loss: tensor(0.1291, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1134\n",
      "Loss: tensor(0.1291, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1135\n",
      "Loss: tensor(0.1291, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1136\n",
      "Loss: tensor(0.1291, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1137\n",
      "Loss: tensor(0.1291, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1138\n",
      "Loss: tensor(0.1291, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1139\n",
      "Loss: tensor(0.1291, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1140\n",
      "Loss: tensor(0.1291, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1141\n",
      "Loss: tensor(0.1291, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1142\n",
      "Loss: tensor(0.1291, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1143\n",
      "Loss: tensor(0.1290, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1144\n",
      "Loss: tensor(0.1290, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1145\n",
      "Loss: tensor(0.1290, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1146\n",
      "Loss: tensor(0.1290, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1147\n",
      "Loss: tensor(0.1290, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1148\n",
      "Loss: tensor(0.1290, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1149\n",
      "Loss: tensor(0.1290, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1150\n",
      "Loss: tensor(0.1290, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1151\n",
      "Loss: tensor(0.1290, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1152\n",
      "Loss: tensor(0.1289, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1153\n",
      "Loss: tensor(0.1289, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1154\n",
      "Loss: tensor(0.1289, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1155\n",
      "Loss: tensor(0.1289, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1156\n",
      "Loss: tensor(0.1289, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1157\n",
      "Loss: tensor(0.1289, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1158\n",
      "Loss: tensor(0.1289, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1159\n",
      "Loss: tensor(0.1289, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1160\n",
      "Loss: tensor(0.1289, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1161\n",
      "Loss: tensor(0.1288, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1162\n",
      "Loss: tensor(0.1288, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1163\n",
      "Loss: tensor(0.1288, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1164\n",
      "Loss: tensor(0.1288, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1165\n",
      "Loss: tensor(0.1288, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1166\n",
      "Loss: tensor(0.1288, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1167\n",
      "Loss: tensor(0.1288, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1168\n",
      "Loss: tensor(0.1288, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1169\n",
      "Loss: tensor(0.1287, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1170\n",
      "Loss: tensor(0.1287, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1171\n",
      "Loss: tensor(0.1287, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1172\n",
      "Loss: tensor(0.1287, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1173\n",
      "Loss: tensor(0.1287, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1174\n",
      "Loss: tensor(0.1287, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1175\n",
      "Loss: tensor(0.1287, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1176\n",
      "Loss: tensor(0.1287, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1177\n",
      "Loss: tensor(0.1287, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1178\n",
      "Loss: tensor(0.1286, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1179\n",
      "Loss: tensor(0.1286, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1180\n",
      "Loss: tensor(0.1286, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1181\n",
      "Loss: tensor(0.1286, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1182\n",
      "Loss: tensor(0.1286, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1183\n",
      "Loss: tensor(0.1286, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1184\n",
      "Loss: tensor(0.1286, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1185\n",
      "Loss: tensor(0.1286, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1186\n",
      "Loss: tensor(0.1285, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1187\n",
      "Loss: tensor(0.1285, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1188\n",
      "Loss: tensor(0.1285, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1189\n",
      "Loss: tensor(0.1285, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1190\n",
      "Loss: tensor(0.1285, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1191\n",
      "Loss: tensor(0.1285, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1192\n",
      "Loss: tensor(0.1285, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1193\n",
      "Loss: tensor(0.1285, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1194\n",
      "Loss: tensor(0.1284, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1195\n",
      "Loss: tensor(0.1284, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1196\n",
      "Loss: tensor(0.1284, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1197\n",
      "Loss: tensor(0.1284, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1198\n",
      "Loss: tensor(0.1284, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1199\n",
      "Loss: tensor(0.1284, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1200\n",
      "Loss: tensor(0.1284, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1201\n",
      "Loss: tensor(0.1284, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1202\n",
      "Loss: tensor(0.1283, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1203\n",
      "Loss: tensor(0.1283, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1204\n",
      "Loss: tensor(0.1283, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1205\n",
      "Loss: tensor(0.1283, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1206\n",
      "Loss: tensor(0.1283, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1207\n",
      "Loss: tensor(0.1283, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1208\n",
      "Loss: tensor(0.1283, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1209\n",
      "Loss: tensor(0.1282, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1210\n",
      "Loss: tensor(0.1282, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1211\n",
      "Loss: tensor(0.1282, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1212\n",
      "Loss: tensor(0.1282, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1213\n",
      "Loss: tensor(0.1282, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1214\n",
      "Loss: tensor(0.1282, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1215\n",
      "Loss: tensor(0.1282, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1216\n",
      "Loss: tensor(0.1281, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1217\n",
      "Loss: tensor(0.1281, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1218\n",
      "Loss: tensor(0.1281, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1219\n",
      "Loss: tensor(0.1281, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1220\n",
      "Loss: tensor(0.1281, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1221\n",
      "Loss: tensor(0.1281, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1222\n",
      "Loss: tensor(0.1281, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1223\n",
      "Loss: tensor(0.1281, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1224\n",
      "Loss: tensor(0.1280, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1225\n",
      "Loss: tensor(0.1280, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1226\n",
      "Loss: tensor(0.1280, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1227\n",
      "Loss: tensor(0.1280, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1228\n",
      "Loss: tensor(0.1280, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1229\n",
      "Loss: tensor(0.1280, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1230\n",
      "Loss: tensor(0.1279, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1231\n",
      "Loss: tensor(0.1279, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1232\n",
      "Loss: tensor(0.1279, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1279, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1234\n",
      "Loss: tensor(0.1279, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1235\n",
      "Loss: tensor(0.1279, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1236\n",
      "Loss: tensor(0.1279, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1237\n",
      "Loss: tensor(0.1278, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1238\n",
      "Loss: tensor(0.1278, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1239\n",
      "Loss: tensor(0.1278, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1240\n",
      "Loss: tensor(0.1278, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1241\n",
      "Loss: tensor(0.1278, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1242\n",
      "Loss: tensor(0.1278, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1243\n",
      "Loss: tensor(0.1278, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1244\n",
      "Loss: tensor(0.1277, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1245\n",
      "Loss: tensor(0.1277, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1246\n",
      "Loss: tensor(0.1277, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1247\n",
      "Loss: tensor(0.1277, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1248\n",
      "Loss: tensor(0.1277, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1249\n",
      "Loss: tensor(0.1277, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1250\n",
      "Loss: tensor(0.1276, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1251\n",
      "Loss: tensor(0.1276, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1252\n",
      "Loss: tensor(0.1276, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1253\n",
      "Loss: tensor(0.1276, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1254\n",
      "Loss: tensor(0.1276, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1255\n",
      "Loss: tensor(0.1276, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1256\n",
      "Loss: tensor(0.1276, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1257\n",
      "Loss: tensor(0.1275, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1258\n",
      "Loss: tensor(0.1275, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1259\n",
      "Loss: tensor(0.1275, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1260\n",
      "Loss: tensor(0.1275, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1261\n",
      "Loss: tensor(0.1275, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1262\n",
      "Loss: tensor(0.1275, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1263\n",
      "Loss: tensor(0.1274, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1264\n",
      "Loss: tensor(0.1274, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1265\n",
      "Loss: tensor(0.1274, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1266\n",
      "Loss: tensor(0.1274, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1267\n",
      "Loss: tensor(0.1274, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1268\n",
      "Loss: tensor(0.1274, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1269\n",
      "Loss: tensor(0.1274, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1270\n",
      "Loss: tensor(0.1273, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1271\n",
      "Loss: tensor(0.1273, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1272\n",
      "Loss: tensor(0.1273, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1273\n",
      "Loss: tensor(0.1273, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1274\n",
      "Loss: tensor(0.1273, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1275\n",
      "Loss: tensor(0.1273, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1276\n",
      "Loss: tensor(0.1272, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1277\n",
      "Loss: tensor(0.1272, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1278\n",
      "Loss: tensor(0.1272, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1279\n",
      "Loss: tensor(0.1272, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1280\n",
      "Loss: tensor(0.1272, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1281\n",
      "Loss: tensor(0.1272, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1282\n",
      "Loss: tensor(0.1272, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1283\n",
      "Loss: tensor(0.1271, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1284\n",
      "Loss: tensor(0.1271, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1285\n",
      "Loss: tensor(0.1271, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1286\n",
      "Loss: tensor(0.1271, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1287\n",
      "Loss: tensor(0.1271, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1288\n",
      "Loss: tensor(0.1271, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1289\n",
      "Loss: tensor(0.1270, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1290\n",
      "Loss: tensor(0.1270, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1291\n",
      "Loss: tensor(0.1270, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1292\n",
      "Loss: tensor(0.1270, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1293\n",
      "Loss: tensor(0.1270, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1294\n",
      "Loss: tensor(0.1270, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1295\n",
      "Loss: tensor(0.1269, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1296\n",
      "Loss: tensor(0.1269, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1297\n",
      "Loss: tensor(0.1269, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1298\n",
      "Loss: tensor(0.1269, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1299\n",
      "Loss: tensor(0.1269, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1300\n",
      "Loss: tensor(0.1269, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1301\n",
      "Loss: tensor(0.1268, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1302\n",
      "Loss: tensor(0.1268, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1303\n",
      "Loss: tensor(0.1268, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1304\n",
      "Loss: tensor(0.1268, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1305\n",
      "Loss: tensor(0.1268, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1306\n",
      "Loss: tensor(0.1268, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1307\n",
      "Loss: tensor(0.1268, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1308\n",
      "Loss: tensor(0.1267, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1309\n",
      "Loss: tensor(0.1267, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1310\n",
      "Loss: tensor(0.1267, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1311\n",
      "Loss: tensor(0.1267, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1312\n",
      "Loss: tensor(0.1267, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1313\n",
      "Loss: tensor(0.1267, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1314\n",
      "Loss: tensor(0.1266, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1315\n",
      "Loss: tensor(0.1266, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1316\n",
      "Loss: tensor(0.1266, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1317\n",
      "Loss: tensor(0.1266, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1318\n",
      "Loss: tensor(0.1266, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1319\n",
      "Loss: tensor(0.1266, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1320\n",
      "Loss: tensor(0.1265, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1321\n",
      "Loss: tensor(0.1265, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1322\n",
      "Loss: tensor(0.1265, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1323\n",
      "Loss: tensor(0.1265, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1324\n",
      "Loss: tensor(0.1265, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1325\n",
      "Loss: tensor(0.1265, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1326\n",
      "Loss: tensor(0.1264, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1327\n",
      "Loss: tensor(0.1264, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1328\n",
      "Loss: tensor(0.1264, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1329\n",
      "Loss: tensor(0.1264, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1330\n",
      "Loss: tensor(0.1264, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1331\n",
      "Loss: tensor(0.1264, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1332\n",
      "Loss: tensor(0.1263, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1333\n",
      "Loss: tensor(0.1263, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1334\n",
      "Loss: tensor(0.1263, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1335\n",
      "Loss: tensor(0.1263, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1336\n",
      "Loss: tensor(0.1263, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1337\n",
      "Loss: tensor(0.1263, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1338\n",
      "Loss: tensor(0.1262, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1339\n",
      "Loss: tensor(0.1262, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1340\n",
      "Loss: tensor(0.1262, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1341\n",
      "Loss: tensor(0.1262, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1342\n",
      "Loss: tensor(0.1262, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1343\n",
      "Loss: tensor(0.1262, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1344\n",
      "Loss: tensor(0.1261, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1345\n",
      "Loss: tensor(0.1261, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1346\n",
      "Loss: tensor(0.1261, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1347\n",
      "Loss: tensor(0.1261, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1348\n",
      "Loss: tensor(0.1261, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1349\n",
      "Loss: tensor(0.1261, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1350\n",
      "Loss: tensor(0.1260, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1351\n",
      "Loss: tensor(0.1260, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1352\n",
      "Loss: tensor(0.1260, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1260, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1354\n",
      "Loss: tensor(0.1260, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1355\n",
      "Loss: tensor(0.1260, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1356\n",
      "Loss: tensor(0.1259, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1357\n",
      "Loss: tensor(0.1259, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1358\n",
      "Loss: tensor(0.1259, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1359\n",
      "Loss: tensor(0.1259, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1360\n",
      "Loss: tensor(0.1259, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1361\n",
      "Loss: tensor(0.1259, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1362\n",
      "Loss: tensor(0.1258, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1363\n",
      "Loss: tensor(0.1258, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1364\n",
      "Loss: tensor(0.1258, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1365\n",
      "Loss: tensor(0.1258, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1366\n",
      "Loss: tensor(0.1258, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1367\n",
      "Loss: tensor(0.1258, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1368\n",
      "Loss: tensor(0.1257, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1369\n",
      "Loss: tensor(0.1257, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1370\n",
      "Loss: tensor(0.1257, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1371\n",
      "Loss: tensor(0.1257, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1372\n",
      "Loss: tensor(0.1257, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1373\n",
      "Loss: tensor(0.1257, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1374\n",
      "Loss: tensor(0.1256, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1375\n",
      "Loss: tensor(0.1256, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1376\n",
      "Loss: tensor(0.1256, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1377\n",
      "Loss: tensor(0.1256, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1378\n",
      "Loss: tensor(0.1256, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1379\n",
      "Loss: tensor(0.1255, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1380\n",
      "Loss: tensor(0.1255, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1381\n",
      "Loss: tensor(0.1255, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1382\n",
      "Loss: tensor(0.1255, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1383\n",
      "Loss: tensor(0.1255, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1384\n",
      "Loss: tensor(0.1255, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1385\n",
      "Loss: tensor(0.1254, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1386\n",
      "Loss: tensor(0.1254, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1387\n",
      "Loss: tensor(0.1254, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1388\n",
      "Loss: tensor(0.1254, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1389\n",
      "Loss: tensor(0.1254, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1390\n",
      "Loss: tensor(0.1254, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1391\n",
      "Loss: tensor(0.1253, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1392\n",
      "Loss: tensor(0.1253, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1393\n",
      "Loss: tensor(0.1253, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1394\n",
      "Loss: tensor(0.1253, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1395\n",
      "Loss: tensor(0.1253, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1396\n",
      "Loss: tensor(0.1252, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1397\n",
      "Loss: tensor(0.1252, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1398\n",
      "Loss: tensor(0.1252, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1399\n",
      "Loss: tensor(0.1252, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1400\n",
      "Loss: tensor(0.1252, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1401\n",
      "Loss: tensor(0.1252, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1402\n",
      "Loss: tensor(0.1251, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1403\n",
      "Loss: tensor(0.1251, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1404\n",
      "Loss: tensor(0.1251, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1405\n",
      "Loss: tensor(0.1251, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1406\n",
      "Loss: tensor(0.1251, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1407\n",
      "Loss: tensor(0.1251, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1408\n",
      "Loss: tensor(0.1250, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1409\n",
      "Loss: tensor(0.1250, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1410\n",
      "Loss: tensor(0.1250, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1411\n",
      "Loss: tensor(0.1250, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1412\n",
      "Loss: tensor(0.1250, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1413\n",
      "Loss: tensor(0.1249, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1414\n",
      "Loss: tensor(0.1249, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1415\n",
      "Loss: tensor(0.1249, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1416\n",
      "Loss: tensor(0.1249, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1417\n",
      "Loss: tensor(0.1249, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1418\n",
      "Loss: tensor(0.1249, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1419\n",
      "Loss: tensor(0.1248, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1420\n",
      "Loss: tensor(0.1248, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1421\n",
      "Loss: tensor(0.1248, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1422\n",
      "Loss: tensor(0.1248, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1423\n",
      "Loss: tensor(0.1248, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1424\n",
      "Loss: tensor(0.1248, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1425\n",
      "Loss: tensor(0.1247, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1426\n",
      "Loss: tensor(0.1247, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1427\n",
      "Loss: tensor(0.1247, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1428\n",
      "Loss: tensor(0.1247, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1429\n",
      "Loss: tensor(0.1247, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1430\n",
      "Loss: tensor(0.1246, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1431\n",
      "Loss: tensor(0.1246, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1432\n",
      "Loss: tensor(0.1246, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1433\n",
      "Loss: tensor(0.1246, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1434\n",
      "Loss: tensor(0.1246, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1435\n",
      "Loss: tensor(0.1245, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1436\n",
      "Loss: tensor(0.1245, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1437\n",
      "Loss: tensor(0.1245, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1438\n",
      "Loss: tensor(0.1245, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1439\n",
      "Loss: tensor(0.1245, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1440\n",
      "Loss: tensor(0.1244, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1441\n",
      "Loss: tensor(0.1244, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1442\n",
      "Loss: tensor(0.1244, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1443\n",
      "Loss: tensor(0.1244, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1444\n",
      "Loss: tensor(0.1244, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1445\n",
      "Loss: tensor(0.1243, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1446\n",
      "Loss: tensor(0.1243, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1447\n",
      "Loss: tensor(0.1243, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1448\n",
      "Loss: tensor(0.1243, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1449\n",
      "Loss: tensor(0.1243, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1450\n",
      "Loss: tensor(0.1243, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1451\n",
      "Loss: tensor(0.1242, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1452\n",
      "Loss: tensor(0.1242, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1453\n",
      "Loss: tensor(0.1242, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1454\n",
      "Loss: tensor(0.1242, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1455\n",
      "Loss: tensor(0.1242, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1456\n",
      "Loss: tensor(0.1241, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1457\n",
      "Loss: tensor(0.1241, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1458\n",
      "Loss: tensor(0.1241, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1459\n",
      "Loss: tensor(0.1241, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1460\n",
      "Loss: tensor(0.1241, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1461\n",
      "Loss: tensor(0.1240, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1462\n",
      "Loss: tensor(0.1240, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1463\n",
      "Loss: tensor(0.1240, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1464\n",
      "Loss: tensor(0.1240, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1465\n",
      "Loss: tensor(0.1240, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1466\n",
      "Loss: tensor(0.1239, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1467\n",
      "Loss: tensor(0.1239, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1468\n",
      "Loss: tensor(0.1239, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1469\n",
      "Loss: tensor(0.1239, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1470\n",
      "Loss: tensor(0.1239, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1471\n",
      "Loss: tensor(0.1238, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1472\n",
      "Loss: tensor(0.1238, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1473\n",
      "Loss: tensor(0.1238, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1474\n",
      "Loss: tensor(0.1238, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1238, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1476\n",
      "Loss: tensor(0.1238, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1477\n",
      "Loss: tensor(0.1237, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1478\n",
      "Loss: tensor(0.1237, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1479\n",
      "Loss: tensor(0.1237, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1480\n",
      "Loss: tensor(0.1237, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1481\n",
      "Loss: tensor(0.1237, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1482\n",
      "Loss: tensor(0.1236, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1483\n",
      "Loss: tensor(0.1236, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1484\n",
      "Loss: tensor(0.1236, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1485\n",
      "Loss: tensor(0.1236, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1486\n",
      "Loss: tensor(0.1236, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1487\n",
      "Loss: tensor(0.1235, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1488\n",
      "Loss: tensor(0.1235, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1489\n",
      "Loss: tensor(0.1235, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1490\n",
      "Loss: tensor(0.1235, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1491\n",
      "Loss: tensor(0.1235, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1492\n",
      "Loss: tensor(0.1234, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1493\n",
      "Loss: tensor(0.1234, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1494\n",
      "Loss: tensor(0.1234, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1495\n",
      "Loss: tensor(0.1234, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1496\n",
      "Loss: tensor(0.1234, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1497\n",
      "Loss: tensor(0.1233, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1498\n",
      "Loss: tensor(0.1233, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1499\n",
      "Loss: tensor(0.1233, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1500\n",
      "Loss: tensor(0.1233, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1501\n",
      "Loss: tensor(0.1232, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1502\n",
      "Loss: tensor(0.1232, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1503\n",
      "Loss: tensor(0.1232, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1504\n",
      "Loss: tensor(0.1232, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1505\n",
      "Loss: tensor(0.1232, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1506\n",
      "Loss: tensor(0.1231, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1507\n",
      "Loss: tensor(0.1231, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1508\n",
      "Loss: tensor(0.1231, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1509\n",
      "Loss: tensor(0.1231, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1510\n",
      "Loss: tensor(0.1231, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1511\n",
      "Loss: tensor(0.1230, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1512\n",
      "Loss: tensor(0.1230, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1513\n",
      "Loss: tensor(0.1230, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1514\n",
      "Loss: tensor(0.1230, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1515\n",
      "Loss: tensor(0.1230, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1516\n",
      "Loss: tensor(0.1229, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1517\n",
      "Loss: tensor(0.1229, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1518\n",
      "Loss: tensor(0.1229, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1519\n",
      "Loss: tensor(0.1229, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1520\n",
      "Loss: tensor(0.1229, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1521\n",
      "Loss: tensor(0.1228, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1522\n",
      "Loss: tensor(0.1228, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1523\n",
      "Loss: tensor(0.1228, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1524\n",
      "Loss: tensor(0.1228, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1525\n",
      "Loss: tensor(0.1228, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1526\n",
      "Loss: tensor(0.1228, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1527\n",
      "Loss: tensor(0.1227, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1528\n",
      "Loss: tensor(0.1227, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1529\n",
      "Loss: tensor(0.1227, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1530\n",
      "Loss: tensor(0.1227, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1531\n",
      "Loss: tensor(0.1226, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1532\n",
      "Loss: tensor(0.1226, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1533\n",
      "Loss: tensor(0.1226, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1534\n",
      "Loss: tensor(0.1226, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1535\n",
      "Loss: tensor(0.1226, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1536\n",
      "Loss: tensor(0.1225, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1537\n",
      "Loss: tensor(0.1225, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1538\n",
      "Loss: tensor(0.1225, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1539\n",
      "Loss: tensor(0.1225, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1540\n",
      "Loss: tensor(0.1225, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1541\n",
      "Loss: tensor(0.1224, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1542\n",
      "Loss: tensor(0.1224, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1543\n",
      "Loss: tensor(0.1224, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1544\n",
      "Loss: tensor(0.1224, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1545\n",
      "Loss: tensor(0.1224, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1546\n",
      "Loss: tensor(0.1223, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1547\n",
      "Loss: tensor(0.1223, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1548\n",
      "Loss: tensor(0.1223, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1549\n",
      "Loss: tensor(0.1223, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1550\n",
      "Loss: tensor(0.1223, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1551\n",
      "Loss: tensor(0.1222, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1552\n",
      "Loss: tensor(0.1222, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1553\n",
      "Loss: tensor(0.1222, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1554\n",
      "Loss: tensor(0.1222, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1555\n",
      "Loss: tensor(0.1222, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1556\n",
      "Loss: tensor(0.1221, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1557\n",
      "Loss: tensor(0.1221, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1558\n",
      "Loss: tensor(0.1221, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1559\n",
      "Loss: tensor(0.1221, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1560\n",
      "Loss: tensor(0.1221, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1561\n",
      "Loss: tensor(0.1220, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1562\n",
      "Loss: tensor(0.1220, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1563\n",
      "Loss: tensor(0.1220, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1564\n",
      "Loss: tensor(0.1220, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1565\n",
      "Loss: tensor(0.1220, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1566\n",
      "Loss: tensor(0.1219, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1567\n",
      "Loss: tensor(0.1219, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1568\n",
      "Loss: tensor(0.1219, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1569\n",
      "Loss: tensor(0.1219, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1570\n",
      "Loss: tensor(0.1219, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1571\n",
      "Loss: tensor(0.1218, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1572\n",
      "Loss: tensor(0.1218, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1573\n",
      "Loss: tensor(0.1218, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1574\n",
      "Loss: tensor(0.1218, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1575\n",
      "Loss: tensor(0.1218, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1576\n",
      "Loss: tensor(0.1217, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1577\n",
      "Loss: tensor(0.1217, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1578\n",
      "Loss: tensor(0.1217, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1579\n",
      "Loss: tensor(0.1217, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1580\n",
      "Loss: tensor(0.1217, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1581\n",
      "Loss: tensor(0.1216, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1582\n",
      "Loss: tensor(0.1216, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1583\n",
      "Loss: tensor(0.1216, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1584\n",
      "Loss: tensor(0.1216, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1585\n",
      "Loss: tensor(0.1216, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1586\n",
      "Loss: tensor(0.1215, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1587\n",
      "Loss: tensor(0.1215, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1588\n",
      "Loss: tensor(0.1215, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1589\n",
      "Loss: tensor(0.1215, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1590\n",
      "Loss: tensor(0.1215, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1591\n",
      "Loss: tensor(0.1214, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1214, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1593\n",
      "Loss: tensor(0.1214, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1594\n",
      "Loss: tensor(0.1214, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1595\n",
      "Loss: tensor(0.1214, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1596\n",
      "Loss: tensor(0.1213, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1597\n",
      "Loss: tensor(0.1213, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1598\n",
      "Loss: tensor(0.1213, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1599\n",
      "Loss: tensor(0.1213, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1600\n",
      "Loss: tensor(0.1213, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1601\n",
      "Loss: tensor(0.1212, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1602\n",
      "Loss: tensor(0.1212, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1603\n",
      "Loss: tensor(0.1212, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1604\n",
      "Loss: tensor(0.1212, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1605\n",
      "Loss: tensor(0.1212, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1606\n",
      "Loss: tensor(0.1211, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1607\n",
      "Loss: tensor(0.1211, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1608\n",
      "Loss: tensor(0.1211, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1609\n",
      "Loss: tensor(0.1211, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1610\n",
      "Loss: tensor(0.1211, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1611\n",
      "Loss: tensor(0.1210, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1612\n",
      "Loss: tensor(0.1210, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1613\n",
      "Loss: tensor(0.1210, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1614\n",
      "Loss: tensor(0.1210, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1615\n",
      "Loss: tensor(0.1210, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1616\n",
      "Loss: tensor(0.1209, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1617\n",
      "Loss: tensor(0.1209, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1618\n",
      "Loss: tensor(0.1209, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1619\n",
      "Loss: tensor(0.1209, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1620\n",
      "Loss: tensor(0.1209, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1621\n",
      "Loss: tensor(0.1208, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1622\n",
      "Loss: tensor(0.1208, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1623\n",
      "Loss: tensor(0.1208, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1624\n",
      "Loss: tensor(0.1208, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1625\n",
      "Loss: tensor(0.1208, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1626\n",
      "Loss: tensor(0.1207, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1627\n",
      "Loss: tensor(0.1207, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1628\n",
      "Loss: tensor(0.1207, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1629\n",
      "Loss: tensor(0.1207, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1630\n",
      "Loss: tensor(0.1207, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1631\n",
      "Loss: tensor(0.1206, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1632\n",
      "Loss: tensor(0.1206, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1633\n",
      "Loss: tensor(0.1206, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1634\n",
      "Loss: tensor(0.1206, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1635\n",
      "Loss: tensor(0.1206, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1636\n",
      "Loss: tensor(0.1205, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1637\n",
      "Loss: tensor(0.1205, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1638\n",
      "Loss: tensor(0.1205, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1639\n",
      "Loss: tensor(0.1205, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1640\n",
      "Loss: tensor(0.1205, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1641\n",
      "Loss: tensor(0.1205, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1642\n",
      "Loss: tensor(0.1204, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1643\n",
      "Loss: tensor(0.1204, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1644\n",
      "Loss: tensor(0.1204, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1645\n",
      "Loss: tensor(0.1204, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1646\n",
      "Loss: tensor(0.1203, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1647\n",
      "Loss: tensor(0.1203, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1648\n",
      "Loss: tensor(0.1203, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1649\n",
      "Loss: tensor(0.1203, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1650\n",
      "Loss: tensor(0.1203, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1651\n",
      "Loss: tensor(0.1202, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1652\n",
      "Loss: tensor(0.1202, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1653\n",
      "Loss: tensor(0.1202, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1654\n",
      "Loss: tensor(0.1202, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1655\n",
      "Loss: tensor(0.1202, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1656\n",
      "Loss: tensor(0.1201, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1657\n",
      "Loss: tensor(0.1201, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1658\n",
      "Loss: tensor(0.1201, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1659\n",
      "Loss: tensor(0.1201, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1660\n",
      "Loss: tensor(0.1201, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1661\n",
      "Loss: tensor(0.1201, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1662\n",
      "Loss: tensor(0.1200, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1663\n",
      "Loss: tensor(0.1200, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1664\n",
      "Loss: tensor(0.1200, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1665\n",
      "Loss: tensor(0.1200, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1666\n",
      "Loss: tensor(0.1200, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1667\n",
      "Loss: tensor(0.1199, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1668\n",
      "Loss: tensor(0.1199, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1669\n",
      "Loss: tensor(0.1199, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1670\n",
      "Loss: tensor(0.1199, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1671\n",
      "Loss: tensor(0.1199, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1672\n",
      "Loss: tensor(0.1198, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1673\n",
      "Loss: tensor(0.1198, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1674\n",
      "Loss: tensor(0.1198, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1675\n",
      "Loss: tensor(0.1198, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1676\n",
      "Loss: tensor(0.1198, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1677\n",
      "Loss: tensor(0.1197, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1678\n",
      "Loss: tensor(0.1197, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1679\n",
      "Loss: tensor(0.1197, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1680\n",
      "Loss: tensor(0.1197, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1681\n",
      "Loss: tensor(0.1197, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1682\n",
      "Loss: tensor(0.1196, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1683\n",
      "Loss: tensor(0.1196, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1684\n",
      "Loss: tensor(0.1196, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1685\n",
      "Loss: tensor(0.1196, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1686\n",
      "Loss: tensor(0.1196, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1687\n",
      "Loss: tensor(0.1195, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1688\n",
      "Loss: tensor(0.1195, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1689\n",
      "Loss: tensor(0.1195, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1690\n",
      "Loss: tensor(0.1195, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1691\n",
      "Loss: tensor(0.1195, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1692\n",
      "Loss: tensor(0.1194, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1693\n",
      "Loss: tensor(0.1194, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1694\n",
      "Loss: tensor(0.1194, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1695\n",
      "Loss: tensor(0.1194, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1696\n",
      "Loss: tensor(0.1194, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1697\n",
      "Loss: tensor(0.1194, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1698\n",
      "Loss: tensor(0.1193, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1699\n",
      "Loss: tensor(0.1193, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1700\n",
      "Loss: tensor(0.1193, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1701\n",
      "Loss: tensor(0.1193, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1702\n",
      "Loss: tensor(0.1193, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1703\n",
      "Loss: tensor(0.1192, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1704\n",
      "Loss: tensor(0.1192, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1705\n",
      "Loss: tensor(0.1192, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1706\n",
      "Loss: tensor(0.1192, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1707\n",
      "Loss: tensor(0.1192, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1708\n",
      "Loss: tensor(0.1191, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1709\n",
      "Loss: tensor(0.1191, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1710\n",
      "Loss: tensor(0.1191, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1711\n",
      "Loss: tensor(0.1191, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1191, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1713\n",
      "Loss: tensor(0.1190, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1714\n",
      "Loss: tensor(0.1190, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1715\n",
      "Loss: tensor(0.1190, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1716\n",
      "Loss: tensor(0.1190, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1717\n",
      "Loss: tensor(0.1190, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1718\n",
      "Loss: tensor(0.1190, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1719\n",
      "Loss: tensor(0.1189, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1720\n",
      "Loss: tensor(0.1189, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1721\n",
      "Loss: tensor(0.1189, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1722\n",
      "Loss: tensor(0.1189, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1723\n",
      "Loss: tensor(0.1189, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1724\n",
      "Loss: tensor(0.1188, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1725\n",
      "Loss: tensor(0.1188, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1726\n",
      "Loss: tensor(0.1188, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1727\n",
      "Loss: tensor(0.1188, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1728\n",
      "Loss: tensor(0.1188, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1729\n",
      "Loss: tensor(0.1187, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1730\n",
      "Loss: tensor(0.1187, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1731\n",
      "Loss: tensor(0.1187, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1732\n",
      "Loss: tensor(0.1187, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1733\n",
      "Loss: tensor(0.1187, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1734\n",
      "Loss: tensor(0.1186, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1735\n",
      "Loss: tensor(0.1186, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1736\n",
      "Loss: tensor(0.1186, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1737\n",
      "Loss: tensor(0.1186, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1738\n",
      "Loss: tensor(0.1186, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1739\n",
      "Loss: tensor(0.1185, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1740\n",
      "Loss: tensor(0.1185, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1741\n",
      "Loss: tensor(0.1185, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1742\n",
      "Loss: tensor(0.1185, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1743\n",
      "Loss: tensor(0.1185, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1744\n",
      "Loss: tensor(0.1184, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1745\n",
      "Loss: tensor(0.1184, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1746\n",
      "Loss: tensor(0.1184, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1747\n",
      "Loss: tensor(0.1184, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1748\n",
      "Loss: tensor(0.1184, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1749\n",
      "Loss: tensor(0.1183, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1750\n",
      "Loss: tensor(0.1183, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1751\n",
      "Loss: tensor(0.1183, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1752\n",
      "Loss: tensor(0.1183, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1753\n",
      "Loss: tensor(0.1183, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1754\n",
      "Loss: tensor(0.1183, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1755\n",
      "Loss: tensor(0.1182, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1756\n",
      "Loss: tensor(0.1182, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1757\n",
      "Loss: tensor(0.1182, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1758\n",
      "Loss: tensor(0.1182, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1759\n",
      "Loss: tensor(0.1182, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1760\n",
      "Loss: tensor(0.1181, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1761\n",
      "Loss: tensor(0.1181, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1762\n",
      "Loss: tensor(0.1181, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1763\n",
      "Loss: tensor(0.1181, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1764\n",
      "Loss: tensor(0.1181, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1765\n",
      "Loss: tensor(0.1180, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1766\n",
      "Loss: tensor(0.1180, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1767\n",
      "Loss: tensor(0.1180, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1768\n",
      "Loss: tensor(0.1180, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1769\n",
      "Loss: tensor(0.1180, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1770\n",
      "Loss: tensor(0.1179, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1771\n",
      "Loss: tensor(0.1179, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1772\n",
      "Loss: tensor(0.1179, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1773\n",
      "Loss: tensor(0.1179, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1774\n",
      "Loss: tensor(0.1179, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1775\n",
      "Loss: tensor(0.1178, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1776\n",
      "Loss: tensor(0.1178, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1777\n",
      "Loss: tensor(0.1178, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1778\n",
      "Loss: tensor(0.1178, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1779\n",
      "Loss: tensor(0.1178, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1780\n",
      "Loss: tensor(0.1178, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1781\n",
      "Loss: tensor(0.1177, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1782\n",
      "Loss: tensor(0.1177, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1783\n",
      "Loss: tensor(0.1177, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1784\n",
      "Loss: tensor(0.1177, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1785\n",
      "Loss: tensor(0.1177, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1786\n",
      "Loss: tensor(0.1176, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1787\n",
      "Loss: tensor(0.1176, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1788\n",
      "Loss: tensor(0.1176, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1789\n",
      "Loss: tensor(0.1176, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1790\n",
      "Loss: tensor(0.1176, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1791\n",
      "Loss: tensor(0.1176, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1792\n",
      "Loss: tensor(0.1175, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1793\n",
      "Loss: tensor(0.1175, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1794\n",
      "Loss: tensor(0.1175, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1795\n",
      "Loss: tensor(0.1175, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1796\n",
      "Loss: tensor(0.1175, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1797\n",
      "Loss: tensor(0.1174, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1798\n",
      "Loss: tensor(0.1174, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1799\n",
      "Loss: tensor(0.1174, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1800\n",
      "Loss: tensor(0.1174, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1801\n",
      "Loss: tensor(0.1174, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1802\n",
      "Loss: tensor(0.1173, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1803\n",
      "Loss: tensor(0.1173, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1804\n",
      "Loss: tensor(0.1173, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1805\n",
      "Loss: tensor(0.1173, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1806\n",
      "Loss: tensor(0.1173, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1807\n",
      "Loss: tensor(0.1172, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1808\n",
      "Loss: tensor(0.1172, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1809\n",
      "Loss: tensor(0.1172, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1810\n",
      "Loss: tensor(0.1172, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1811\n",
      "Loss: tensor(0.1172, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1812\n",
      "Loss: tensor(0.1171, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1813\n",
      "Loss: tensor(0.1171, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1814\n",
      "Loss: tensor(0.1171, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1815\n",
      "Loss: tensor(0.1171, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1816\n",
      "Loss: tensor(0.1171, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1817\n",
      "Loss: tensor(0.1171, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1818\n",
      "Loss: tensor(0.1170, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1819\n",
      "Loss: tensor(0.1170, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1820\n",
      "Loss: tensor(0.1170, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1821\n",
      "Loss: tensor(0.1170, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1822\n",
      "Loss: tensor(0.1170, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1823\n",
      "Loss: tensor(0.1169, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1824\n",
      "Loss: tensor(0.1169, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1825\n",
      "Loss: tensor(0.1169, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1826\n",
      "Loss: tensor(0.1169, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1827\n",
      "Loss: tensor(0.1169, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1828\n",
      "Loss: tensor(0.1168, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1829\n",
      "Loss: tensor(0.1168, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1830\n",
      "Loss: tensor(0.1168, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1831\n",
      "Loss: tensor(0.1168, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1832\n",
      "Loss: tensor(0.1168, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1168, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1834\n",
      "Loss: tensor(0.1167, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1835\n",
      "Loss: tensor(0.1167, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1836\n",
      "Loss: tensor(0.1167, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1837\n",
      "Loss: tensor(0.1167, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1838\n",
      "Loss: tensor(0.1167, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1839\n",
      "Loss: tensor(0.1166, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1840\n",
      "Loss: tensor(0.1166, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1841\n",
      "Loss: tensor(0.1166, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1842\n",
      "Loss: tensor(0.1166, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1843\n",
      "Loss: tensor(0.1166, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1844\n",
      "Loss: tensor(0.1165, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1845\n",
      "Loss: tensor(0.1165, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1846\n",
      "Loss: tensor(0.1165, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1847\n",
      "Loss: tensor(0.1165, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1848\n",
      "Loss: tensor(0.1165, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1849\n",
      "Loss: tensor(0.1165, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1850\n",
      "Loss: tensor(0.1164, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1851\n",
      "Loss: tensor(0.1164, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1852\n",
      "Loss: tensor(0.1164, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1853\n",
      "Loss: tensor(0.1164, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1854\n",
      "Loss: tensor(0.1164, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1855\n",
      "Loss: tensor(0.1163, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1856\n",
      "Loss: tensor(0.1163, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1857\n",
      "Loss: tensor(0.1163, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1858\n",
      "Loss: tensor(0.1163, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1859\n",
      "Loss: tensor(0.1163, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1860\n",
      "Loss: tensor(0.1163, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1861\n",
      "Loss: tensor(0.1162, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1862\n",
      "Loss: tensor(0.1162, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1863\n",
      "Loss: tensor(0.1162, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1864\n",
      "Loss: tensor(0.1162, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1865\n",
      "Loss: tensor(0.1162, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1866\n",
      "Loss: tensor(0.1162, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1867\n",
      "Loss: tensor(0.1161, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1868\n",
      "Loss: tensor(0.1161, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1869\n",
      "Loss: tensor(0.1161, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1870\n",
      "Loss: tensor(0.1161, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1871\n",
      "Loss: tensor(0.1161, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1872\n",
      "Loss: tensor(0.1160, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1873\n",
      "Loss: tensor(0.1160, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1874\n",
      "Loss: tensor(0.1160, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1875\n",
      "Loss: tensor(0.1160, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1876\n",
      "Loss: tensor(0.1160, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1877\n",
      "Loss: tensor(0.1159, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1878\n",
      "Loss: tensor(0.1159, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1879\n",
      "Loss: tensor(0.1159, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1880\n",
      "Loss: tensor(0.1159, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1881\n",
      "Loss: tensor(0.1159, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1882\n",
      "Loss: tensor(0.1159, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1883\n",
      "Loss: tensor(0.1158, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1884\n",
      "Loss: tensor(0.1158, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1885\n",
      "Loss: tensor(0.1158, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1886\n",
      "Loss: tensor(0.1158, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1887\n",
      "Loss: tensor(0.1158, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1888\n",
      "Loss: tensor(0.1157, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1889\n",
      "Loss: tensor(0.1157, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1890\n",
      "Loss: tensor(0.1157, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1891\n",
      "Loss: tensor(0.1157, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1892\n",
      "Loss: tensor(0.1157, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1893\n",
      "Loss: tensor(0.1157, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1894\n",
      "Loss: tensor(0.1156, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1895\n",
      "Loss: tensor(0.1156, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1896\n",
      "Loss: tensor(0.1156, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1897\n",
      "Loss: tensor(0.1156, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1898\n",
      "Loss: tensor(0.1156, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1899\n",
      "Loss: tensor(0.1155, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1900\n",
      "Loss: tensor(0.1155, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1901\n",
      "Loss: tensor(0.1155, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1902\n",
      "Loss: tensor(0.1155, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1903\n",
      "Loss: tensor(0.1155, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1904\n",
      "Loss: tensor(0.1155, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1905\n",
      "Loss: tensor(0.1154, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1906\n",
      "Loss: tensor(0.1154, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1907\n",
      "Loss: tensor(0.1154, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1908\n",
      "Loss: tensor(0.1154, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1909\n",
      "Loss: tensor(0.1154, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1910\n",
      "Loss: tensor(0.1154, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1911\n",
      "Loss: tensor(0.1153, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1912\n",
      "Loss: tensor(0.1153, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1913\n",
      "Loss: tensor(0.1153, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1914\n",
      "Loss: tensor(0.1153, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1915\n",
      "Loss: tensor(0.1153, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1916\n",
      "Loss: tensor(0.1152, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1917\n",
      "Loss: tensor(0.1152, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1918\n",
      "Loss: tensor(0.1152, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1919\n",
      "Loss: tensor(0.1152, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1920\n",
      "Loss: tensor(0.1152, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1921\n",
      "Loss: tensor(0.1152, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1922\n",
      "Loss: tensor(0.1151, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1923\n",
      "Loss: tensor(0.1151, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1924\n",
      "Loss: tensor(0.1151, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1925\n",
      "Loss: tensor(0.1151, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1926\n",
      "Loss: tensor(0.1151, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1927\n",
      "Loss: tensor(0.1151, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1928\n",
      "Loss: tensor(0.1150, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1929\n",
      "Loss: tensor(0.1150, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1930\n",
      "Loss: tensor(0.1150, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1931\n",
      "Loss: tensor(0.1150, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1932\n",
      "Loss: tensor(0.1150, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1933\n",
      "Loss: tensor(0.1149, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1934\n",
      "Loss: tensor(0.1149, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1935\n",
      "Loss: tensor(0.1149, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1936\n",
      "Loss: tensor(0.1149, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1937\n",
      "Loss: tensor(0.1149, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1938\n",
      "Loss: tensor(0.1149, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1939\n",
      "Loss: tensor(0.1148, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1940\n",
      "Loss: tensor(0.1148, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1941\n",
      "Loss: tensor(0.1148, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1942\n",
      "Loss: tensor(0.1148, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1943\n",
      "Loss: tensor(0.1148, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1944\n",
      "Loss: tensor(0.1148, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1945\n",
      "Loss: tensor(0.1148, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1946\n",
      "Loss: tensor(0.1147, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1947\n",
      "Loss: tensor(0.1147, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1948\n",
      "Loss: tensor(0.1147, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1949\n",
      "Loss: tensor(0.1147, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1950\n",
      "Loss: tensor(0.1147, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1146, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1952\n",
      "Loss: tensor(0.1146, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1953\n",
      "Loss: tensor(0.1146, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1954\n",
      "Loss: tensor(0.1146, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1955\n",
      "Loss: tensor(0.1146, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1956\n",
      "Loss: tensor(0.1146, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1957\n",
      "Loss: tensor(0.1145, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1958\n",
      "Loss: tensor(0.1145, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1959\n",
      "Loss: tensor(0.1145, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1960\n",
      "Loss: tensor(0.1145, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1961\n",
      "Loss: tensor(0.1145, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1962\n",
      "Loss: tensor(0.1144, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1963\n",
      "Loss: tensor(0.1144, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1964\n",
      "Loss: tensor(0.1144, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1965\n",
      "Loss: tensor(0.1144, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1966\n",
      "Loss: tensor(0.1144, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1967\n",
      "Loss: tensor(0.1144, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1968\n",
      "Loss: tensor(0.1143, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1969\n",
      "Loss: tensor(0.1143, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1970\n",
      "Loss: tensor(0.1143, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1971\n",
      "Loss: tensor(0.1143, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1972\n",
      "Loss: tensor(0.1143, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1973\n",
      "Loss: tensor(0.1143, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1974\n",
      "Loss: tensor(0.1142, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1975\n",
      "Loss: tensor(0.1142, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1976\n",
      "Loss: tensor(0.1142, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1977\n",
      "Loss: tensor(0.1142, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1978\n",
      "Loss: tensor(0.1142, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1979\n",
      "Loss: tensor(0.1142, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1980\n",
      "Loss: tensor(0.1141, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1981\n",
      "Loss: tensor(0.1141, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1982\n",
      "Loss: tensor(0.1141, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1983\n",
      "Loss: tensor(0.1141, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1984\n",
      "Loss: tensor(0.1141, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1985\n",
      "Loss: tensor(0.1141, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1986\n",
      "Loss: tensor(0.1140, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1987\n",
      "Loss: tensor(0.1140, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1988\n",
      "Loss: tensor(0.1140, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1989\n",
      "Loss: tensor(0.1140, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1990\n",
      "Loss: tensor(0.1140, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1991\n",
      "Loss: tensor(0.1140, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1992\n",
      "Loss: tensor(0.1140, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1993\n",
      "Loss: tensor(0.1139, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1994\n",
      "Loss: tensor(0.1139, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1995\n",
      "Loss: tensor(0.1139, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1996\n",
      "Loss: tensor(0.1139, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1997\n",
      "Loss: tensor(0.1139, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1998\n",
      "Loss: tensor(0.1139, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1999\n",
      "Loss: tensor(0.1138, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2000\n",
      "Loss: tensor(0.1138, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2001\n",
      "Loss: tensor(0.1138, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2002\n",
      "Loss: tensor(0.1138, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2003\n",
      "Loss: tensor(0.1138, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2004\n",
      "Loss: tensor(0.1138, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2005\n",
      "Loss: tensor(0.1137, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2006\n",
      "Loss: tensor(0.1137, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2007\n",
      "Loss: tensor(0.1137, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2008\n",
      "Loss: tensor(0.1137, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2009\n",
      "Loss: tensor(0.1137, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2010\n",
      "Loss: tensor(0.1137, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2011\n",
      "Loss: tensor(0.1136, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2012\n",
      "Loss: tensor(0.1136, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2013\n",
      "Loss: tensor(0.1136, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2014\n",
      "Loss: tensor(0.1136, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2015\n",
      "Loss: tensor(0.1136, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2016\n",
      "Loss: tensor(0.1136, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2017\n",
      "Loss: tensor(0.1135, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2018\n",
      "Loss: tensor(0.1135, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2019\n",
      "Loss: tensor(0.1135, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2020\n",
      "Loss: tensor(0.1135, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2021\n",
      "Loss: tensor(0.1135, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2022\n",
      "Loss: tensor(0.1135, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2023\n",
      "Loss: tensor(0.1134, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2024\n",
      "Loss: tensor(0.1134, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2025\n",
      "Loss: tensor(0.1134, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2026\n",
      "Loss: tensor(0.1134, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2027\n",
      "Loss: tensor(0.1134, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2028\n",
      "Loss: tensor(0.1134, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2029\n",
      "Loss: tensor(0.1134, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2030\n",
      "Loss: tensor(0.1133, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2031\n",
      "Loss: tensor(0.1133, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2032\n",
      "Loss: tensor(0.1133, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2033\n",
      "Loss: tensor(0.1133, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2034\n",
      "Loss: tensor(0.1133, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2035\n",
      "Loss: tensor(0.1133, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2036\n",
      "Loss: tensor(0.1132, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2037\n",
      "Loss: tensor(0.1132, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2038\n",
      "Loss: tensor(0.1132, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2039\n",
      "Loss: tensor(0.1132, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2040\n",
      "Loss: tensor(0.1132, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2041\n",
      "Loss: tensor(0.1132, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2042\n",
      "Loss: tensor(0.1131, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2043\n",
      "Loss: tensor(0.1131, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2044\n",
      "Loss: tensor(0.1131, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2045\n",
      "Loss: tensor(0.1131, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2046\n",
      "Loss: tensor(0.1131, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2047\n",
      "Loss: tensor(0.1131, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2048\n",
      "Loss: tensor(0.1131, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2049\n",
      "Loss: tensor(0.1130, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2050\n",
      "Loss: tensor(0.1130, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2051\n",
      "Loss: tensor(0.1130, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2052\n",
      "Loss: tensor(0.1130, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2053\n",
      "Loss: tensor(0.1130, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2054\n",
      "Loss: tensor(0.1130, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2055\n",
      "Loss: tensor(0.1129, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2056\n",
      "Loss: tensor(0.1129, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2057\n",
      "Loss: tensor(0.1129, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2058\n",
      "Loss: tensor(0.1129, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2059\n",
      "Loss: tensor(0.1129, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2060\n",
      "Loss: tensor(0.1129, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2061\n",
      "Loss: tensor(0.1129, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2062\n",
      "Loss: tensor(0.1128, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2063\n",
      "Loss: tensor(0.1128, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2064\n",
      "Loss: tensor(0.1128, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2065\n",
      "Loss: tensor(0.1128, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2066\n",
      "Loss: tensor(0.1128, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2067\n",
      "Loss: tensor(0.1128, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2068\n",
      "Loss: tensor(0.1128, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2069\n",
      "Loss: tensor(0.1128, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1127, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2071\n",
      "Loss: tensor(0.1127, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2072\n",
      "Loss: tensor(0.1127, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2073\n",
      "Loss: tensor(0.1127, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2074\n",
      "Loss: tensor(0.1127, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2075\n",
      "Loss: tensor(0.1126, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2076\n",
      "Loss: tensor(0.1126, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2077\n",
      "Loss: tensor(0.1126, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2078\n",
      "Loss: tensor(0.1126, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2079\n",
      "Loss: tensor(0.1126, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2080\n",
      "Loss: tensor(0.1126, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2081\n",
      "Loss: tensor(0.1126, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2082\n",
      "Loss: tensor(0.1125, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2083\n",
      "Loss: tensor(0.1125, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2084\n",
      "Loss: tensor(0.1125, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2085\n",
      "Loss: tensor(0.1125, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2086\n",
      "Loss: tensor(0.1125, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2087\n",
      "Loss: tensor(0.1125, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2088\n",
      "Loss: tensor(0.1125, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2089\n",
      "Loss: tensor(0.1124, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2090\n",
      "Loss: tensor(0.1124, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2091\n",
      "Loss: tensor(0.1124, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2092\n",
      "Loss: tensor(0.1124, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2093\n",
      "Loss: tensor(0.1124, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2094\n",
      "Loss: tensor(0.1124, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2095\n",
      "Loss: tensor(0.1124, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2096\n",
      "Loss: tensor(0.1123, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2097\n",
      "Loss: tensor(0.1123, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2098\n",
      "Loss: tensor(0.1123, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2099\n",
      "Loss: tensor(0.1123, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2100\n",
      "Loss: tensor(0.1123, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2101\n",
      "Loss: tensor(0.1123, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2102\n",
      "Loss: tensor(0.1122, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2103\n",
      "Loss: tensor(0.1122, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2104\n",
      "Loss: tensor(0.1122, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2105\n",
      "Loss: tensor(0.1122, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2106\n",
      "Loss: tensor(0.1122, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2107\n",
      "Loss: tensor(0.1122, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2108\n",
      "Loss: tensor(0.1122, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2109\n",
      "Loss: tensor(0.1121, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2110\n",
      "Loss: tensor(0.1121, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2111\n",
      "Loss: tensor(0.1121, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2112\n",
      "Loss: tensor(0.1121, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2113\n",
      "Loss: tensor(0.1121, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2114\n",
      "Loss: tensor(0.1121, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2115\n",
      "Loss: tensor(0.1121, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2116\n",
      "Loss: tensor(0.1120, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2117\n",
      "Loss: tensor(0.1120, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2118\n",
      "Loss: tensor(0.1120, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2119\n",
      "Loss: tensor(0.1120, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2120\n",
      "Loss: tensor(0.1120, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2121\n",
      "Loss: tensor(0.1120, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2122\n",
      "Loss: tensor(0.1120, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2123\n",
      "Loss: tensor(0.1119, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2124\n",
      "Loss: tensor(0.1119, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2125\n",
      "Loss: tensor(0.1119, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2126\n",
      "Loss: tensor(0.1119, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2127\n",
      "Loss: tensor(0.1119, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2128\n",
      "Loss: tensor(0.1119, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2129\n",
      "Loss: tensor(0.1119, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2130\n",
      "Loss: tensor(0.1119, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2131\n",
      "Loss: tensor(0.1118, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2132\n",
      "Loss: tensor(0.1118, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2133\n",
      "Loss: tensor(0.1118, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2134\n",
      "Loss: tensor(0.1118, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2135\n",
      "Loss: tensor(0.1118, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2136\n",
      "Loss: tensor(0.1118, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2137\n",
      "Loss: tensor(0.1118, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2138\n",
      "Loss: tensor(0.1118, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2139\n",
      "Loss: tensor(0.1117, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2140\n",
      "Loss: tensor(0.1117, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2141\n",
      "Loss: tensor(0.1117, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2142\n",
      "Loss: tensor(0.1117, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2143\n",
      "Loss: tensor(0.1117, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2144\n",
      "Loss: tensor(0.1117, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2145\n",
      "Loss: tensor(0.1116, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2146\n",
      "Loss: tensor(0.1116, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2147\n",
      "Loss: tensor(0.1116, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2148\n",
      "Loss: tensor(0.1116, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2149\n",
      "Loss: tensor(0.1116, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2150\n",
      "Loss: tensor(0.1116, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2151\n",
      "Loss: tensor(0.1116, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2152\n",
      "Loss: tensor(0.1116, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2153\n",
      "Loss: tensor(0.1115, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2154\n",
      "Loss: tensor(0.1115, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2155\n",
      "Loss: tensor(0.1115, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2156\n",
      "Loss: tensor(0.1115, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2157\n",
      "Loss: tensor(0.1115, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2158\n",
      "Loss: tensor(0.1115, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2159\n",
      "Loss: tensor(0.1115, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2160\n",
      "Loss: tensor(0.1114, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2161\n",
      "Loss: tensor(0.1114, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2162\n",
      "Loss: tensor(0.1114, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2163\n",
      "Loss: tensor(0.1114, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2164\n",
      "Loss: tensor(0.1114, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2165\n",
      "Loss: tensor(0.1114, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2166\n",
      "Loss: tensor(0.1114, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2167\n",
      "Loss: tensor(0.1114, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2168\n",
      "Loss: tensor(0.1113, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2169\n",
      "Loss: tensor(0.1113, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2170\n",
      "Loss: tensor(0.1113, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2171\n",
      "Loss: tensor(0.1113, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2172\n",
      "Loss: tensor(0.1113, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2173\n",
      "Loss: tensor(0.1113, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2174\n",
      "Loss: tensor(0.1113, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2175\n",
      "Loss: tensor(0.1112, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2176\n",
      "Loss: tensor(0.1112, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2177\n",
      "Loss: tensor(0.1112, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2178\n",
      "Loss: tensor(0.1112, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2179\n",
      "Loss: tensor(0.1112, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2180\n",
      "Loss: tensor(0.1112, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2181\n",
      "Loss: tensor(0.1112, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2182\n",
      "Loss: tensor(0.1112, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2183\n",
      "Loss: tensor(0.1111, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2184\n",
      "Loss: tensor(0.1111, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2185\n",
      "Loss: tensor(0.1111, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2186\n",
      "Loss: tensor(0.1111, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2187\n",
      "Loss: tensor(0.1111, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1111, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2189\n",
      "Loss: tensor(0.1111, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2190\n",
      "Loss: tensor(0.1110, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2191\n",
      "Loss: tensor(0.1110, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2192\n",
      "Loss: tensor(0.1110, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2193\n",
      "Loss: tensor(0.1110, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2194\n",
      "Loss: tensor(0.1110, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2195\n",
      "Loss: tensor(0.1110, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2196\n",
      "Loss: tensor(0.1110, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2197\n",
      "Loss: tensor(0.1110, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2198\n",
      "Loss: tensor(0.1109, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2199\n",
      "Loss: tensor(0.1109, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2200\n",
      "Loss: tensor(0.1109, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2201\n",
      "Loss: tensor(0.1109, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2202\n",
      "Loss: tensor(0.1109, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2203\n",
      "Loss: tensor(0.1109, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2204\n",
      "Loss: tensor(0.1109, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2205\n",
      "Loss: tensor(0.1109, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2206\n",
      "Loss: tensor(0.1108, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2207\n",
      "Loss: tensor(0.1108, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2208\n",
      "Loss: tensor(0.1108, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2209\n",
      "Loss: tensor(0.1108, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2210\n",
      "Loss: tensor(0.1108, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2211\n",
      "Loss: tensor(0.1108, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2212\n",
      "Loss: tensor(0.1108, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2213\n",
      "Loss: tensor(0.1108, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2214\n",
      "Loss: tensor(0.1107, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2215\n",
      "Loss: tensor(0.1107, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2216\n",
      "Loss: tensor(0.1107, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2217\n",
      "Loss: tensor(0.1107, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2218\n",
      "Loss: tensor(0.1107, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2219\n",
      "Loss: tensor(0.1107, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2220\n",
      "Loss: tensor(0.1107, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2221\n",
      "Loss: tensor(0.1107, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2222\n",
      "Loss: tensor(0.1106, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2223\n",
      "Loss: tensor(0.1106, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2224\n",
      "Loss: tensor(0.1106, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2225\n",
      "Loss: tensor(0.1106, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2226\n",
      "Loss: tensor(0.1106, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2227\n",
      "Loss: tensor(0.1106, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2228\n",
      "Loss: tensor(0.1106, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2229\n",
      "Loss: tensor(0.1106, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2230\n",
      "Loss: tensor(0.1105, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2231\n",
      "Loss: tensor(0.1105, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2232\n",
      "Loss: tensor(0.1105, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2233\n",
      "Loss: tensor(0.1105, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2234\n",
      "Loss: tensor(0.1105, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2235\n",
      "Loss: tensor(0.1105, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2236\n",
      "Loss: tensor(0.1105, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2237\n",
      "Loss: tensor(0.1105, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2238\n",
      "Loss: tensor(0.1105, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2239\n",
      "Loss: tensor(0.1104, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2240\n",
      "Loss: tensor(0.1104, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2241\n",
      "Loss: tensor(0.1104, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2242\n",
      "Loss: tensor(0.1104, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2243\n",
      "Loss: tensor(0.1104, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2244\n",
      "Loss: tensor(0.1104, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2245\n",
      "Loss: tensor(0.1104, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2246\n",
      "Loss: tensor(0.1104, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2247\n",
      "Loss: tensor(0.1103, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2248\n",
      "Loss: tensor(0.1103, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2249\n",
      "Loss: tensor(0.1103, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2250\n",
      "Loss: tensor(0.1103, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2251\n",
      "Loss: tensor(0.1103, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2252\n",
      "Loss: tensor(0.1103, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2253\n",
      "Loss: tensor(0.1103, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2254\n",
      "Loss: tensor(0.1103, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2255\n",
      "Loss: tensor(0.1102, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2256\n",
      "Loss: tensor(0.1102, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2257\n",
      "Loss: tensor(0.1102, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2258\n",
      "Loss: tensor(0.1102, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2259\n",
      "Loss: tensor(0.1102, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2260\n",
      "Loss: tensor(0.1102, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2261\n",
      "Loss: tensor(0.1102, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2262\n",
      "Loss: tensor(0.1102, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2263\n",
      "Loss: tensor(0.1102, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2264\n",
      "Loss: tensor(0.1102, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2265\n",
      "Loss: tensor(0.1101, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2266\n",
      "Loss: tensor(0.1101, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2267\n",
      "Loss: tensor(0.1101, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2268\n",
      "Loss: tensor(0.1101, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2269\n",
      "Loss: tensor(0.1101, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2270\n",
      "Loss: tensor(0.1101, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2271\n",
      "Loss: tensor(0.1101, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2272\n",
      "Loss: tensor(0.1101, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2273\n",
      "Loss: tensor(0.1100, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2274\n",
      "Loss: tensor(0.1100, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2275\n",
      "Loss: tensor(0.1100, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2276\n",
      "Loss: tensor(0.1100, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2277\n",
      "Loss: tensor(0.1100, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2278\n",
      "Loss: tensor(0.1100, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2279\n",
      "Loss: tensor(0.1100, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2280\n",
      "Loss: tensor(0.1100, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2281\n",
      "Loss: tensor(0.1099, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2282\n",
      "Loss: tensor(0.1099, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2283\n",
      "Loss: tensor(0.1099, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2284\n",
      "Loss: tensor(0.1099, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2285\n",
      "Loss: tensor(0.1099, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2286\n",
      "Loss: tensor(0.1099, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2287\n",
      "Loss: tensor(0.1099, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2288\n",
      "Loss: tensor(0.1099, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2289\n",
      "Loss: tensor(0.1099, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2290\n",
      "Loss: tensor(0.1098, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2291\n",
      "Loss: tensor(0.1098, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2292\n",
      "Loss: tensor(0.1098, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2293\n",
      "Loss: tensor(0.1098, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2294\n",
      "Loss: tensor(0.1098, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2295\n",
      "Loss: tensor(0.1098, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2296\n",
      "Loss: tensor(0.1098, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2297\n",
      "Loss: tensor(0.1098, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2298\n",
      "Loss: tensor(0.1098, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2299\n",
      "Loss: tensor(0.1097, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2300\n",
      "Loss: tensor(0.1097, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2301\n",
      "Loss: tensor(0.1097, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2302\n",
      "Loss: tensor(0.1097, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2303\n",
      "Loss: tensor(0.1097, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2304\n",
      "Loss: tensor(0.1097, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2305\n",
      "Loss: tensor(0.1097, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2306\n",
      "Loss: tensor(0.1097, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2307\n",
      "Loss: tensor(0.1097, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1097, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2309\n",
      "Loss: tensor(0.1096, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2310\n",
      "Loss: tensor(0.1096, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2311\n",
      "Loss: tensor(0.1096, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2312\n",
      "Loss: tensor(0.1096, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2313\n",
      "Loss: tensor(0.1096, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2314\n",
      "Loss: tensor(0.1096, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2315\n",
      "Loss: tensor(0.1096, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2316\n",
      "Loss: tensor(0.1096, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2317\n",
      "Loss: tensor(0.1096, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2318\n",
      "Loss: tensor(0.1095, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2319\n",
      "Loss: tensor(0.1095, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2320\n",
      "Loss: tensor(0.1095, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2321\n",
      "Loss: tensor(0.1095, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2322\n",
      "Loss: tensor(0.1095, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2323\n",
      "Loss: tensor(0.1095, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2324\n",
      "Loss: tensor(0.1095, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2325\n",
      "Loss: tensor(0.1095, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2326\n",
      "Loss: tensor(0.1095, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2327\n",
      "Loss: tensor(0.1095, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2328\n",
      "Loss: tensor(0.1094, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2329\n",
      "Loss: tensor(0.1094, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2330\n",
      "Loss: tensor(0.1094, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2331\n",
      "Loss: tensor(0.1094, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2332\n",
      "Loss: tensor(0.1094, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2333\n",
      "Loss: tensor(0.1094, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2334\n",
      "Loss: tensor(0.1094, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2335\n",
      "Loss: tensor(0.1094, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2336\n",
      "Loss: tensor(0.1094, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2337\n",
      "Loss: tensor(0.1093, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2338\n",
      "Loss: tensor(0.1093, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2339\n",
      "Loss: tensor(0.1093, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2340\n",
      "Loss: tensor(0.1093, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2341\n",
      "Loss: tensor(0.1093, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2342\n",
      "Loss: tensor(0.1093, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2343\n",
      "Loss: tensor(0.1093, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2344\n",
      "Loss: tensor(0.1093, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2345\n",
      "Loss: tensor(0.1093, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2346\n",
      "Loss: tensor(0.1093, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2347\n",
      "Loss: tensor(0.1092, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2348\n",
      "Loss: tensor(0.1092, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2349\n",
      "Loss: tensor(0.1092, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2350\n",
      "Loss: tensor(0.1092, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2351\n",
      "Loss: tensor(0.1092, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2352\n",
      "Loss: tensor(0.1092, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2353\n",
      "Loss: tensor(0.1092, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2354\n",
      "Loss: tensor(0.1092, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2355\n",
      "Loss: tensor(0.1092, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2356\n",
      "Loss: tensor(0.1092, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2357\n",
      "Loss: tensor(0.1091, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2358\n",
      "Loss: tensor(0.1091, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2359\n",
      "Loss: tensor(0.1091, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2360\n",
      "Loss: tensor(0.1091, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2361\n",
      "Loss: tensor(0.1091, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2362\n",
      "Loss: tensor(0.1091, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2363\n",
      "Loss: tensor(0.1091, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2364\n",
      "Loss: tensor(0.1091, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2365\n",
      "Loss: tensor(0.1091, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2366\n",
      "Loss: tensor(0.1091, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2367\n",
      "Loss: tensor(0.1090, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2368\n",
      "Loss: tensor(0.1090, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2369\n",
      "Loss: tensor(0.1090, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2370\n",
      "Loss: tensor(0.1090, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2371\n",
      "Loss: tensor(0.1090, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2372\n",
      "Loss: tensor(0.1090, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2373\n",
      "Loss: tensor(0.1090, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2374\n",
      "Loss: tensor(0.1090, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2375\n",
      "Loss: tensor(0.1090, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2376\n",
      "Loss: tensor(0.1090, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2377\n",
      "Loss: tensor(0.1089, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2378\n",
      "Loss: tensor(0.1089, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2379\n",
      "Loss: tensor(0.1089, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2380\n",
      "Loss: tensor(0.1089, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2381\n",
      "Loss: tensor(0.1089, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2382\n",
      "Loss: tensor(0.1089, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2383\n",
      "Loss: tensor(0.1089, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2384\n",
      "Loss: tensor(0.1089, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2385\n",
      "Loss: tensor(0.1089, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2386\n",
      "Loss: tensor(0.1089, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2387\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2388\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2389\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2390\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2391\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2392\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2393\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2394\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2395\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2396\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2397\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2398\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2399\n",
      "Loss: tensor(0.1088, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2400\n",
      "Loss: tensor(0.1087, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2401\n",
      "Loss: tensor(0.1087, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2402\n",
      "Loss: tensor(0.1087, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2403\n",
      "Loss: tensor(0.1087, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2404\n",
      "Loss: tensor(0.1087, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2405\n",
      "Loss: tensor(0.1087, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2406\n",
      "Loss: tensor(0.1087, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2407\n",
      "Loss: tensor(0.1087, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2408\n",
      "Loss: tensor(0.1087, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2409\n",
      "Loss: tensor(0.1086, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2410\n",
      "Loss: tensor(0.1086, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2411\n",
      "Loss: tensor(0.1086, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2412\n",
      "Loss: tensor(0.1086, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2413\n",
      "Loss: tensor(0.1086, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2414\n",
      "Loss: tensor(0.1086, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2415\n",
      "Loss: tensor(0.1086, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2416\n",
      "Loss: tensor(0.1086, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2417\n",
      "Loss: tensor(0.1086, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2418\n",
      "Loss: tensor(0.1086, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2419\n",
      "Loss: tensor(0.1086, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2420\n",
      "Loss: tensor(0.1085, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2421\n",
      "Loss: tensor(0.1085, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2422\n",
      "Loss: tensor(0.1085, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2423\n",
      "Loss: tensor(0.1085, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2424\n",
      "Loss: tensor(0.1085, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2425\n",
      "Loss: tensor(0.1085, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2426\n",
      "Loss: tensor(0.1085, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2427\n",
      "Loss: tensor(0.1085, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2428\n",
      "Loss: tensor(0.1085, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1085, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2430\n",
      "Loss: tensor(0.1085, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2431\n",
      "Loss: tensor(0.1084, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2432\n",
      "Loss: tensor(0.1084, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2433\n",
      "Loss: tensor(0.1084, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2434\n",
      "Loss: tensor(0.1084, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2435\n",
      "Loss: tensor(0.1084, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2436\n",
      "Loss: tensor(0.1084, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2437\n",
      "Loss: tensor(0.1084, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2438\n",
      "Loss: tensor(0.1084, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2439\n",
      "Loss: tensor(0.1084, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2440\n",
      "Loss: tensor(0.1084, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2441\n",
      "Loss: tensor(0.1084, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2442\n",
      "Loss: tensor(0.1083, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2443\n",
      "Loss: tensor(0.1083, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2444\n",
      "Loss: tensor(0.1083, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2445\n",
      "Loss: tensor(0.1083, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2446\n",
      "Loss: tensor(0.1083, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2447\n",
      "Loss: tensor(0.1083, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2448\n",
      "Loss: tensor(0.1083, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2449\n",
      "Loss: tensor(0.1083, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2450\n",
      "Loss: tensor(0.1083, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2451\n",
      "Loss: tensor(0.1083, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2452\n",
      "Loss: tensor(0.1083, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2453\n",
      "Loss: tensor(0.1082, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2454\n",
      "Loss: tensor(0.1082, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2455\n",
      "Loss: tensor(0.1082, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2456\n",
      "Loss: tensor(0.1082, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2457\n",
      "Loss: tensor(0.1082, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2458\n",
      "Loss: tensor(0.1082, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2459\n",
      "Loss: tensor(0.1082, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2460\n",
      "Loss: tensor(0.1082, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2461\n",
      "Loss: tensor(0.1082, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2462\n",
      "Loss: tensor(0.1082, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2463\n",
      "Loss: tensor(0.1082, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2464\n",
      "Loss: tensor(0.1082, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2465\n",
      "Loss: tensor(0.1081, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2466\n",
      "Loss: tensor(0.1081, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2467\n",
      "Loss: tensor(0.1081, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2468\n",
      "Loss: tensor(0.1081, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2469\n",
      "Loss: tensor(0.1081, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2470\n",
      "Loss: tensor(0.1081, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2471\n",
      "Loss: tensor(0.1081, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2472\n",
      "Loss: tensor(0.1081, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2473\n",
      "Loss: tensor(0.1081, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2474\n",
      "Loss: tensor(0.1081, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2475\n",
      "Loss: tensor(0.1081, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2476\n",
      "Loss: tensor(0.1081, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2477\n",
      "Loss: tensor(0.1080, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2478\n",
      "Loss: tensor(0.1080, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2479\n",
      "Loss: tensor(0.1080, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2480\n",
      "Loss: tensor(0.1080, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2481\n",
      "Loss: tensor(0.1080, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2482\n",
      "Loss: tensor(0.1080, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2483\n",
      "Loss: tensor(0.1080, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2484\n",
      "Loss: tensor(0.1080, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2485\n",
      "Loss: tensor(0.1080, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2486\n",
      "Loss: tensor(0.1080, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2487\n",
      "Loss: tensor(0.1080, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2488\n",
      "Loss: tensor(0.1080, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2489\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2490\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2491\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2492\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2493\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2494\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2495\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2496\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2497\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2498\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2499\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2500\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2501\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2502\n",
      "Loss: tensor(0.1079, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2503\n",
      "Loss: tensor(0.1078, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2504\n",
      "Loss: tensor(0.1078, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2505\n",
      "Loss: tensor(0.1078, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2506\n",
      "Loss: tensor(0.1078, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2507\n",
      "Loss: tensor(0.1078, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2508\n",
      "Loss: tensor(0.1078, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2509\n",
      "Loss: tensor(0.1078, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2510\n",
      "Loss: tensor(0.1078, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2511\n",
      "Loss: tensor(0.1078, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2512\n",
      "Loss: tensor(0.1078, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2513\n",
      "Loss: tensor(0.1078, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2514\n",
      "Loss: tensor(0.1078, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2515\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2516\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2517\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2518\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2519\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2520\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2521\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2522\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2523\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2524\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2525\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2526\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2527\n",
      "Loss: tensor(0.1077, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2528\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2529\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2530\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2531\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2532\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2533\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2534\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2535\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2536\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2537\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2538\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2539\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2540\n",
      "Loss: tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2541\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2542\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2543\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2544\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2545\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2546\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2547\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2548\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2550\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2551\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2552\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2553\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2554\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2555\n",
      "Loss: tensor(0.1075, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2556\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2557\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2558\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2559\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2560\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2561\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2562\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2563\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2564\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2565\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2566\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2567\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2568\n",
      "Loss: tensor(0.1074, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2569\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2570\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2571\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2572\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2573\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2574\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2575\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2576\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2577\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2578\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2579\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2580\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2581\n",
      "Loss: tensor(0.1073, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2582\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2583\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2584\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2585\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2586\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2587\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2588\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2589\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2590\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2591\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2592\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2593\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2594\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2595\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2596\n",
      "Loss: tensor(0.1072, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2597\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2598\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2599\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2600\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2601\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2602\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2603\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2604\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2605\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2606\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2607\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2608\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2609\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2610\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2611\n",
      "Loss: tensor(0.1071, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2612\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2613\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2614\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2615\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2616\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2617\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2618\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2619\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2620\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2621\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2622\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2623\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2624\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2625\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2626\n",
      "Loss: tensor(0.1070, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2627\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2628\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2629\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2630\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2631\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2632\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2633\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2634\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2635\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2636\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2637\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2638\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2639\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2640\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2641\n",
      "Loss: tensor(0.1069, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2642\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2643\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2644\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2645\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2646\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2647\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2648\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2649\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2650\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2651\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2652\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2653\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2654\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2655\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2656\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2657\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2658\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2659\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2660\n",
      "Loss: tensor(0.1068, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2661\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2662\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2663\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2664\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2665\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2666\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2667\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2668\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2670\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2671\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2672\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2673\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2674\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2675\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2676\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2677\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2678\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2679\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2680\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2681\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2682\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2683\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2684\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2685\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2686\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2687\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2688\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2689\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2690\n",
      "Loss: tensor(0.1066, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2691\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2692\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2693\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2694\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2695\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2696\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2697\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2698\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2699\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2700\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2701\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2702\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2703\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2704\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2705\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2706\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2707\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2708\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2709\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2710\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2711\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2712\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2713\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2714\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2715\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2716\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2717\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2718\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2719\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2720\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2721\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2722\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2723\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2724\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2725\n",
      "Loss: tensor(0.1064, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2726\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2727\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2728\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2729\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2730\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2731\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2732\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2733\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2734\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2735\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2736\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2737\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2738\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2739\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2740\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2741\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2742\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2743\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2744\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2745\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2746\n",
      "Loss: tensor(0.1063, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2747\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2748\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2749\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2750\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2751\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2752\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2753\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2754\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2755\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2756\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2757\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2758\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2759\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2760\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2761\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2762\n",
      "Loss: tensor(0.1062, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2763\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2764\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2765\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2766\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2767\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2768\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2769\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2770\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2771\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2772\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2773\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2774\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2775\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2776\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2777\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2778\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2779\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2780\n",
      "Loss: tensor(0.1061, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2781\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2782\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2783\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2784\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2785\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2786\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2787\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2788\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2789\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2790\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2792\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2793\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2794\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2795\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2796\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2797\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2798\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2799\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2800\n",
      "Loss: tensor(0.1060, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2801\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2802\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2803\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2804\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2805\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2806\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2807\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2808\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2809\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2810\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2811\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2812\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2813\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2814\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2815\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2816\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2817\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2818\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2819\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2820\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2821\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2822\n",
      "Loss: tensor(0.1059, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2823\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2824\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2825\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2826\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2827\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2828\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2829\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2830\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2831\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2832\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2833\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2834\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2835\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2836\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2837\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2838\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2839\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2840\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2841\n",
      "Loss: tensor(0.1058, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2842\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2843\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2844\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2845\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2846\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2847\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2848\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2849\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2850\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2851\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2852\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2853\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2854\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2855\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2856\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2857\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2858\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2859\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2860\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2861\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2862\n",
      "Loss: tensor(0.1057, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2863\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2864\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2865\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2866\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2867\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2868\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2869\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2870\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2871\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2872\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2873\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2874\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2875\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2876\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2877\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2878\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2879\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2880\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2881\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2882\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2883\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2884\n",
      "Loss: tensor(0.1056, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2885\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2886\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2887\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2888\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2889\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2890\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2891\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2892\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2893\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2894\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2895\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2896\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2897\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2898\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2899\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2900\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2901\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2902\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2903\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2904\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2905\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2906\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2907\n",
      "Loss: tensor(0.1055, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2908\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2909\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2910\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2911\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2913\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2914\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2915\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2916\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2917\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2918\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2919\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2920\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2921\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2922\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2923\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2924\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2925\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2926\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2927\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2928\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2929\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2930\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2931\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2932\n",
      "Loss: tensor(0.1054, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2933\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2934\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2935\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2936\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2937\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2938\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2939\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2940\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2941\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2942\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2943\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2944\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2945\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2946\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2947\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2948\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2949\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2950\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2951\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2952\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2953\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2954\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2955\n",
      "Loss: tensor(0.1053, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2956\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2957\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2958\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2959\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2960\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2961\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2962\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2963\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2964\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2965\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2966\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2967\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2968\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2969\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2970\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2971\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2972\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2973\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2974\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2975\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2976\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2977\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2978\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2979\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2980\n",
      "Loss: tensor(0.1052, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2981\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2982\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2983\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2984\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2985\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2986\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2987\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2988\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2989\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2990\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2991\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2992\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2993\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2994\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2995\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2996\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2997\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2998\n",
      "Loss: tensor(0.1051, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2999\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3000):  \n",
    "        \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net(input.float())\n",
    "\n",
    "    loss = criterion(output, targets)\n",
    "    print('Loss:', loss, ' at epoch:', epoch)\n",
    "\n",
    "    loss.backward()  #backprop\n",
    "    optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the FCNN model\n",
    "\n",
    "stage='NNetwork6Way/'\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/\"+stage\n",
    "PATH = SavesDirectory+' name of saved model here .pth'\n",
    "\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# more on saving pytorch networks: https://pytorch.org/docs/stable/notes/serialization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load previously saved FCNN model \n",
    "\n",
    "stage='NNetwork6Way/'\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/\"+stage\n",
    "#PATH = SavesDirectory+'Tanh_MSE_adam4271.pth'\n",
    "\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test data\n",
    "\n",
    "TestData=pd.read_excel('testReputation.xlsx' )\n",
    "TestData=TestData.iloc[:,:-2].astype(float)\n",
    "TestData=TestData/200\n",
    "\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Saves/\"\n",
    "TF_Output=pd.read_csv( SavesDirectory+'testOut.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0050,  0.0100,  0.0050,  ...,  0.3123,  0.0958,  0.0531],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.2837,  0.1089,  0.0245],\n",
       "        [ 0.0200,  0.0250,  0.0600,  ...,  0.3047,  0.0793,  0.0184],\n",
       "        ...,\n",
       "        [ 0.0100,  0.0050,  0.0250,  ...,  0.3257,  0.1226,  0.0760],\n",
       "        [ 0.2200,  0.0950,  0.0350,  ...,  0.3313,  0.1141,  0.0054],\n",
       "        [ 0.0050,  0.0600,  0.0100,  ...,  0.3450,  0.1208, -0.0110]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "TestData=pd.concat([TestData,TF_Output], axis=1)\n",
    "TestData=torch.tensor(TestData.values)\n",
    "TestData\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1283 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5\n",
       "0     0  0  0  0  1  0\n",
       "1     0  0  0  1  0  0\n",
       "2     0  1  0  0  0  0\n",
       "3     0  0  0  0  0  1\n",
       "4     0  0  0  0  0  1\n",
       "...  .. .. .. .. .. ..\n",
       "1278  0  1  0  0  0  0\n",
       "1279  0  0  0  0  1  0\n",
       "1280  0  0  1  0  0  0\n",
       "1281  1  0  0  0  0  0\n",
       "1282  0  1  0  0  0  0\n",
       "\n",
       "[1283 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=pd.read_excel('testReputation.xlsx' )\n",
    "labels=labels.iloc[:,-1] \n",
    "labelsOneHot=pd.get_dummies(labels)\n",
    "labelsOneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestLables =torch.tensor(labelsOneHot.values)\n",
    "TestLables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 3 1 3 3 4 4 4 4 4 3 3 5 3 3 1 3 3 4 4 4 3 3 3 4 3 4 3 4 4 3 0 4 4 3 4 4 4 4 1 3 4 1 4 4 3 4 4 4 4 3 3 3 3 4 5 2 4 3 3 3 4 4 1 4 3 3 5 4 3 1 3 3 3 3 3 3 4 4 3 3 3 4 3 3 4 4 3 4 4 4 3 1 1 4 4 3 3 1 4 0 4 3 3 1 3 3 3 3 1 4 3 3 3 1 0 4 3 2 2 4 4 3 1 1 1 1 3 1 3 3 3 1 3 2 0 4 2 3 4 1 5 3 3 4 4 3 4 1 0 1 1 3 5 1 4 3 1 3 3 1 4 4 3 5 4 3 3 1 1 3 3 5 3 1 3 4 0 2 3 4 4 1 1 4 2 4 1 3 3 3 4 0 3 3 2 3 3 1 1 4 5 3 5 3 3 5 1 3 3 4 4 1 4 3 3 3 3 1 4 3 4 3 1 3 3 1 2 0 4 1 0 1 1 4 5 0 4 2 2 1 1 4 4 4 2 3 4 3 0 4 5 4 3 3 4 0 4 1 3 1 3 1 0 3 1 3 4 0 3 4 1 2 5 2 5 3 1 5 3 1 5 3 1 5 3 3 3 0 3 3 3 4 1 1 3 4 2 3 1 4 5 3 5 3 2 5 3 3 1 4 1 2 3 5 5 3 3 3 5 1 3 1 3 0 5 1 5 4 1 3 3 3 5 1 1 3 3 2 4 1 3 4 3 5 2 4 5 2 1 0 4 1 4 3 3 3 1 2 1 1 1 1 2 0 4 4 2 1 0 1 3 5 4 3 4 4 3 1 3 1 4 4 3 3 3 4 3 4 4 1 3 1 1 3 3 3 3 4 4 3 0 3 4 5 2 4 3 3 3 1 4 3 2 4 1 3 3 2 5 4 4 1 1 3 4 3 4 3 4 1 3 3 2 4 4 3 4 1 5 4 1 1 3 1 3 2 1 3 1 3 1 1 4 3 4 3 1 1 5 1 3 4 4 4 1 3 1 3 3 4 1 0 3 3 3 1 3 3 2 3 3 3 3 1 2 0 3 1 3 3 3 3 3 3 3 4 3 4 3 0 3 3 3 3 5 0 3 4 1 5 2 3 1 5 3 3 1 3 1 3 2 1 1 4 5 3 1 5 1 0 0 3 1 4 3 1 3 5 3 5 4 2 1 3 0 5 3 1 1 3 1 2 3 3 4 2 2 1 3 5 4 1 0 1 3 4 1 3 4 3 5 0 2 3 3 4 3 4 2 3 3 1 2 3 2 3 3 3 5 1 3 1 2 3 3 4 5 3 5 5 1 4 3 3 2 1 4 4 1 5 4 3 4 5 4 3 3 4 2 2 3 1 2 4 4 5 4 0 1 2 3 5 3 1 4 4 2 3 2 3 3 3 3 4 1 1 3 3 4 2 3 2 1 4 1 3 4 3 4 2 4 5 3 1 4 3 3 0 5 5 3 3 2 5 3 1 3 3 2 3 5 3 4 1 3 3 2 4 3 4 3 4 3 4 4 4 3 0 4 3 4 4 5 3 2 3 1 2 4 4 3 4 3 4 5 1 1 4 1 4 4 1 5 5 5 4 3 2 3 2 3 4 4 3 3 4 0 1 3 4 4 3 4 2 5 0 3 0 3 4 4 3 3 5 5 2 1 3 2 5 0 4 1 4 1 1 4 2 2 3 4 4 1 2 1 5 4 4 4 3 5 1 0 5 1 1 1 4 4 4 5 5 5 3 1 2 5 3 4 4 1 1 5 1 0 3 3 5 4 1 1 4 3 1 5 4 0 4 1 5 3 4 3 3 3 4 3 4 4 2 5 5 4 1 4 5 4 2 4 4 3 5 4 3 4 0 3 5 0 3 3 0 4 4 3 3 0 3 4 5 5 2 2 1 3 3 0 0 4 5 1 4 0 1 3 0 3 1 5 1 1 3 4 2 5 4 3 5 4 2 1 4 4 3 3 1 3 2 3 4 1 3 1 1 4 1 5 5 2 0 3 1 1 4 4 0 0 2 3 3 1 3 3 5 3 1 0 2 2 3 5 2 1 3 3 1 1 1 4 4 1 4 4 3 3 3 0 5 3 3 1 1 3 3 1 4 1 5 0 3 3 3 5 5 3 2 3 1 4 1 0 4 4 4 4 4 4 5 5 2 3 4 4 4 3 3 4 3 3 1 3 4 3 4 3 3 5 5 1 4 5 1 1 3 0 1 3 2 3 1 2 3 3 4 1 3 5 3 4 0 5 4 5 1 4 3 2 0 4 4 1 0 4 4 4 3 4 4 4 3 4 1 5 4 5 3 0 1 1 0 4 3 4 1 4 4 0 3 2 0 4 4 3 0 2 1 3 4 3 3 3 4 4 0 3 5 1 4 0 3 3 4 3 3 1 4 1 3 4 3 4 0 2 2 4 1 3 1 1 4 4 1 1 4 3 1 1 4 5 3 0 1 3 2 1 5 2 1 3 3 4 5 1 2 3 3 3 1 4 5 3 1 3 1 1 4 2 5 4 3 2 3 2 4 3 1 0 3 5 3 1 4 1 4 4 4 4 4 1 5 3 2 2 4 1 1 5 4 3 3 4 1 4 2 0 2 2 1 4 4 1 1 3 0 3 4 2 4 2 1 4 4 1 0 4 3 3 3 1 1 4 4 1 1 1 3 1 1 5 1 4 2 0 1 4 5 5 1 2 1 1 1 1 2 4 1 3 1 4 1 4 2 1 0 2 1 4 3 4 1 4 1 4 4 4 3 4 3 2 4 4 4 1 4 4 1 3 1 4 1 1 2 3 4 4 1 3 3 1 4 1 4 0 0 1 2 4 4 2 0 1 Correct: 573 out of: 1283\n",
      "Accuracy of the network :  44.66095089633671\n"
     ]
    }
   ],
   "source": [
    " correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "Y=[]  #target\n",
    "Pred=[]  #predicted\n",
    "\n",
    "with torch.no_grad():\n",
    "    for row in range(len(TestData)):\n",
    "        outputs = net(TestData[row,:].float())\n",
    "        result=0\n",
    "        total+=1\n",
    "        if outputs[0]<outputs[1]:result=1\n",
    "        if outputs[result]<outputs[2]:result=2\n",
    "        if outputs[result]<outputs[3]:result=3\n",
    "        if outputs[result]<outputs[4]:result=4\n",
    "        if outputs[result]<outputs[5]:result=5\n",
    "        \n",
    "        if labelsOneHot.iloc[row,result]==1: correct+=1\n",
    "        \n",
    "        Y.append(result)\n",
    "        Pred.append(labels.iloc[row])\n",
    "        \n",
    "        print(result, end=' ')\n",
    "        \n",
    "       \n",
    "print('Correct:', correct, 'out of:', total )\n",
    "print('Accuracy of the network : ',( 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 48  14   7   1   2   4]\n",
      " [ 21 115  36  35  28  25]\n",
      " [  3  16  68   7   8   6]\n",
      " [ 12  66  61 141  69  49]\n",
      " [  6  33  32  69 129  55]\n",
      " [  2   6  10  14  13  72]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "print(metrics.confusion_matrix(Y,Pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Pants       0.52      0.63      0.57        76\n",
      "       False       0.46      0.44      0.45       260\n",
      " Barely-True       0.32      0.63      0.42       108\n",
      "   Hlaf-True       0.53      0.35      0.42       398\n",
      " Mostly-True       0.52      0.40      0.45       324\n",
      "        True       0.34      0.62      0.44       117\n",
      "\n",
      "    accuracy                           0.45      1283\n",
      "   macro avg       0.45      0.51      0.46      1283\n",
      "weighted avg       0.48      0.45      0.45      1283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Pants', 'False', 'Barely-True','Hlaf-True','Mostly-True','True']\n",
    "\n",
    "print(metrics.classification_report(Y, Pred,target_names =target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What this file is for\n",
    "\n",
    "It is possible to capture the hidden states of a transformer at any layer.  The ones captured here are a 796 dimensional embedding at the output,  representing each token in a statement.\n",
    "\n",
    "Classification using these was attempted but was not successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedure for getting the data sets and formatting them for the transformer\n",
    " \n",
    "\n",
    "def prepareDataset( filename):\n",
    "     \n",
    "    ReadSet=pd.read_excel(filename )\n",
    "\n",
    "    ReadSet['text']=ReadSet['Statement']\n",
    "    ReadSet['labels']=ReadSet['Label']\n",
    "    \n",
    "    ReadSet=ReadSet.drop(['ID','Label','Statement','Subject','Speaker','Job','From','Affiliation','PantsTotal','NotRealTotal','BarelyTotal','HalfTotal','MostlyTotal','RealTotal','Context'],axis=1)\n",
    "     \n",
    "    return ReadSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>President Obama is a Muslim.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An independent payment advisory board created ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. Sen. Bill Nelson was the deciding vote fo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Large phone companies and their trade associat...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RIPTA has really some of the fullest buses for...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10261</th>\n",
       "      <td>The Georgia Dome has returned $10 billion in e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10262</th>\n",
       "      <td>Then-Gov. Carl Sanders put 56 percent of the s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10263</th>\n",
       "      <td>Nathan Deal saved the HOPE scholarship program.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10264</th>\n",
       "      <td>John Faso took money from fossil fuel companie...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10265</th>\n",
       "      <td>With the exception of slavery and the Chinese ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10266 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  labels\n",
       "0                           President Obama is a Muslim.       0\n",
       "1      An independent payment advisory board created ...       0\n",
       "2      U.S. Sen. Bill Nelson was the deciding vote fo...       2\n",
       "3      Large phone companies and their trade associat...       4\n",
       "4      RIPTA has really some of the fullest buses for...       4\n",
       "...                                                  ...     ...\n",
       "10261  The Georgia Dome has returned $10 billion in e...       1\n",
       "10262  Then-Gov. Carl Sanders put 56 percent of the s...       4\n",
       "10263    Nathan Deal saved the HOPE scholarship program.       4\n",
       "10264  John Faso took money from fossil fuel companie...       3\n",
       "10265  With the exception of slavery and the Chinese ...       4\n",
       "\n",
       "[10266 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the training dataset\n",
    "train=prepareDataset( 'train-clean.xlsx')\n",
    "# and display for inspecting\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Jerseys once-broken pension system is now ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The new health care law will cut $500 billion ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For thousands of public employees, Wisconsin G...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Because as a Senator Toomey stood up for Wall ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The governors budget proposal reduces the stat...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>You can import as many hemp products into this...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>Says when Republicans took over the state legi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>Wisconsin's laws ranked the worst in the world...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>There currently are 825,000 student stations s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>Black people are eight times more likely to be...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1284 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "0     New Jerseys once-broken pension system is now ...       3\n",
       "1     The new health care law will cut $500 billion ...       2\n",
       "2     For thousands of public employees, Wisconsin G...       3\n",
       "3     Because as a Senator Toomey stood up for Wall ...       4\n",
       "4     The governors budget proposal reduces the stat...       5\n",
       "...                                                 ...     ...\n",
       "1279  You can import as many hemp products into this...       5\n",
       "1280  Says when Republicans took over the state legi...       3\n",
       "1281  Wisconsin's laws ranked the worst in the world...       2\n",
       "1282  There currently are 825,000 student stations s...       4\n",
       "1283  Black people are eight times more likely to be...       3\n",
       "\n",
       "[1284 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the evaluation/validation dataset\n",
    "Eval=prepareDataset('valid-clean.xlsx')\n",
    "# and display for inspecting\n",
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a lawsuit between private citizens, a Flori...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Obama-Nelson economic record: Job creation ......</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Says George LeMieux even compared Marco Rubio ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gene Green is the NRAs favorite Democrat in Co...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In labor negotiations with city employees, Mil...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>Says Milwaukee County Executive Chris Abele sp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>The words subhuman mongrel, which Ted Nugent c...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>California's Prop 55 prevents $4 billion in ne...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>Says One of the states largest governments mad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>Expanding the sale of full-strength beer and w...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1282 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "0     In a lawsuit between private citizens, a Flori...       4\n",
       "1     Obama-Nelson economic record: Job creation ......       4\n",
       "2     Says George LeMieux even compared Marco Rubio ...       2\n",
       "3     Gene Green is the NRAs favorite Democrat in Co...       2\n",
       "4     In labor negotiations with city employees, Mil...       2\n",
       "...                                                 ...     ...\n",
       "1277  Says Milwaukee County Executive Chris Abele sp...       1\n",
       "1278  The words subhuman mongrel, which Ted Nugent c...       5\n",
       "1279  California's Prop 55 prevents $4 billion in ne...       2\n",
       "1280  Says One of the states largest governments mad...       0\n",
       "1281  Expanding the sale of full-strength beer and w...       3\n",
       "\n",
       "[1282 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the test set dataset\n",
    "test=prepareDataset('test-clean.xlsx')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Capturing the hidden layers \n",
    "\n",
    "The simplest way to do this is with the Huggingface Transformers library\n",
    "https://huggingface.co/transformers/model_doc/bert.html\n",
    "\n",
    "At this point you should terminate this notebook and start running the remaining steps from this point\n",
    "\n",
    "##### Using  BertModel\n",
    "The sentence vector can be capture with outputs[1]\n",
    "\n",
    "The 1X6 classification vector can be capture with outputs[0]\n",
    "\n",
    "adding output_hidden_states=True  allows you to capture all hidden states in output[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class='bert'  # bert or roberta or albert\n",
    "model_version='bert-base-cased' #bert-base-cased, roberta-base, roberta-large, albert-base-v2 OR albert-large-v2\n",
    "output_folder='./TunedModels/'+model_class+'/'+model_version+\"/\"\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "CheckPoint='checkpoint-161-epoch-4'  #epoch 2\n",
    "preSavedCheckpoint=output_folder+CheckPoint\n",
    "model =BertModel.from_pretrained(preSavedCheckpoint) # ,output_hidden_states=True  if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(preSavedCheckpoint)\n",
    "\n",
    "#for testing it (uncomment the following 2 lines)\n",
    "#tokens_tensor = torch.tensor(tokenizer.encode(\"Hello, my dog is very cute\")).unsqueeze(0)\n",
    "#tokens_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# place model in evaluation mode\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "# it deactivates the DropOut modules\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2796, -0.6438,  0.0258,  ..., -0.0553,  0.6021,  0.4983],\n",
      "         [-0.1477, -0.1678,  0.7997,  ..., -0.1568,  0.7950,  0.6561],\n",
      "         [ 0.3069,  0.5455,  1.1903,  ..., -0.1290, -0.0614,  0.7561],\n",
      "         ...,\n",
      "         [-0.3995,  0.2563,  0.1365,  ...,  0.7624,  0.4063,  0.4120],\n",
      "         [ 0.1616, -0.3017,  0.4328,  ..., -0.3868,  0.6118,  0.2967],\n",
      "         [ 0.4040, -0.6882,  0.8193,  ..., -0.3489,  0.5836, -0.0777]]])\n",
      "_________________________\n",
      "torch.Size([1, 9, 768])\n",
      "_____ Sentence Vector_____\n",
      "tensor([[-0.3891,  0.2266,  0.9752, -0.6544,  0.1215, -0.5357,  0.6574,  0.0100,\n",
      "         -0.3325, -0.0315, -0.0804,  0.8440, -0.2456, -0.8986, -0.5873, -0.2237,\n",
      "          0.1653,  0.1258, -0.9787,  0.0976, -0.3274, -0.9096,  0.5075, -0.1409,\n",
      "          0.2600, -0.0811,  0.1712,  0.9795, -0.2689,  0.9629,  0.1324, -0.6161,\n",
      "         -0.6890, -0.8535,  0.1986,  0.3653, -0.9317, -0.2481,  0.3966, -0.4678,\n",
      "         -0.1731,  0.6675,  0.0152, -0.2752,  0.1034, -0.1125,  0.0279, -0.2501,\n",
      "         -0.0994,  0.9963,  0.2000,  0.9803,  0.0725,  0.2534,  0.4116,  0.0374,\n",
      "          0.7587, -0.0505, -0.1677, -0.1025,  0.2396,  0.0030, -0.0055,  0.5125,\n",
      "         -0.5816, -0.4119, -0.4563,  0.3967,  0.4208,  0.0729, -0.5572,  0.2513,\n",
      "          0.3005, -0.2370,  0.0383, -0.1376,  0.8692, -0.9686, -0.6937,  0.9775,\n",
      "         -0.5310, -0.9458,  0.2036, -0.1106,  0.4908, -0.7664, -0.1444, -0.8464,\n",
      "         -0.0629,  0.5422,  0.1863, -0.2630, -0.2358, -0.2511,  0.9761,  0.6077,\n",
      "         -0.1300,  0.2888, -0.6352, -0.3125,  0.2546, -0.7039,  0.2003, -0.0190,\n",
      "          0.7025, -0.3064, -0.6459, -0.0703, -0.8528,  0.3592, -0.2075, -0.6299,\n",
      "          0.3148,  0.0886,  0.5911, -0.6988,  0.3510,  0.6487, -0.3518,  0.9858,\n",
      "         -0.3765, -0.6416,  0.7933, -0.8080,  0.6542,  0.2274,  0.0436, -0.0875,\n",
      "         -0.4023,  0.6287,  0.3308, -0.0232, -0.1206,  0.7611, -0.4230,  0.9848,\n",
      "         -0.8453,  0.3220, -0.9827, -0.0966,  0.9228, -0.3688, -0.0623,  0.2514,\n",
      "          0.4718,  0.3228,  0.7440,  0.6020,  0.6314,  0.7190,  0.3842,  0.0010,\n",
      "          0.3180,  0.8818,  0.3769, -0.0984,  0.0475,  0.0167,  0.6098, -0.7094,\n",
      "          0.1436, -0.8874, -0.4457, -0.1518,  0.9129,  0.2176, -0.7501,  0.1123,\n",
      "          0.9077,  0.3416,  0.6828, -0.6565, -0.2355, -0.2463,  0.0421,  0.5156,\n",
      "         -0.1559,  0.9224, -0.1516,  0.9868,  0.9909, -0.0902,  0.2457,  0.2849,\n",
      "         -0.7592,  0.0099, -0.6357,  0.4218, -0.6567,  0.7574,  0.4329,  0.7053,\n",
      "          0.2102, -0.3309, -0.9080, -0.3126,  0.2772, -0.3058,  0.9840,  0.8464,\n",
      "         -0.8985, -0.1391, -0.7692,  0.5900,  0.2591,  0.4377,  0.5552, -0.2936,\n",
      "         -0.5668, -0.9843,  0.0848, -0.0029, -0.3097, -0.2916,  0.2137,  0.2752,\n",
      "         -0.0763, -0.0358,  0.4219,  0.9823, -0.8694,  0.2212,  0.1101, -0.8932,\n",
      "         -0.6998,  0.3455, -0.1444,  0.6565, -0.1558, -0.9002, -0.1367, -0.3971,\n",
      "          0.4462,  0.6367,  0.7486, -0.9546,  0.2012,  0.2691,  0.4834, -0.0916,\n",
      "          0.3921, -0.0325,  0.0211, -0.1630,  0.0540, -0.3954,  0.3492, -0.1980,\n",
      "          0.8362, -0.3479,  0.0852,  0.0848, -0.9007,  0.8844,  0.6557, -0.1648,\n",
      "         -0.2600, -0.6507, -0.7802, -0.3683, -0.4335, -0.9189,  0.4907, -0.6720,\n",
      "          0.8938, -0.3653,  0.6585,  0.1013,  0.7572, -0.1637, -0.8996,  0.2353,\n",
      "         -0.2147,  0.6644, -0.9190,  0.8925, -0.1904, -0.5921, -0.5815, -0.9509,\n",
      "          0.0501,  0.9903, -0.5266, -0.1273,  0.9701,  0.2318,  0.4151, -0.2783,\n",
      "         -0.0170, -0.8599, -0.4058,  0.5338, -0.0290, -0.0039, -0.0337,  0.0392,\n",
      "          0.9379,  0.4383,  0.0433, -0.3890, -0.1148, -0.3071, -0.9874,  0.3878,\n",
      "         -0.0354, -0.9668,  0.9112, -0.4664,  0.9690, -0.5013,  0.3395,  0.1042,\n",
      "         -0.1225,  0.4534,  0.0061,  0.9505,  0.2420, -0.1246,  0.2662, -0.4894,\n",
      "          0.1613, -0.5565,  0.1227,  0.1066,  0.2902, -0.3282,  0.3109, -0.8726,\n",
      "         -0.4051,  0.4424, -0.2501, -0.2678,  0.6019,  0.3819,  0.3334, -0.0932,\n",
      "          0.1761, -0.1231, -0.9886,  0.5285, -0.0087, -0.1914, -0.3531,  0.2036,\n",
      "          0.3975, -0.1551,  0.8021,  0.2356, -0.3574, -0.4088,  0.9918, -0.7119,\n",
      "          0.1729,  0.2595,  0.8820,  0.6918,  0.4314, -0.2210, -0.6298,  0.8785,\n",
      "         -0.4317, -0.3078,  0.3122,  0.6206, -0.3080, -0.0787,  0.6577,  0.8499,\n",
      "         -0.2312,  0.3042, -0.9654, -0.1752,  0.3167,  0.7076, -0.5301, -0.1833,\n",
      "          0.1735,  0.3879,  0.0526,  0.2323, -0.0873,  0.1624,  0.2056,  0.0036,\n",
      "          0.2430,  0.9396, -0.4485,  0.4591,  0.5704, -0.2338, -0.6065,  0.3157,\n",
      "          0.3583, -0.4041,  0.0233, -0.8841, -0.3136, -0.8034,  0.7232,  0.0490,\n",
      "         -0.3140,  0.2414, -0.1565,  0.0456,  0.2650, -0.4601,  0.4559, -0.3812,\n",
      "          0.0373, -0.3168,  0.3112,  0.1024, -0.2675,  0.3174, -0.1861,  0.9843,\n",
      "          0.6193, -0.9717, -0.1833,  0.7116, -0.9816, -0.7833, -0.7746,  0.1247,\n",
      "         -0.5748,  0.1120, -0.0845, -0.5344, -0.8300, -0.4713, -0.2379, -0.1728,\n",
      "         -0.9209,  0.9836, -0.2765, -0.7760, -0.1177,  0.2588, -0.7032,  0.1343,\n",
      "         -0.4913, -0.9784, -0.0732,  0.5351, -0.1460,  0.3905, -0.4460,  0.9692,\n",
      "          0.2205,  0.1704,  0.2029,  0.7271,  0.1981, -0.9827,  0.5830,  0.9172,\n",
      "          0.1735,  0.5748, -0.1900,  0.9522, -0.8140,  0.4398,  0.1077, -0.9202,\n",
      "         -0.3268,  0.4849, -0.0018, -0.4036, -0.0848, -0.3628, -0.9510, -0.0675,\n",
      "          0.0749,  0.4093,  0.7513, -0.6268, -0.1630, -0.6475, -0.0715, -0.9532,\n",
      "         -0.0388, -0.7048, -0.1787,  0.4549,  0.4956,  0.3115,  0.2011, -0.5750,\n",
      "         -0.7279,  0.1970,  0.3899,  0.4829,  0.1065,  0.2043, -0.0568, -0.4561,\n",
      "         -0.3743,  0.8340, -0.5564, -0.0334, -0.1365,  0.6005, -0.4960,  0.4316,\n",
      "         -0.1537, -0.7969, -0.3594, -0.1220, -0.9574,  0.9740, -0.9683,  0.3800,\n",
      "          0.3405, -0.4213,  0.0081, -0.4549, -0.9680, -0.9544, -0.9900, -0.1742,\n",
      "          0.3094,  0.0769, -0.0626,  0.8762, -0.5373,  0.5355,  0.4924,  0.8263,\n",
      "         -0.3730,  0.9451, -0.3048, -0.8326,  0.4787, -0.9499, -0.6249,  0.0259,\n",
      "          0.6228, -0.0076, -0.2322,  0.9805, -0.9762,  0.5217, -0.9892, -0.4022,\n",
      "          0.9318, -0.3615,  0.7966, -0.9383, -0.6326, -0.7918, -0.0417, -0.3356,\n",
      "          0.0083, -0.9047, -0.8006,  0.9329,  0.8733, -0.0466,  0.0111,  0.5291,\n",
      "          0.7001, -0.1705,  0.1868, -0.5016,  0.2975,  0.9161,  0.6645,  0.8566,\n",
      "         -0.7266,  0.3869,  0.3071, -0.0680,  0.2290, -0.3234, -0.3878,  0.1780,\n",
      "         -0.7628,  0.2422,  0.0054, -0.6645, -0.2139,  0.5497,  0.1742, -0.0779,\n",
      "         -0.0015,  0.3789,  0.0952, -0.9113,  0.1275,  0.3412, -0.2483, -0.1215,\n",
      "          0.5772, -0.3616,  0.9166,  0.9235, -0.9588,  0.1042,  0.3732,  0.1118,\n",
      "          0.3374, -0.3717, -0.0922,  0.6034, -0.0043,  0.6347,  0.3427, -0.2061,\n",
      "          0.4161, -0.5979,  0.8189, -0.0240,  0.0246, -0.5275, -0.6783, -0.0485,\n",
      "         -0.1952, -0.2081, -0.8397, -0.8981, -0.6561,  0.0737,  0.4332,  0.9756,\n",
      "          0.9731,  0.3564, -0.3058, -0.5561, -0.9715, -0.5370,  0.0583, -0.1251,\n",
      "          0.4725,  0.2750, -0.1505, -0.6673, -0.9863,  0.3138,  0.1926,  0.3715,\n",
      "         -0.6165, -0.2717,  0.1388, -0.1317,  0.4989,  0.1028, -0.8439, -0.4731,\n",
      "          0.9532, -0.2122,  0.9868, -0.8540, -0.4513, -0.6994, -0.1401, -0.1064,\n",
      "          0.0654,  0.4513,  0.3156, -0.5343,  0.3038,  0.8786, -0.5212, -0.7489,\n",
      "          0.9400, -0.9059, -0.0365, -0.2283, -0.2250,  0.2178, -0.3291, -0.5995,\n",
      "         -0.0941, -0.2838, -0.6984, -0.7342,  0.6709,  0.0618,  0.0956,  0.1099,\n",
      "         -0.9487,  0.2536,  0.5918,  0.9858, -0.9463, -0.2944,  0.1120,  0.8964,\n",
      "          0.1235, -0.3943, -0.5509,  0.9117,  0.2309, -0.4981, -0.1398,  0.0893,\n",
      "          0.5972, -0.8650,  0.6061,  0.8119,  0.4484, -0.3945, -0.9688,  0.9165,\n",
      "          0.0784,  0.7546, -0.0610,  0.6313, -0.0240, -0.1364,  0.0174, -0.2022,\n",
      "         -0.9738,  0.3712, -0.9869, -0.2250,  0.1481,  0.3325, -0.9357, -0.4615,\n",
      "         -0.0122, -0.9875, -0.3867,  0.0527,  0.7073, -0.3529,  0.0414,  0.4597,\n",
      "          0.8891,  0.1664,  0.1577, -0.0437, -0.8487,  0.8490,  0.3308,  0.1560,\n",
      "         -0.2201,  0.2153,  0.7807, -0.5284, -0.4981, -0.4710, -0.0970,  0.8271,\n",
      "          0.5258, -0.9361, -0.6699, -0.2877, -0.5522,  0.4444, -0.3760, -0.9538,\n",
      "         -0.9582,  0.3066,  0.0319,  0.6417, -0.0499,  0.9797, -0.2182,  0.4035,\n",
      "         -0.5611,  0.1489, -0.5117, -0.6153, -0.3687,  0.9818,  0.3358,  0.7468]])\n",
      "_________________________\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# use this to check that the model is working\n",
    "\n",
    "with torch.no_grad():\n",
    "        tokens_tensor = torch.tensor(tokenizer.encode(\"Hello, my dog is very cute\")).unsqueeze(0)\n",
    "        outputs = model(tokens_tensor)\n",
    "        classification = outputs[0]\n",
    "        sentenceVector = outputs[1]\n",
    "\n",
    "\n",
    "print(classification)\n",
    "print('_________________________')\n",
    "print(classification.size())\n",
    "print('_____ Sentence Vector_____')\n",
    "print(sentenceVector)\n",
    "print('_________________________')\n",
    "print(sentenceVector.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.38907713,  0.22658618,  0.9751551 , -0.6543652 ,  0.12148222,\n",
       "        -0.5356933 ,  0.6574321 ,  0.01001288, -0.3325216 , -0.03150015,\n",
       "        -0.08035869,  0.84399843, -0.24559331, -0.8986449 , -0.587316  ,\n",
       "        -0.22366308,  0.16534932,  0.12576467, -0.97867686,  0.09760844,\n",
       "        -0.32740077, -0.9095873 ,  0.5075457 , -0.14091869,  0.2600149 ,\n",
       "        -0.08106023,  0.17119773,  0.9795097 , -0.26889402,  0.9629115 ,\n",
       "         0.13242915, -0.616064  , -0.6890336 , -0.8535495 ,  0.19860597,\n",
       "         0.36531705, -0.93170583, -0.24810933,  0.39660627, -0.4678403 ,\n",
       "        -0.17307971,  0.66752434,  0.01520166, -0.27515113,  0.10336161,\n",
       "        -0.1125436 ,  0.02793297, -0.25013205, -0.09938671,  0.9962989 ,\n",
       "         0.20001042,  0.9803463 ,  0.07254764,  0.25343397,  0.4116171 ,\n",
       "         0.03744793,  0.7586972 , -0.05051298, -0.16765298, -0.10250951,\n",
       "         0.23956813,  0.00301847, -0.00553937,  0.51250166, -0.58163184,\n",
       "        -0.41187403, -0.45626318,  0.39665696,  0.42079988,  0.07291493,\n",
       "        -0.55722517,  0.25128725,  0.30054033, -0.23695324,  0.03829913,\n",
       "        -0.13762687,  0.8692395 , -0.9685981 , -0.6937077 ,  0.97750574,\n",
       "        -0.5309656 , -0.94575626,  0.20355129, -0.11059942,  0.49079996,\n",
       "        -0.76642174, -0.1443818 , -0.84643596, -0.06292923,  0.54221183,\n",
       "         0.18627466, -0.26300612, -0.23584755, -0.25112757,  0.97612894,\n",
       "         0.60771847, -0.1299582 ,  0.28880492, -0.63518816, -0.31253484,\n",
       "         0.2546458 , -0.7038558 ,  0.20032643, -0.01901478,  0.7025222 ,\n",
       "        -0.30639428, -0.6459075 , -0.07030826, -0.85276437,  0.35921222,\n",
       "        -0.20752372, -0.62993425,  0.31480074,  0.08863047,  0.5910776 ,\n",
       "        -0.6987786 ,  0.35100052,  0.6486783 , -0.3517903 ,  0.9858442 ,\n",
       "        -0.37645188, -0.6416198 ,  0.79328376, -0.8079715 ,  0.6541612 ,\n",
       "         0.22736137,  0.04364171, -0.08746336, -0.40229106,  0.6286798 ,\n",
       "         0.3308282 , -0.02316797, -0.12060405,  0.7611389 , -0.4230223 ,\n",
       "         0.98480994, -0.8452571 ,  0.32200825, -0.9827033 , -0.09658274,\n",
       "         0.9227864 , -0.36884478, -0.06232834,  0.251362  ,  0.47177935,\n",
       "         0.32276395,  0.7440484 ,  0.60197234,  0.63143355,  0.7190211 ,\n",
       "         0.38423088,  0.0010145 ,  0.31799835,  0.88184243,  0.37691563,\n",
       "        -0.09843589,  0.04749061,  0.01671544,  0.609759  , -0.70938706,\n",
       "         0.14356512, -0.88744473, -0.44569495, -0.15184271,  0.91294247,\n",
       "         0.21758667, -0.7500974 ,  0.11229917,  0.9076846 ,  0.341612  ,\n",
       "         0.6828371 , -0.65647525, -0.23545228, -0.2463443 ,  0.0420825 ,\n",
       "         0.51562816, -0.15588158,  0.92240375, -0.1515859 ,  0.9868424 ,\n",
       "         0.9909235 , -0.09020478,  0.24573854,  0.28494564, -0.7591875 ,\n",
       "         0.00990393, -0.6357318 ,  0.42183423, -0.6566979 ,  0.7574314 ,\n",
       "         0.4329101 ,  0.7053283 ,  0.21018507, -0.33088806, -0.907964  ,\n",
       "        -0.3125978 ,  0.27719986, -0.30576104,  0.98403585,  0.846385  ,\n",
       "        -0.8985221 , -0.13910758, -0.76921165,  0.59001696,  0.259064  ,\n",
       "         0.43767154,  0.55520326, -0.29357672, -0.56683207, -0.98433274,\n",
       "         0.08478633, -0.00289117, -0.30973628, -0.2916463 ,  0.213678  ,\n",
       "         0.2751977 , -0.07625935, -0.03577059,  0.4219031 ,  0.98233324,\n",
       "        -0.8694039 ,  0.2211822 ,  0.11010676, -0.8931769 , -0.6998187 ,\n",
       "         0.34545898, -0.14443026,  0.65652525, -0.15578365, -0.9002294 ,\n",
       "        -0.13673174, -0.39710903,  0.44623446,  0.6366623 ,  0.74857754,\n",
       "        -0.9545964 ,  0.20119175,  0.26913896,  0.48339936, -0.09162907,\n",
       "         0.39208272, -0.03247353,  0.02109877, -0.16303886,  0.05398357,\n",
       "        -0.39537156,  0.34921998, -0.19801892,  0.836179  , -0.3479114 ,\n",
       "         0.08523204,  0.08483906, -0.9007434 ,  0.88442713,  0.65572304,\n",
       "        -0.16482651, -0.2599755 , -0.65067434, -0.78016245, -0.3682764 ,\n",
       "        -0.4334599 , -0.9189328 ,  0.49073604, -0.67203885,  0.89376813,\n",
       "        -0.36527765,  0.65854144,  0.10126995,  0.75724965, -0.1636625 ,\n",
       "        -0.89957845,  0.23527902, -0.21471038,  0.66443634, -0.91902256,\n",
       "         0.89252913, -0.19043581, -0.5921332 , -0.58145016, -0.95094746,\n",
       "         0.05012368,  0.9903313 , -0.5266478 , -0.12733363,  0.9700715 ,\n",
       "         0.23178807,  0.415061  , -0.27828354, -0.01695935, -0.85990775,\n",
       "        -0.4057846 ,  0.53382343, -0.02902136, -0.00386422, -0.03373177,\n",
       "         0.03919309,  0.9378624 ,  0.43829304,  0.04329848, -0.38898098,\n",
       "        -0.11477669, -0.30709523, -0.98736   ,  0.38777304, -0.03539318,\n",
       "        -0.9667627 ,  0.91115177, -0.46639633,  0.968964  , -0.5012752 ,\n",
       "         0.33951846,  0.10418787, -0.12248234,  0.45338932,  0.00612282,\n",
       "         0.9504764 ,  0.2420464 , -0.12463825,  0.26621142, -0.48936   ,\n",
       "         0.1612525 , -0.55646026,  0.12271274,  0.10657211,  0.29021016,\n",
       "        -0.3281969 ,  0.31088087, -0.87263423, -0.4051034 ,  0.44238654,\n",
       "        -0.2500644 , -0.2677702 ,  0.60194504,  0.38185945,  0.33336982,\n",
       "        -0.09322599,  0.1761447 , -0.12311605, -0.98859525,  0.52846473,\n",
       "        -0.00874548, -0.19139585, -0.35305098,  0.20355925,  0.3974726 ,\n",
       "        -0.15511885,  0.8021038 ,  0.23564398, -0.35737368, -0.4088277 ,\n",
       "         0.99175334, -0.71192706,  0.1728741 ,  0.2595428 ,  0.8819664 ,\n",
       "         0.69179976,  0.4314367 , -0.22103854, -0.6297819 ,  0.8785055 ,\n",
       "        -0.43171367, -0.30777735,  0.3121552 ,  0.62057793, -0.3079806 ,\n",
       "        -0.07870336,  0.65766597,  0.8499389 , -0.231206  ,  0.3042006 ,\n",
       "        -0.9654221 , -0.17518343,  0.31671342,  0.70756024, -0.5301224 ,\n",
       "        -0.18325189,  0.17345078,  0.38792527,  0.05260386,  0.23227134,\n",
       "        -0.08730713,  0.16238983,  0.20557159,  0.00356903,  0.24300115,\n",
       "         0.939638  , -0.44849798,  0.45912093,  0.5703953 , -0.23380111,\n",
       "        -0.6064871 ,  0.31569147,  0.35831693, -0.40410838,  0.02332704,\n",
       "        -0.8841031 , -0.31361976, -0.8033657 ,  0.7231659 ,  0.04901194,\n",
       "        -0.31397837,  0.24144678, -0.15647353,  0.04564249,  0.2649649 ,\n",
       "        -0.46011198,  0.45590475, -0.38123628,  0.03726682, -0.31681025,\n",
       "         0.3111885 ,  0.10241341, -0.26747298,  0.31743118, -0.18614504,\n",
       "         0.9842694 ,  0.6192876 , -0.97165865, -0.18333875,  0.71161354,\n",
       "        -0.981596  , -0.783288  , -0.7746327 ,  0.12469825, -0.5747775 ,\n",
       "         0.11197337, -0.08450027, -0.53438073, -0.8300416 , -0.47131124,\n",
       "        -0.23787786, -0.17276111, -0.9209155 ,  0.98356426, -0.2764675 ,\n",
       "        -0.77604735, -0.11774075,  0.25878704, -0.70319694,  0.13428439,\n",
       "        -0.49125656, -0.97837985, -0.07321638,  0.5350936 , -0.14597254,\n",
       "         0.39046684, -0.44600278,  0.9691587 ,  0.22054332,  0.17042446,\n",
       "         0.20294245,  0.7270765 ,  0.19812065, -0.9826762 ,  0.5829841 ,\n",
       "         0.9172171 ,  0.17351994,  0.5747953 , -0.19002593,  0.95216346,\n",
       "        -0.81396794,  0.4397634 ,  0.10769187, -0.92024976, -0.32681113,\n",
       "         0.48493612, -0.00182486, -0.40361875, -0.08477185, -0.36284426,\n",
       "        -0.95100975, -0.06751851,  0.07493532,  0.40925953,  0.7512943 ,\n",
       "        -0.62678856, -0.16300203, -0.6474689 , -0.07149911, -0.953209  ,\n",
       "        -0.03880573, -0.70475864, -0.17869116,  0.4549322 ,  0.49556214,\n",
       "         0.31147236,  0.2011001 , -0.57504153, -0.72785085,  0.19699757,\n",
       "         0.38989058,  0.48292276,  0.10647796,  0.20432103, -0.05676595,\n",
       "        -0.4560684 , -0.37434208,  0.8339874 , -0.55639917, -0.03335573,\n",
       "        -0.13651592,  0.6004913 , -0.4959896 ,  0.43158114, -0.15367655,\n",
       "        -0.79692173, -0.35937735, -0.12202782, -0.95742774,  0.9739511 ,\n",
       "        -0.9682876 ,  0.3799596 ,  0.3404569 , -0.42130527,  0.0080722 ,\n",
       "        -0.45493975, -0.9680427 , -0.95435214, -0.98997194, -0.17421055,\n",
       "         0.30940744,  0.0769288 , -0.06259479,  0.87615514, -0.5372518 ,\n",
       "         0.5355222 ,  0.49242303,  0.8263249 , -0.37299982,  0.94510955,\n",
       "        -0.30476898, -0.83261985,  0.4786612 , -0.9499255 , -0.6249089 ,\n",
       "         0.02589589,  0.6228081 , -0.00761727, -0.23217385,  0.980511  ,\n",
       "        -0.9762118 ,  0.5216867 , -0.98915654, -0.40217873,  0.93180954,\n",
       "        -0.36149648,  0.7966082 , -0.93827385, -0.6325924 , -0.7918472 ,\n",
       "        -0.04172752, -0.3356467 ,  0.00833308, -0.90473485, -0.80064434,\n",
       "         0.9329096 ,  0.8733034 , -0.04664967,  0.01106344,  0.5291218 ,\n",
       "         0.7000604 , -0.17050168,  0.18675335, -0.5016319 ,  0.29749408,\n",
       "         0.91607696,  0.66452163,  0.85656166, -0.72662586,  0.38686013,\n",
       "         0.3071357 , -0.06804873,  0.22901239, -0.32337195, -0.3877547 ,\n",
       "         0.17800151, -0.7628202 ,  0.24222732,  0.00544937, -0.6644743 ,\n",
       "        -0.21390095,  0.5496607 ,  0.17422374, -0.07794503, -0.00150134,\n",
       "         0.37894902,  0.09516666, -0.9113422 ,  0.1274644 ,  0.3412016 ,\n",
       "        -0.2483014 , -0.12153443,  0.5772278 , -0.361641  ,  0.9165856 ,\n",
       "         0.9234523 , -0.9587969 ,  0.10424085,  0.3732322 ,  0.11180875,\n",
       "         0.3374141 , -0.37171987, -0.09221207,  0.6034112 , -0.00429355,\n",
       "         0.63466066,  0.34272727, -0.2060611 ,  0.41611087, -0.5978745 ,\n",
       "         0.8188578 , -0.0239785 ,  0.02459064, -0.52748394, -0.6783149 ,\n",
       "        -0.048538  , -0.19515176, -0.20813707, -0.83974034, -0.89808065,\n",
       "        -0.65607655,  0.07368171,  0.43317604,  0.9755854 ,  0.9731216 ,\n",
       "         0.35638028, -0.30580223, -0.5561118 , -0.97152406, -0.53703135,\n",
       "         0.0582761 , -0.12510094,  0.47245005,  0.27501416, -0.15054558,\n",
       "        -0.6673329 , -0.9863302 ,  0.31379923,  0.19263518,  0.37153536,\n",
       "        -0.61652446, -0.2717178 ,  0.13880268, -0.1317329 ,  0.49889806,\n",
       "         0.10281729, -0.84390044, -0.47308916,  0.9532075 , -0.2121976 ,\n",
       "         0.98680526, -0.85402554, -0.451321  , -0.699397  , -0.14006987,\n",
       "        -0.10640895,  0.0654198 ,  0.45128542,  0.31555676, -0.5342604 ,\n",
       "         0.30378783,  0.8786068 , -0.52124244, -0.7488774 ,  0.93998   ,\n",
       "        -0.9059167 , -0.03647586, -0.22831997, -0.22502144,  0.21777247,\n",
       "        -0.32909703, -0.59952635, -0.09410586, -0.28380057, -0.6984213 ,\n",
       "        -0.7342052 ,  0.6708883 ,  0.06175353,  0.09560744,  0.10990801,\n",
       "        -0.94865847,  0.25360644,  0.5917803 ,  0.98583806, -0.9462657 ,\n",
       "        -0.29443663,  0.11204542,  0.8963682 ,  0.12351939, -0.39428365,\n",
       "        -0.5508812 ,  0.91168284,  0.23092383, -0.4980701 , -0.13980207,\n",
       "         0.08928804,  0.59717786, -0.8649885 ,  0.6061437 ,  0.8118675 ,\n",
       "         0.44840345, -0.39445937, -0.9688097 ,  0.91648096,  0.07835965,\n",
       "         0.7546314 , -0.06100769,  0.6313444 , -0.02404879, -0.13642316,\n",
       "         0.0173747 , -0.20223612, -0.9738012 ,  0.37122676, -0.9868822 ,\n",
       "        -0.22496119,  0.14807421,  0.33247578, -0.9356845 , -0.4614904 ,\n",
       "        -0.01215332, -0.9874657 , -0.38674566,  0.05274197,  0.7072866 ,\n",
       "        -0.35288507,  0.0413546 ,  0.45969602,  0.88914734,  0.16640681,\n",
       "         0.15765889, -0.04370866, -0.84869695,  0.8489811 ,  0.33082026,\n",
       "         0.1559671 , -0.22010723,  0.21532105,  0.78071195, -0.5283623 ,\n",
       "        -0.4980998 , -0.47100714, -0.0969547 ,  0.8270642 ,  0.5258382 ,\n",
       "        -0.936066  , -0.6698736 , -0.287691  , -0.5522265 ,  0.4444492 ,\n",
       "        -0.3759507 , -0.95375013, -0.95823884,  0.30658543,  0.03192018,\n",
       "         0.64171386, -0.04993   ,  0.9796605 , -0.21815799,  0.40353003,\n",
       "        -0.56110877,  0.14889765, -0.51169324, -0.6153357 , -0.36874244,\n",
       "         0.98179376,  0.33577347,  0.7468131 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceVector.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We capture the vectors of every statement, and save it with this procedure\n",
    "\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Vectors/\"\n",
    "\n",
    "def saveVectors(set,filename):\n",
    "    statementVectors=[] # we collect the sentence vectors in this array\n",
    "    print (\"Fetching Vectors...\", end='')\n",
    "    for row in range(len(set)):\n",
    "        with torch.no_grad():\n",
    "            text=train.iloc[row,0]\n",
    "            tokens_tensor = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "            outputs = model(tokens_tensor)    \n",
    "            sentenceVector = outputs[1]\n",
    "            statementVectors.append(sentenceVector[0].numpy())\n",
    "     \n",
    "    print('Saving...',end='')\n",
    "    fileOut = pd.DataFrame(data= statementVectors)\n",
    "    fileOut.to_csv(SavesDirectory+filename+'.tsv', sep='\\t',  index=False)\n",
    "     \n",
    "    print('Saving Complete!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Vectors...Saving...Saving Complete!\n"
     ]
    }
   ],
   "source": [
    "saveVectors(train,'trainOut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Vectors...Saving...Saving Complete!\n"
     ]
    }
   ],
   "source": [
    "saveVectors(Eval,'evalOut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Vectors...Saving...Saving Complete!\n"
     ]
    }
   ],
   "source": [
    "saveVectors(test,'testOut')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Adding the reputation vector to the statement Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PantsTotal</th>\n",
       "      <th>NotRealTotal</th>\n",
       "      <th>BarelyTotal</th>\n",
       "      <th>HalfTotal</th>\n",
       "      <th>MostlyTotal</th>\n",
       "      <th>RealTotal</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.353628</td>\n",
       "      <td>0.144715</td>\n",
       "      <td>0.958464</td>\n",
       "      <td>-0.681585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463839</td>\n",
       "      <td>0.643878</td>\n",
       "      <td>-0.712667</td>\n",
       "      <td>0.672926</td>\n",
       "      <td>-0.043116</td>\n",
       "      <td>0.436722</td>\n",
       "      <td>-0.138033</td>\n",
       "      <td>0.965976</td>\n",
       "      <td>0.144237</td>\n",
       "      <td>0.848983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144056</td>\n",
       "      <td>-0.214195</td>\n",
       "      <td>0.095764</td>\n",
       "      <td>0.488308</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051924</td>\n",
       "      <td>0.318266</td>\n",
       "      <td>-0.376326</td>\n",
       "      <td>0.485823</td>\n",
       "      <td>-0.274125</td>\n",
       "      <td>-0.069330</td>\n",
       "      <td>0.407660</td>\n",
       "      <td>-0.244027</td>\n",
       "      <td>0.078874</td>\n",
       "      <td>-0.227743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.225200</td>\n",
       "      <td>0.035485</td>\n",
       "      <td>0.773057</td>\n",
       "      <td>-0.188975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115203</td>\n",
       "      <td>0.861145</td>\n",
       "      <td>-0.733366</td>\n",
       "      <td>0.676177</td>\n",
       "      <td>-0.448811</td>\n",
       "      <td>0.543391</td>\n",
       "      <td>0.091833</td>\n",
       "      <td>0.594777</td>\n",
       "      <td>0.602495</td>\n",
       "      <td>0.173886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.309293</td>\n",
       "      <td>-0.111637</td>\n",
       "      <td>0.026551</td>\n",
       "      <td>0.546971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599675</td>\n",
       "      <td>-0.301488</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>0.116902</td>\n",
       "      <td>-0.345542</td>\n",
       "      <td>0.248941</td>\n",
       "      <td>0.455902</td>\n",
       "      <td>-0.292546</td>\n",
       "      <td>-0.026388</td>\n",
       "      <td>-0.631524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175661</td>\n",
       "      <td>0.124550</td>\n",
       "      <td>0.784903</td>\n",
       "      <td>-0.417388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610993</td>\n",
       "      <td>0.133260</td>\n",
       "      <td>-0.258388</td>\n",
       "      <td>0.465152</td>\n",
       "      <td>-0.376480</td>\n",
       "      <td>-0.024658</td>\n",
       "      <td>0.114852</td>\n",
       "      <td>0.673887</td>\n",
       "      <td>0.473460</td>\n",
       "      <td>0.106824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10260</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247382</td>\n",
       "      <td>-0.038540</td>\n",
       "      <td>-0.220711</td>\n",
       "      <td>0.490467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101806</td>\n",
       "      <td>0.142231</td>\n",
       "      <td>0.082679</td>\n",
       "      <td>-0.188128</td>\n",
       "      <td>-0.736837</td>\n",
       "      <td>0.289984</td>\n",
       "      <td>0.260620</td>\n",
       "      <td>-0.663452</td>\n",
       "      <td>-0.147648</td>\n",
       "      <td>-0.314752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10261</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.158101</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>0.912721</td>\n",
       "      <td>-0.635446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594023</td>\n",
       "      <td>0.451269</td>\n",
       "      <td>-0.065867</td>\n",
       "      <td>0.616350</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.076593</td>\n",
       "      <td>0.162441</td>\n",
       "      <td>0.884791</td>\n",
       "      <td>0.198261</td>\n",
       "      <td>0.514505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10262</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047984</td>\n",
       "      <td>-0.056740</td>\n",
       "      <td>0.280595</td>\n",
       "      <td>0.227913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020549</td>\n",
       "      <td>0.670855</td>\n",
       "      <td>-0.584498</td>\n",
       "      <td>0.435903</td>\n",
       "      <td>-0.434411</td>\n",
       "      <td>0.462449</td>\n",
       "      <td>0.254470</td>\n",
       "      <td>-0.284243</td>\n",
       "      <td>0.449133</td>\n",
       "      <td>-0.215167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10263</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.225869</td>\n",
       "      <td>-0.048088</td>\n",
       "      <td>0.832683</td>\n",
       "      <td>-0.486722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019364</td>\n",
       "      <td>0.678839</td>\n",
       "      <td>-0.327944</td>\n",
       "      <td>0.195281</td>\n",
       "      <td>-0.392379</td>\n",
       "      <td>-0.510906</td>\n",
       "      <td>0.112248</td>\n",
       "      <td>0.907283</td>\n",
       "      <td>-0.540267</td>\n",
       "      <td>0.543526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10264</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226412</td>\n",
       "      <td>-0.022735</td>\n",
       "      <td>-0.226564</td>\n",
       "      <td>0.572206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.667242</td>\n",
       "      <td>0.456739</td>\n",
       "      <td>-0.530005</td>\n",
       "      <td>0.294204</td>\n",
       "      <td>-0.705761</td>\n",
       "      <td>-0.443171</td>\n",
       "      <td>0.230582</td>\n",
       "      <td>-0.219111</td>\n",
       "      <td>-0.511201</td>\n",
       "      <td>0.033514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10265 rows × 774 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PantsTotal  NotRealTotal  BarelyTotal  HalfTotal  MostlyTotal  \\\n",
       "0            0.00         0.000        0.000      0.000        0.005   \n",
       "1            0.01         0.000        0.000      0.000        0.005   \n",
       "2            0.01         0.000        0.000      0.000        0.005   \n",
       "3            0.00         0.000        0.000      0.005        0.000   \n",
       "4            0.00         0.000        0.000      0.005        0.000   \n",
       "...           ...           ...          ...        ...          ...   \n",
       "10260        0.00         0.005        0.000      0.010        0.000   \n",
       "10261        0.00         0.005        0.000      0.010        0.000   \n",
       "10262        0.00         0.005        0.000      0.010        0.000   \n",
       "10263        0.00         0.000        0.005      0.000        0.000   \n",
       "10264        0.00         0.000        0.000      0.005        0.000   \n",
       "\n",
       "       RealTotal         0         1         2         3  ...       758  \\\n",
       "0            0.0 -0.353628  0.144715  0.958464 -0.681585  ...  0.463839   \n",
       "1            0.0  0.144056 -0.214195  0.095764  0.488308  ... -0.051924   \n",
       "2            0.0 -0.225200  0.035485  0.773057 -0.188975  ...  0.115203   \n",
       "3            0.0  0.309293 -0.111637  0.026551  0.546971  ...  0.599675   \n",
       "4            0.0  0.175661  0.124550  0.784903 -0.417388  ...  0.610993   \n",
       "...          ...       ...       ...       ...       ...  ...       ...   \n",
       "10260        0.0  0.247382 -0.038540 -0.220711  0.490467  ...  0.101806   \n",
       "10261        0.0 -0.158101 -0.000162  0.912721 -0.635446  ...  0.594023   \n",
       "10262        0.0  0.047984 -0.056740  0.280595  0.227913  ... -0.020549   \n",
       "10263        0.0 -0.225869 -0.048088  0.832683 -0.486722  ...  0.019364   \n",
       "10264        0.0  0.226412 -0.022735 -0.226564  0.572206  ... -0.667242   \n",
       "\n",
       "            759       760       761       762       763       764       765  \\\n",
       "0      0.643878 -0.712667  0.672926 -0.043116  0.436722 -0.138033  0.965976   \n",
       "1      0.318266 -0.376326  0.485823 -0.274125 -0.069330  0.407660 -0.244027   \n",
       "2      0.861145 -0.733366  0.676177 -0.448811  0.543391  0.091833  0.594777   \n",
       "3     -0.301488  0.279851  0.116902 -0.345542  0.248941  0.455902 -0.292546   \n",
       "4      0.133260 -0.258388  0.465152 -0.376480 -0.024658  0.114852  0.673887   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10260  0.142231  0.082679 -0.188128 -0.736837  0.289984  0.260620 -0.663452   \n",
       "10261  0.451269 -0.065867  0.616350  0.004871  0.076593  0.162441  0.884791   \n",
       "10262  0.670855 -0.584498  0.435903 -0.434411  0.462449  0.254470 -0.284243   \n",
       "10263  0.678839 -0.327944  0.195281 -0.392379 -0.510906  0.112248  0.907283   \n",
       "10264  0.456739 -0.530005  0.294204 -0.705761 -0.443171  0.230582 -0.219111   \n",
       "\n",
       "            766       767  \n",
       "0      0.144237  0.848983  \n",
       "1      0.078874 -0.227743  \n",
       "2      0.602495  0.173886  \n",
       "3     -0.026388 -0.631524  \n",
       "4      0.473460  0.106824  \n",
       "...         ...       ...  \n",
       "10260 -0.147648 -0.314752  \n",
       "10261  0.198261  0.514505  \n",
       "10262  0.449133 -0.215167  \n",
       "10263 -0.540267  0.543526  \n",
       "10264 -0.511201  0.033514  \n",
       "\n",
       "[10265 rows x 774 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train=pd.read_excel('train-clean-Reputation.xlsx' )\n",
    "train=train.iloc[:,:-1].astype(float)\n",
    "train=train/200  #for scaling\n",
    "#train\n",
    "\n",
    "model_class='bert'  # bert or roberta or albert\n",
    "model_version='bert-base-cased' #bert-base-cased, roberta-base, roberta-large, albert-base-v2 OR albert-large-v2\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Vectors/\"\n",
    "TF_Output=pd.read_csv( SavesDirectory+'trainOut.tsv', sep='\\t')\n",
    "\n",
    "train=pd.concat([train,TF_Output], axis=1)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10260</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10261</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10262</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10263</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10264</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10265 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4  5\n",
       "0      1  0  0  0  0  0\n",
       "1      1  0  0  0  0  0\n",
       "2      0  0  1  0  0  0\n",
       "3      0  0  0  0  1  0\n",
       "4      0  0  0  0  1  0\n",
       "...   .. .. .. .. .. ..\n",
       "10260  0  1  0  0  0  0\n",
       "10261  0  0  0  0  1  0\n",
       "10262  0  0  0  0  1  0\n",
       "10263  0  0  0  1  0  0\n",
       "10264  0  0  0  0  1  0\n",
       "\n",
       "[10265 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLables=pd.read_excel('train-clean-Reputation.xlsx' )\n",
    "TrainLables=TrainLables.iloc[:,-1] \n",
    "\n",
    "TrainLables=pd.get_dummies(TrainLables)\n",
    "TrainLables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.9660,  0.1442,  0.8490],\n",
       "        [ 0.0100,  0.0000,  0.0000,  ..., -0.2440,  0.0789, -0.2277],\n",
       "        [ 0.0100,  0.0000,  0.0000,  ...,  0.5948,  0.6025,  0.1739],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0050,  0.0000,  ..., -0.2842,  0.4491, -0.2152],\n",
       "        [ 0.0000,  0.0000,  0.0050,  ...,  0.9073, -0.5403,  0.5435],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.2191, -0.5112,  0.0335]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.tensor(train.values)\n",
    "del(train)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets=torch.tensor(TrainLables.astype(float).values)\n",
    "del(TrainLables)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: 774\n",
      "output size: 6\n"
     ]
    }
   ],
   "source": [
    " \n",
    "size= torch.tensor(input[0].size())\n",
    "InputSize=size.item()\n",
    "\n",
    "OutputSize=torch.tensor(targets[0].size()).item()\n",
    "\n",
    "print('input size:', InputSize)\n",
    "print('output size:', OutputSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "         \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(InputSize, 120)  # input size 32\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, OutputSize)  #classifies 'outputsize' different classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x)) \n",
    "        x = torch.tanh(self.fc3(x)).double()\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "#now we use it\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we  setup the neural network parameters\n",
    "# pick an optimizer (Simple Gradient Descent)\n",
    "\n",
    "learning_rate = 4e-4\n",
    "criterion = nn.MSELoss()  #computes the loss Function\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# creating optimizer\n",
    "#optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0857, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 0\n",
      "Loss: tensor(0.2733, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1\n",
      "Loss: tensor(0.1323, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2\n",
      "Loss: tensor(0.1271, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 3\n",
      "Loss: tensor(0.1663, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 4\n",
      "Loss: tensor(0.1570, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 5\n",
      "Loss: tensor(0.1189, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 6\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 7\n",
      "Loss: tensor(0.1065, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 8\n",
      "Loss: tensor(0.1257, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 9\n",
      "Loss: tensor(0.1238, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 10\n",
      "Loss: tensor(0.1067, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 11\n",
      "Loss: tensor(0.0907, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 12\n",
      "Loss: tensor(0.0892, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 13\n",
      "Loss: tensor(0.1028, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 14\n",
      "Loss: tensor(0.1094, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 15\n",
      "Loss: tensor(0.1001, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 16\n",
      "Loss: tensor(0.0881, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 17\n",
      "Loss: tensor(0.0859, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 18\n",
      "Loss: tensor(0.0918, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 19\n",
      "Loss: tensor(0.0973, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 20\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 21\n",
      "Loss: tensor(0.0886, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 22\n",
      "Loss: tensor(0.0839, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 23\n",
      "Loss: tensor(0.0861, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 24\n",
      "Loss: tensor(0.0904, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 25\n",
      "Loss: tensor(0.0910, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 26\n",
      "Loss: tensor(0.0873, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 27\n",
      "Loss: tensor(0.0839, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 28\n",
      "Loss: tensor(0.0842, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 29\n",
      "Loss: tensor(0.0867, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 30\n",
      "Loss: tensor(0.0877, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 31\n",
      "Loss: tensor(0.0860, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 32\n",
      "Loss: tensor(0.0838, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 33\n",
      "Loss: tensor(0.0834, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 34\n",
      "Loss: tensor(0.0849, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 35\n",
      "Loss: tensor(0.0858, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 36\n",
      "Loss: tensor(0.0848, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 37\n",
      "Loss: tensor(0.0831, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 38\n",
      "Loss: tensor(0.0831, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 39\n",
      "Loss: tensor(0.0842, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 40\n",
      "Loss: tensor(0.0846, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 41\n",
      "Loss: tensor(0.0837, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 42\n",
      "Loss: tensor(0.0828, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 43\n",
      "Loss: tensor(0.0830, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 44\n",
      "Loss: tensor(0.0837, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 45\n",
      "Loss: tensor(0.0836, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 46\n",
      "Loss: tensor(0.0830, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 47\n",
      "Loss: tensor(0.0826, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 48\n",
      "Loss: tensor(0.0829, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 49\n",
      "Loss: tensor(0.0832, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 50\n",
      "Loss: tensor(0.0830, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 51\n",
      "Loss: tensor(0.0826, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 52\n",
      "Loss: tensor(0.0826, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 53\n",
      "Loss: tensor(0.0828, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 54\n",
      "Loss: tensor(0.0829, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 55\n",
      "Loss: tensor(0.0826, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 56\n",
      "Loss: tensor(0.0824, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 57\n",
      "Loss: tensor(0.0826, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 58\n",
      "Loss: tensor(0.0827, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 59\n",
      "Loss: tensor(0.0826, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 60\n",
      "Loss: tensor(0.0824, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 61\n",
      "Loss: tensor(0.0824, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 62\n",
      "Loss: tensor(0.0825, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 63\n",
      "Loss: tensor(0.0825, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 64\n",
      "Loss: tensor(0.0823, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 65\n",
      "Loss: tensor(0.0823, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 66\n",
      "Loss: tensor(0.0824, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 67\n",
      "Loss: tensor(0.0824, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 68\n",
      "Loss: tensor(0.0823, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 69\n",
      "Loss: tensor(0.0822, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 70\n",
      "Loss: tensor(0.0823, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 71\n",
      "Loss: tensor(0.0823, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 72\n",
      "Loss: tensor(0.0822, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 73\n",
      "Loss: tensor(0.0822, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 74\n",
      "Loss: tensor(0.0822, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 75\n",
      "Loss: tensor(0.0822, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 76\n",
      "Loss: tensor(0.0822, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 77\n",
      "Loss: tensor(0.0821, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 78\n",
      "Loss: tensor(0.0822, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 79\n",
      "Loss: tensor(0.0822, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 80\n",
      "Loss: tensor(0.0821, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 81\n",
      "Loss: tensor(0.0821, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 82\n",
      "Loss: tensor(0.0821, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 83\n",
      "Loss: tensor(0.0821, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 84\n",
      "Loss: tensor(0.0821, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 85\n",
      "Loss: tensor(0.0821, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 86\n",
      "Loss: tensor(0.0821, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 87\n",
      "Loss: tensor(0.0821, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 88\n",
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 89\n",
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 90\n",
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 91\n",
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 92\n",
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 93\n",
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 94\n",
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 95\n",
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 96\n",
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 97\n",
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 99\n",
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 100\n",
      "Loss: tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 101\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 102\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 103\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 104\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 105\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 106\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 107\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 108\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 109\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 110\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 111\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 112\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 113\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 114\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 115\n",
      "Loss: tensor(0.0819, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 116\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 117\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 118\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 119\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 120\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 121\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 122\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 123\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 124\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 125\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 126\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 127\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 128\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 129\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 130\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 131\n",
      "Loss: tensor(0.0818, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 132\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 133\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 134\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 135\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 136\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 137\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 138\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 139\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 140\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 141\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 142\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 143\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 144\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 145\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 146\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 147\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 148\n",
      "Loss: tensor(0.0817, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 149\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 150\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 151\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 152\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 153\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 154\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 155\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 156\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 157\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 158\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 159\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 160\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 161\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 162\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 163\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 164\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 165\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 166\n",
      "Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 167\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 168\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 169\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 170\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 171\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 172\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 173\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 174\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 175\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 176\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 177\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 178\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 179\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 180\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 181\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 182\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 183\n",
      "Loss: tensor(0.0815, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 184\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 185\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 186\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 187\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 188\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 189\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 190\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 191\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 192\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 193\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 194\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 195\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 196\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 197\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 199\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 200\n",
      "Loss: tensor(0.0814, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 201\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 202\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 203\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 204\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 205\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 206\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 207\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 208\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 209\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 210\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 211\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 212\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 213\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 214\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 215\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 216\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 217\n",
      "Loss: tensor(0.0813, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 218\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 219\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 220\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 221\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 222\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 223\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 224\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 225\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 226\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 227\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 228\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 229\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 230\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 231\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 232\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 233\n",
      "Loss: tensor(0.0812, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 234\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 235\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 236\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 237\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 238\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 239\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 240\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 241\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 242\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 243\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 244\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 245\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 246\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 247\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 248\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 249\n",
      "Loss: tensor(0.0811, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 250\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 251\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 252\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 253\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 254\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 255\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 256\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 257\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 258\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 259\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 260\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 261\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 262\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 263\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 264\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 265\n",
      "Loss: tensor(0.0810, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 266\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 267\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 268\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 269\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 270\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 271\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 272\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 273\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 274\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 275\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 276\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 277\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 278\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 279\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 280\n",
      "Loss: tensor(0.0809, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 281\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 282\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 283\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 284\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 285\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 286\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 287\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 288\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 289\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 290\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 291\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 292\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 293\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 294\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 295\n",
      "Loss: tensor(0.0808, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0807, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 297\n",
      "Loss: tensor(0.0807, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 298\n",
      "Loss: tensor(0.0807, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 299\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):  \n",
    "        \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net(input.float())\n",
    "\n",
    "    loss = criterion(output, targets)\n",
    "    print('Loss:', loss, ' at epoch:', epoch)\n",
    "\n",
    "    loss.backward()  #backprop\n",
    "    optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the FCNN model\n",
    "\n",
    "stage='NNetworkStatementVector/'\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/\"+stage\n",
    "PATH = SavesDirectory+'Tanh_MSE_adam1.pth'\n",
    "\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# more on saving pytorch networks: https://pytorch.org/docs/stable/notes/serialization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load previously saved FCNN model \n",
    "\n",
    "stage='NNetworkStatementVector/'\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/\"+stage\n",
    "PATH = SavesDirectory+'Tanh_MSE_adam1.pth'\n",
    "\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0100,  0.0050,  0.0200,  ...,  0.9660,  0.1442,  0.8490],\n",
       "        [ 0.0100,  0.0050,  0.0200,  ..., -0.2440,  0.0789, -0.2277],\n",
       "        [ 0.0100,  0.0050,  0.0200,  ...,  0.5948,  0.6025,  0.1739],\n",
       "        ...,\n",
       "        [ 0.0050,  0.0000,  0.0000,  ...,  0.9461,  0.1329,  0.6838],\n",
       "        [ 0.0000,  0.0000,  0.0050,  ...,  0.9595, -0.4551,  0.7177],\n",
       "        [ 0.0000,  0.0000,  0.0050,  ...,  0.8213, -0.4577,  0.7748]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the test data\n",
    "\n",
    "TestData=pd.read_excel('test-clean-Reputation.xlsx' )\n",
    "TestData=TestData.iloc[:,:-1].astype(float)\n",
    "TestData=TestData/200\n",
    "\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Vectors/\"\n",
    "TF_Output=pd.read_csv( SavesDirectory+'testOut.tsv', sep='\\t')\n",
    "\n",
    "TestData=pd.concat([TestData,TF_Output], axis=1)\n",
    "\n",
    "\n",
    "TestData=torch.tensor(TestData.values)\n",
    "TestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=pd.read_excel('test-clean-Reputation.xlsx' )\n",
    "labels=labels.iloc[:,-1] \n",
    "labelsOneHot=pd.get_dummies(labels)\n",
    " \n",
    "TestLables =torch.tensor(labelsOneHot.values)\n",
    "TestLables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 2 4 3 5 3 4 1 3 5 1 5 3 3 3 2 4 1 3 5 3 3 3 1 1 1 4 1 4 2 3 4 4 1 1 3 2 3 3 4 3 4 2 1 3 4 1 3 2 3 3 5 5 1 3 4 5 2 5 3 5 4 5 5 1 5 3 4 1 2 1 3 4 1 1 3 3 4 5 3 3 3 5 4 3 3 1 4 2 3 5 3 5 5 3 3 3 1 1 2 2 3 1 4 3 3 1 5 5 5 3 4 3 3 3 4 4 4 3 3 3 2 3 3 5 3 3 3 4 1 4 1 3 3 1 1 2 1 4 5 5 4 3 2 4 1 1 3 2 5 4 4 3 5 2 1 3 3 3 1 4 1 1 2 2 0 1 3 3 2 2 1 1 3 3 2 3 2 3 1 2 5 5 2 1 4 3 1 2 1 3 3 2 2 4 4 1 5 5 4 2 1 0 0 0 1 2 0 0 0 1 0 0 1 1 1 1 1 4 3 4 4 3 3 3 3 1 4 3 3 5 2 3 3 3 2 1 1 3 3 2 1 1 2 5 1 1 3 1 2 3 3 4 3 2 4 3 4 2 2 3 3 3 2 1 1 3 1 3 3 1 1 2 3 3 5 3 3 3 4 5 2 2 5 4 3 1 3 3 1 3 4 1 1 3 4 3 5 3 3 2 0 2 3 2 1 4 3 4 1 3 1 4 2 3 3 3 1 3 1 2 1 3 0 3 3 2 1 5 1 3 0 2 1 5 1 4 1 3 3 1 3 3 1 4 3 1 0 1 3 0 1 1 1 1 1 4 3 1 1 1 2 1 3 0 1 1 1 4 1 1 3 1 1 1 2 4 2 3 4 2 4 5 3 1 5 1 3 1 4 4 4 1 4 5 4 5 2 1 3 5 5 5 5 3 5 2 1 3 3 4 4 1 5 2 5 1 1 5 3 3 3 1 1 1 4 2 1 5 3 3 5 5 3 4 5 1 5 1 2 1 1 3 3 3 3 1 3 4 2 5 1 1 4 4 5 2 2 4 1 1 1 4 5 2 5 1 3 4 3 3 1 1 1 2 1 3 3 1 4 1 3 3 2 3 2 2 5 5 2 4 4 5 4 2 2 3 1 3 5 1 2 4 3 5 1 2 5 5 5 5 1 5 1 5 5 1 2 5 5 3 5 4 2 1 1 5 5 3 3 5 4 5 2 5 3 2 3 3 1 3 5 3 5 3 4 3 5 3 1 2 4 1 4 3 5 5 1 1 1 1 3 4 3 3 2 1 3 3 2 4 5 3 5 3 2 1 5 4 1 3 2 3 5 1 1 1 3 1 1 4 3 5 3 3 1 3 1 5 2 1 3 1 1 3 5 1 2 3 3 4 4 1 3 3 1 1 4 3 1 1 3 5 5 2 5 2 3 2 1 3 1 4 1 1 4 5 4 2 3 4 5 1 3 3 1 4 3 3 2 3 4 1 5 3 3 5 3 1 1 1 1 4 3 3 2 3 3 1 3 3 3 1 4 1 1 1 3 3 1 1 0 1 2 1 3 2 3 4 3 5 3 4 2 3 3 3 4 2 5 2 2 3 3 3 1 3 2 3 3 4 1 1 1 1 3 4 2 4 5 5 3 1 5 2 4 5 5 3 3 4 2 2 1 1 1 5 4 3 3 2 1 1 3 1 3 3 5 1 1 2 1 1 5 3 3 1 4 2 4 3 3 3 3 2 2 2 3 4 3 5 1 4 4 3 3 3 1 4 5 2 3 4 2 2 2 3 3 2 4 1 3 4 3 1 3 2 1 3 3 4 3 3 3 3 3 1 1 2 3 5 1 3 1 4 5 5 2 3 3 5 2 5 3 3 4 3 5 3 1 4 1 5 4 1 3 4 5 4 1 4 1 3 1 5 2 1 2 4 3 4 5 1 5 1 5 3 2 3 4 3 3 1 4 1 5 1 3 3 3 2 3 3 3 2 3 3 3 5 3 3 4 4 3 3 3 2 4 4 3 4 3 1 5 2 3 3 2 3 4 1 3 5 3 2 3 4 3 1 3 3 3 3 3 1 3 4 4 3 5 3 3 3 2 3 4 4 2 4 3 1 3 3 1 2 1 4 3 3 1 2 4 3 5 1 2 4 4 3 1 3 3 1 4 2 1 1 5 4 1 1 4 5 0 0 3 5 2 3 1 5 5 3 1 4 5 5 3 5 1 3 1 1 1 5 2 1 2 3 3 3 1 1 2 2 2 1 0 1 1 4 1 1 4 1 1 1 2 1 3 2 3 1 1 1 1 3 3 5 1 5 3 3 3 2 3 3 3 5 3 3 3 4 5 3 2 2 3 1 3 2 2 3 3 5 2 1 3 5 2 2 3 3 3 2 4 1 2 3 3 1 3 3 4 1 3 5 1 4 3 2 3 1 1 5 1 4 5 2 2 3 3 1 3 2 4 4 3 5 2 1 2 3 5 4 3 2 3 4 4 3 1 1 4 3 3 3 3 4 3 1 1 1 3 5 3 4 4 1 1 3 1 1 3 4 3 4 5 3 2 1 3 2 1 3 4 1 1 5 3 2 1 3 1 5 5 1 3 2 3 3 4 5 3 3 4 3 1 5 4 1 3 4 5 3 4 1 2 4 3 4 5 4 2 2 1 3 3 3 5 4 4 5 4 2 5 3 3 5 5 2 2 3 3 4 2 4 3 1 2 1 1 1 5 4 4 3 4 4 4 2 1 1 3 5 5 2 1 3 5 3 5 4 3 2 3 4 3 4 3 1 4 2 3 1 3 3 5 3 4 1 3 2 3 3 4 2 1 3 1 5 3 4 1 1 2 4 4 3 3 4 2 2 4 3 3 4 3 1 1 4 1 3 3 1 2 4 3 1 3 2 Correct: 282 out of: 1282\n",
      "Accuracy of the network :  21.996879875195006\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "Y=[]  #target\n",
    "Pred=[]  #predicted\n",
    "\n",
    "with torch.no_grad():\n",
    "    for row in range(len(TestData)):\n",
    "        outputs = net(TestData[row,:].float())\n",
    "        result=0\n",
    "        total+=1\n",
    "        if outputs[0]<outputs[1]:result=1\n",
    "        if outputs[result]<outputs[2]:result=2\n",
    "        if outputs[result]<outputs[3]:result=3\n",
    "        if outputs[result]<outputs[4]:result=4\n",
    "        if outputs[result]<outputs[5]:result=5\n",
    "        \n",
    "        if labelsOneHot.iloc[row,result]==1: correct+=1\n",
    "        \n",
    "        Y.append(result)\n",
    "        Pred.append(labels.iloc[row])\n",
    "        \n",
    "        print(result, end=' ')\n",
    "\n",
    "                \n",
    "print('Correct:', correct, 'out of:', total )\n",
    "print('Accuracy of the network : ',( 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5   2   3   5   1   3]\n",
      " [ 24  67  55  51  64  48]\n",
      " [ 10  38  36  36  26  31]\n",
      " [ 29  75  57 100  81  71]\n",
      " [ 14  33  44  35  44  26]\n",
      " [  9  31  26  39  33  30]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "print(metrics.confusion_matrix(Y,Pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Pants       0.05      0.26      0.09        19\n",
      "       False       0.27      0.22      0.24       309\n",
      " Barely-True       0.16      0.20      0.18       177\n",
      "   Half-True       0.38      0.24      0.29       413\n",
      " Mostly-True       0.18      0.22      0.20       196\n",
      "        True       0.14      0.18      0.16       168\n",
      "\n",
      "    accuracy                           0.22      1282\n",
      "   macro avg       0.20      0.22      0.19      1282\n",
      "weighted avg       0.26      0.22      0.23      1282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Pants', 'False', 'Barely-True','Half-True','Mostly-True','True']\n",
    "\n",
    "print(metrics.classification_report(Y, Pred,target_names =target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

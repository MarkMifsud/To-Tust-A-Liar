{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we first do the classification using the transformer This is our first classification task.\n",
    "\n",
    "The output classification vector from the transformer is saved to be used by the FCNN This is our second classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6092,  0.9993,  0.0077, -2.5364]])\n",
      "tensor([[ 1.3585,  0.1174, -0.8305, -1.0718]])\n",
      "tensor([0.3727])\n"
     ]
    }
   ],
   "source": [
    "input1 = torch.randn(1,4)\n",
    "input2 = torch.randn(1,4)\n",
    "output = torch.cosine_similarity(input1, input2)\n",
    "print(input1)\n",
    "print(input2)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "Some pre-processing to the dataset has already been done in preparation for various tests, so this processing is not from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedure for getting the data sets and formatting them for the transformer\n",
    " \n",
    "\n",
    "def prepareDataset( filename):\n",
    "     \n",
    "    ReadSet=pd.read_excel(filename )\n",
    "\n",
    "    ReadSet['text']=ReadSet['Statement']\n",
    "    ReadSet['labels']=ReadSet['Label']\n",
    "    \n",
    "    ReadSet=ReadSet.drop(['ID','Label','Statement','Subject','Speaker','Job','From','Affiliation','PantsTotal','NotRealTotal','BarelyTotal','HalfTotal','MostlyTotal','Truths','Context'\n",
    "],axis=1)\n",
    "     \n",
    "    return ReadSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The attorney general requires that rape victim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>President Clinton reduced the scale of our mil...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I used tax cuts to help create over 80,000 job...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Mexico moved \"up to\" sixth in the nation i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Corporate profits are up, CEO pay is up, but a...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10264</th>\n",
       "      <td>Under Obamacare, premiums have doubled and tri...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10265</th>\n",
       "      <td>We adopted the modern Social Security system a...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266</th>\n",
       "      <td>More than two months ago President Barack Obam...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10267</th>\n",
       "      <td>We had a massive landslide victory, as you kno...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10268</th>\n",
       "      <td>Says U.S. Rep. Nancy Pelosi said, Employers cu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10269 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  labels\n",
       "0      The attorney general requires that rape victim...       0\n",
       "1      President Clinton reduced the scale of our mil...       3\n",
       "2      I used tax cuts to help create over 80,000 job...       4\n",
       "3      New Mexico moved \"up to\" sixth in the nation i...       4\n",
       "4      Corporate profits are up, CEO pay is up, but a...       5\n",
       "...                                                  ...     ...\n",
       "10264  Under Obamacare, premiums have doubled and tri...       4\n",
       "10265  We adopted the modern Social Security system a...       5\n",
       "10266  More than two months ago President Barack Obam...       3\n",
       "10267  We had a massive landslide victory, as you kno...       1\n",
       "10268  Says U.S. Rep. Nancy Pelosi said, Employers cu...       1\n",
       "\n",
       "[10269 rows x 2 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the training dataset\n",
    "train=prepareDataset( 'train.xlsx')\n",
    "# and display for inspecting\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The president is brain-dead.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barack Obama supported keeping troops in Iraq,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He's leading by example, refusing contribution...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm the first person who really took up the is...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I built that border fence in San Diego...and i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>CNN accidentally aired 30 minutes of pornograp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>President Obamas American Recovery and Reinves...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>We (in Illinois) have the fifth-highest tax bu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>Says Donald Trump won more counties than any c...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>A recent study found that cities where Uber op...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1284 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "0                          The president is brain-dead.       0\n",
       "1     Barack Obama supported keeping troops in Iraq,...       3\n",
       "2     He's leading by example, refusing contribution...       3\n",
       "3     I'm the first person who really took up the is...       4\n",
       "4     I built that border fence in San Diego...and i...       4\n",
       "...                                                 ...     ...\n",
       "1279  CNN accidentally aired 30 minutes of pornograp...       1\n",
       "1280  President Obamas American Recovery and Reinves...       2\n",
       "1281  We (in Illinois) have the fifth-highest tax bu...       4\n",
       "1282  Says Donald Trump won more counties than any c...       4\n",
       "1283  A recent study found that cities where Uber op...       3\n",
       "\n",
       "[1284 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the evaluation/validation dataset\n",
    "Eval=prepareDataset('valid.xlsx')\n",
    "# and display for inspecting\n",
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Mexico was 46th in teacher pay (when he wa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barack Obama and Hillary Clinton have changed ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll tell you what I can tell this country: If...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tommy Thompson created the first school choice...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty-six percent decline in overall crime. A ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>We have trade agreements with 20 countries, an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>On Donald Trumps plan to cut federal funding t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>Black Lives Matter, who are attacking law enfo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>Latina who enthusiastically supported Donald T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>Theres been no conclusive or specific report t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1283 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "0     New Mexico was 46th in teacher pay (when he wa...       4\n",
       "1     Barack Obama and Hillary Clinton have changed ...       3\n",
       "2     I'll tell you what I can tell this country: If...       1\n",
       "3     Tommy Thompson created the first school choice...       5\n",
       "4     Fifty-six percent decline in overall crime. A ...       5\n",
       "...                                                 ...     ...\n",
       "1278  We have trade agreements with 20 countries, an...       1\n",
       "1279  On Donald Trumps plan to cut federal funding t...       4\n",
       "1280  Black Lives Matter, who are attacking law enfo...       2\n",
       "1281  Latina who enthusiastically supported Donald T...       0\n",
       "1282  Theres been no conclusive or specific report t...       1\n",
       "\n",
       "[1283 rows x 2 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the test set dataset\n",
    "test=prepareDataset('test.xlsx')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the transformer for fine tuning\n",
    "\n",
    "This is where changes are done to optimise the model\n",
    "\n",
    "The simpletransformers library is the quickest way to do this at the time of writing. \n",
    "For more information on the settings and their default value go here:\n",
    "https://github.com/ThilinaRajapakse/simpletransformers#default-settings \n",
    "\n",
    "###### Please do read that reference before changing any parameters. Don't try to be a hero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model variables were set up: \n"
     ]
    }
   ],
   "source": [
    "#Set the model being used here\n",
    "model_class='albert'  # bert or roberta or albert\n",
    "model_version='albert-large-v2' #bert-base-cased, roberta-base, roberta-large, albert-base-v2 OR albert-large-v2\n",
    "\n",
    "\n",
    "output_folder='./TunedModels/'+model_class+'/'+model_version+\"/\"\n",
    "cache_directory= \"./TunedModels/\"+model_class+\"/\"+model_version+\"/cache/\"\n",
    "labels_count=6  # the number of classification classes\n",
    "\n",
    "print('model variables were set up: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\0 finalThesis\\siamese\n",
      "./TunedModels/albert/albert-large-v2/\n",
      "./TunedModels/albert/albert-large-v2/cache/\n"
     ]
    }
   ],
   "source": [
    "# use this to test if writing to the directories is working\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "print(output_folder)\n",
    "print(cache_directory)\n",
    "\n",
    "testWrite=train.head(30)\n",
    " \n",
    "testWrite.to_csv(output_folder+'DeleteThisToo.tsv', sep='\\t')\n",
    "testWrite.to_csv(cache_directory+'DeleteThisToo.tsv', sep='\\t')\n",
    "\n",
    "del(testWrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "save_every_steps=1285\n",
    "# assuming training batch size of 8\n",
    "# any number above 1284 saves the model only at every epoch\n",
    "# Saving the model mid training very often will consume disk space fast\n",
    "\n",
    "train_args={\n",
    "    \"output_dir\":output_folder,\n",
    "    \"cache_dir\":cache_directory,\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'num_train_epochs': 1,\n",
    "    \"save_steps\": save_every_steps, \n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"train_batch_size\": 64,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"evaluate_during_training_steps\": 312,\n",
    "    \"max_seq_length\": 64,\n",
    "    \"n_gpu\": 1,\n",
    "}\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(model_class, model_version, num_labels=labels_count, args=train_args) \n",
    "\n",
    "# You can set class weights by using the optional weight argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a saved model (based on above args{})\n",
    "\n",
    "If you stopped training you can continue training from a previously saved check point.\n",
    "The next cell allows you to load a model from any checkpoint.\n",
    "The number of epochs in the train_args{} will be done and continue tuning from your checkpoint.\n",
    "\n",
    "###### HOWEVER\n",
    "It will overwrite previous checkpoints!\n",
    "Example:  If you load an epoch-3 checkpoint, the epoch-1 checkpoint will be overwritten by the 4th epoch and it will be equivalent to a 4th epoch even if you have epoch-1 in the name.\n",
    "###### SO BE CAREFUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model, please wait...\n",
      "model in use is : ./TunedModels/albert/albert-large-v2/checkpoint-161-epoch-3\n"
     ]
    }
   ],
   "source": [
    "# loading a previously saved model based on this particular Transformer Class and model_name\n",
    "\n",
    "# loading the checkpoint that gave the best result\n",
    "CheckPoint='checkpoint-161-epoch-3'  #epoch 1\n",
    "\n",
    "\n",
    "preSavedCheckpoint=output_folder+CheckPoint\n",
    "\n",
    "print('Loading model, please wait...')\n",
    "model = ClassificationModel( model_class, preSavedCheckpoint, num_labels=labels_count, args=train_args) \n",
    "print('model in use is :', preSavedCheckpoint )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Transformer\n",
    "\n",
    "Skip the next cell if you want to skip the training and go directly to the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21dffda5322e490ebea8db30c5e07c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10269.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3473af57ed42e992b8d9fee35bde36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f9fca873894dc8ac7679626d6ca76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=161.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.753227Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Running loss: 1.537029Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Running loss: 1.507523\n",
      "\n",
      "Training of albert model complete. Saved to ./TunedModels/albert/albert-large-v2/.\n",
      "Training time:  0:03:08.390604\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "current_time = datetime.now()\n",
    "model.train_model(train)\n",
    "print(\"Training time: \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded from cache at ./TunedModels/albert/albert-large-v2/cache/cached_dev_albert_64_6_10269\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636c95f0481b4d54bf693f3ab08a5de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1284.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': 0.20466921249805126, 'acc': 0.35018015386113543, 'eval_loss': 1.558491243937305}\n",
      "Features loaded from cache at ./TunedModels/albert/albert-large-v2/cache/cached_dev_albert_64_6_1284\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8f3bc77df7429881a61a6c99ccbb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=161.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': 0.09416109138513852, 'acc': 0.26557632398753894, 'eval_loss': 1.6665791613715035}\n",
      "Features loaded from cache at ./TunedModels/albert/albert-large-v2/cache/cached_dev_albert_64_6_1283\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89d341d229348508ff699ea512090ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=161.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': 0.11524783095303312, 'acc': 0.2821512081060016, 'eval_loss': 1.6612607847829783}\n",
      "Training Result: 0.35018015386113543\n",
      "Eval Result: 0.26557632398753894\n",
      "Test Set Result: 0.2821512081060016\n"
     ]
    }
   ],
   "source": [
    "TrainResult, TrainModel_outputs, wrong_predictions = model.eval_model(train, acc=sklearn.metrics.accuracy_score)\n",
    "\n",
    "EvalResult, EvalModel_outputs, wrong_predictions = model.eval_model(Eval, acc=sklearn.metrics.accuracy_score)\n",
    "\n",
    "TestResult, TestModel_outputs, wrong_predictions = model.eval_model(test, acc=sklearn.metrics.accuracy_score)\n",
    "\n",
    "print('Training Result:', TrainResult['acc'])\n",
    "#print('Model Out:', TrainModel_outputs)\n",
    "\n",
    "print('Eval Result:', EvalResult['acc'])\n",
    "#print('Model Out:', EvalModel_outputs)\n",
    "\n",
    "print('Test Set Result:', TestResult['acc'])\n",
    "#print('Model Out:', TestModel_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.7587891  -0.18713379 -0.25805664  0.7626953   0.9614258   0.9638672 ] 5   4 \n",
      "[-0.07824707  0.62353516  0.3293457   0.11523438 -0.49829102 -0.60498047] 1   3 \n",
      "[ 0.11871338  0.39746094  0.09368896  0.10760498 -0.3293457  -0.46264648] 1   1 Match 1\n",
      "\n",
      "[-1.3369141  -0.3317871   0.15209961  1.0771484   0.8388672   0.16247559] 3   5 \n",
      "[-1.6191406  -0.5517578  -0.15795898  1.1708984   0.7915039   0.2397461 ] 3   5 \n",
      "[-1.671875   -0.24450684 -0.12158203  0.89501953  0.93603516  0.81347656] 4   2 \n",
      "[-1.7392578  -0.41430664 -0.38916016  1.0058594   1.0585938   0.7133789 ] 4   4 Match 2\n",
      "\n",
      "[-1.4775391  -0.17382812  0.07879639  0.68847656  0.73046875  0.44873047] 4   5 \n",
      "[-1.7880859  -0.23486328 -0.39086914  0.6459961   1.2226562   1.1923828 ] 4   4 Match 3\n",
      "\n",
      "[-1.5947266  -0.22485352 -0.19970703  0.8613281   1.0185547   0.60302734] 4   5 \n",
      "[-0.4194336   0.25268555  0.4345703   0.5605469  -0.1472168  -0.8354492 ] 3   5 \n",
      "[-1.6865234   0.00200653  0.06481934  0.8300781   0.84277344  0.4716797 ] 4   3 \n",
      "[-0.56591797  0.6933594  -0.10882568  0.01794434 -0.35302734 -0.04272461] 1   2 \n",
      "[-0.28320312  0.6430664   0.3569336   0.15283203 -0.390625   -0.49072266] 1   1 Match 4\n",
      "\n",
      "[-0.8334961   0.16882324  0.1003418   0.22143555  0.10656738  0.2993164 ] 5   4 \n",
      "[-0.7104492   0.62597656  0.16467285  0.27734375 -0.07843018 -0.00978851] 1   4 \n",
      "[-1.6367188   0.037323   -0.13122559  0.41210938  0.83935547  0.94628906] 5   4 \n",
      "[-0.9604492   0.5053711   0.30664062  0.39379883  0.28320312 -0.11834717] 1   3 \n",
      "[-0.7495117   0.49023438  0.09814453  0.27246094 -0.03509521  0.02603149] 1   1 Match 5\n",
      "\n",
      "[-1.4814453   0.07794189 -0.04522705  0.27294922  0.61083984  0.67529297] 5   1 \n",
      "[-0.9145508   0.28295898  0.5097656   0.60498047  0.31811523 -0.30566406] 3   1 \n",
      "[-1.7998047  -0.24645996 -0.22460938  0.62353516  1.1914062   0.9453125 ] 4   3 \n",
      "[-1.6035156   0.05075073 -0.10668945  0.46435547  0.78271484  0.78564453] 5   1 \n",
      "[-1.1083984   0.13415527  0.5097656   0.8935547   0.3630371  -0.44335938] 3   5 \n",
      "[-0.29614258  0.12939453  0.31689453  0.3557129  -0.02101135 -0.34179688] 3   5 \n",
      "[ 0.4338379   0.50878906  0.01161194 -0.28466797 -0.70214844 -0.3112793 ] 1   5 \n",
      "[-1.0703125  -0.20056152  0.31152344  0.54052734  0.38012695  0.28393555] 3   5 \n",
      "[-0.5805664   0.5942383   0.24902344  0.22509766 -0.12927246 -0.36206055] 1   4 \n",
      "[-0.90527344  0.5522461   0.40893555  0.48364258  0.12054443 -0.19799805] 1   5 \n",
      "[-0.7241211   0.08087158  0.21679688  0.31982422  0.20373535  0.31860352] 3   3 Match 6\n",
      "\n",
      "[-1.0732422   0.2770996   0.41748047  0.5703125   0.2076416  -0.18603516] 3   5 \n",
      "[-1.2929688   0.4963379  -0.07293701  0.22631836  0.22644043  0.49194336] 1   2 \n",
      "[-0.31884766  0.3798828  -0.0287323   0.06143188 -0.26586914  0.24157715] 1   0 \n",
      "[-1.5136719  -0.5234375  -0.00248337  1.109375    1.0615234   0.5       ] 3   5 \n",
      "[-1.3769531  -0.11529541  0.14807129  0.79589844  0.62841797  0.14501953] 3   5 \n",
      "[-1.0361328  -0.3708496   0.20654297  0.5854492   0.4650879   0.28393555] 3   5 \n",
      "[-0.05819702  0.3671875   0.3725586  -0.1451416  -0.37158203 -0.22766113] 2   4 \n",
      "[-0.10467529  0.24633789  0.58203125  0.52734375 -0.38842773 -0.9243164 ] 2   3 \n",
      "[-0.9238281   0.18591309 -0.04473877  0.3881836   0.2529297   0.42797852] 5   3 \n",
      "[-1.1845703   0.17480469  0.08197021  0.31982422  0.39453125  0.515625  ] 5   5 Match 7\n",
      "\n",
      "[ 0.07806396  0.5444336   0.13439941 -0.08166504 -0.46704102 -0.41381836] 1   5 \n",
      "[-0.6591797   0.54003906 -0.00982666  0.1430664  -0.26342773  0.11358643] 1   4 \n",
      "[-0.64941406  0.28344727  0.23547363  0.1821289  -0.18774414 -0.18798828] 1   3 \n",
      "[ 0.07470703  0.6738281   0.55078125 -0.06365967 -0.39453125 -0.7963867 ] 1   2 \n",
      "[ 0.37963867  0.6977539  -0.04153442 -0.4645996  -0.72216797 -0.36889648] 1   1 Match 8\n",
      "\n",
      "[-0.8466797   0.5629883   0.60302734  0.67871094 -0.02653503 -0.94433594] 3   2 \n",
      "[-1.8066406  -0.16821289 -0.33935547  0.64697266  1.1484375   1.0517578 ] 4   3 \n",
      "[-0.8959961   0.16174316  0.5541992   0.5810547   0.25439453 -0.359375  ] 3   2 \n",
      "[-0.11590576  0.56152344  0.29223633 -0.04998779 -0.45507812 -0.3630371 ] 1   2 \n",
      "[-1.5556641   0.18835449 -0.18603516  0.5673828   0.63427734  0.7363281 ] 5   5 Match 9\n",
      "\n",
      "[-1.6601562  -0.07989502 -0.00635529  0.78271484  0.96435547  0.61865234] 4   5 \n",
      "[-1.6972656   0.20959473 -0.01374817  0.43579102  0.8725586   0.53125   ] 4   2 \n",
      "[-1.546875    0.07330322  0.15197754  1.0830078   0.5185547  -0.00302505] 3   4 \n",
      "[-0.6582031   0.6220703   0.23425293  0.09753418 -0.23986816 -0.09002686] 1   2 \n",
      "[-1.0976562   0.4111328  -0.18188477  0.19506836  0.20117188  0.6933594 ] 5   1 \n",
      "[-1.0351562   0.13500977  0.3540039   0.67822266  0.20788574 -0.12054443] 3   5 \n",
      "[-1.7382812  -0.03579712 -0.31347656  0.6455078   1.0019531   1.1162109 ] 5   5 Match 10\n",
      "\n",
      "[-1.1367188   0.14074707  0.18322754  0.5083008   0.40356445  0.38476562] 3   2 \n",
      "[-1.1748047   0.33374023  0.01860046  0.24658203  0.08074951  0.29760742] 1   5 \n",
      "[-0.7055664   0.5449219  -0.17614746  0.12200928 -0.06793213  0.20166016] 1   2 \n",
      "[-0.8847656   1.1503906  -0.4091797   0.15466309 -0.33618164 -0.31103516] 1   1 Match 11\n",
      "\n",
      "[-1.5761719   0.02009583 -0.14501953  0.67041016  0.8725586   0.56933594] 4   1 \n",
      "[-1.2900391   1.0742188  -0.53027344  0.2770996   0.09838867  0.36450195] 1   3 \n",
      "[-1.1279297   0.18664551  0.42822266  0.49145508  0.20153809 -0.14697266] 3   5 \n",
      "[ 0.42993164  0.69189453  0.33789062 -0.19812012 -0.8178711  -0.8364258 ] 1   0 \n",
      "[-1.4609375  -0.16894531  0.24230957  0.9116211   0.7363281   0.42163086] 3   5 \n",
      "[-1.6972656  -0.20898438 -0.18249512  0.81103516  0.84375     0.7480469 ] 4   3 \n",
      "[-1.7099609  -0.28564453 -0.35766602  0.6694336   1.1425781   1.0527344 ] 4   4 Match 12\n",
      "\n",
      "[-0.6142578   0.78515625  0.4309082   0.25097656 -0.2019043  -0.52441406] 1   3 \n",
      "[-0.9243164  -0.10736084 -0.03421021  0.52734375  0.30810547  0.31152344] 3   1 \n",
      "[-1.5917969   0.00816345 -0.20373535  0.6064453   0.7919922   0.7705078 ] 4   5 \n",
      "[-0.3474121   0.27734375  0.21484375  0.19677734 -0.15234375 -0.28344727] 1   1 Match 13\n",
      "\n",
      "[-1.5898438   0.14318848  0.14453125  0.8964844   0.68652344  0.21520996] 3   3 Match 14\n",
      "\n",
      "[-0.62060547 -0.09075928  0.15429688  0.2475586   0.14794922 -0.07116699] 3   5 \n",
      "[-0.11358643  0.6791992  -0.04284668 -0.1928711  -0.4921875  -0.04348755] 1   4 \n",
      "[-0.74365234  1.21875    -0.56103516  0.04315186 -0.40112305 -0.10235596] 1   1 Match 15\n",
      "\n",
      "[-1.5732422  -0.1282959  -0.28320312  0.67041016  0.8544922   0.8432617 ] 4   4 Match 16\n",
      "\n",
      "[-0.7060547   1.0546875  -0.05792236  0.18493652 -0.39453125 -0.49926758] 1   3 \n",
      "[-1.4355469   0.00936127  0.16467285  0.6694336   0.5629883   0.5385742 ] 3   1 \n",
      "[-1.171875   -0.24511719  0.1673584   0.8417969   0.43945312  0.20056152] 3   3 Match 17\n",
      "\n",
      "[-1.0429688   0.00606537  0.6274414   0.7685547   0.27978516 -0.37402344] 3   3 Match 18\n",
      "\n",
      "[-0.7348633   0.6098633   0.28393555  0.20410156 -0.07415771 -0.32080078] 1   5 \n",
      "[-1.2207031   0.25561523  0.22009277  0.5395508   0.26953125  0.42016602] 3   2 \n",
      "[-1.7597656  -0.10906982 -0.40966797  0.5107422   1.2128906   0.94628906] 4   1 \n",
      "[ 0.54052734  0.65625     0.27807617 -0.29785156 -0.8725586  -0.74853516] 1   4 \n",
      "[-0.9291992  -0.08691406  0.4243164   0.40576172  0.1574707   0.03942871] 2   3 \n",
      "[-1.1162109   0.7783203   0.02326965  0.40576172  0.0473938   0.09210205] 1   3 \n",
      "[-0.35424805  0.16113281  0.45166016  0.26391602 -0.18151855 -0.2927246 ] 2   3 \n",
      "[-0.5991211   0.44311523  0.14086914  0.10986328 -0.22546387  0.07019043] 1   4 \n",
      "[-1.4755859   0.15661621  0.20178223  0.64941406  0.7451172   0.04266357] 4   5 \n",
      "[-0.19372559  0.41723633  0.10144043 -0.09082031 -0.16357422 -0.3479004 ] 1   4 \n",
      "[-1.0722656   0.82177734 -0.26391602  0.07232666  0.17346191  0.29492188] 1   3 \n",
      "[ 0.6166992   0.6855469   0.12298584 -0.45141602 -0.81933594 -0.6611328 ] 1   0 \n",
      "[-1.5771484  -0.09735107 -0.01797485  0.80908203  0.95751953  0.58447266] 4   1 \n",
      "[-1.7841797  -0.04971313 -0.07189941  0.7739258   1.0107422   0.5       ] 4   1 \n",
      "[-1.0009766   0.40234375  0.4790039   0.81591797  0.07965088 -0.71484375] 3   1 \n",
      "[-1.2167969   0.5996094  -0.31860352  0.1352539   0.34643555  0.67626953] 5   5 Match 19\n",
      "\n",
      "[-1.0976562   0.49804688 -0.03164673  0.15393066  0.1697998   0.45214844] 1   5 \n",
      "[-0.88427734  0.19482422  0.5126953   0.55566406  0.30541992 -0.26513672] 3   3 Match 20\n",
      "\n",
      "[-0.2512207   0.54296875  0.49047852  0.11865234 -0.3623047  -0.5151367 ] 1   1 Match 21\n",
      "\n",
      "[-1.8730469  -0.03201294 -0.3557129   0.59277344  1.2265625   1.1523438 ] 4   4 Match 22\n",
      "\n",
      "[-0.25024414  0.5410156   0.35986328  0.25219727 -0.4111328  -0.7373047 ] 1   0 \n",
      "[-1.6210938  -0.1381836   0.27441406  1.0654297   1.0029297   0.09790039] 3   3 Match 23\n",
      "\n",
      "[-0.06707764  0.7841797   0.0094223  -0.23754883 -0.4206543  -0.13769531] 1   1 Match 24\n",
      "\n",
      "[-1.0585938  -0.1151123   0.19470215  0.37695312  0.3684082   0.2626953 ] 3   2 \n",
      "[-1.0615234   0.25561523  0.36547852  0.90283203  0.21936035 -0.5253906 ] 3   1 \n",
      "[ 0.10333252  0.61328125  0.22106934  0.07006836 -0.61279297 -0.8408203 ] 1   2 \n",
      "[-0.40039062  0.62841797  0.77734375  0.57470703 -0.32739258 -1.0800781 ] 2   5 \n",
      "[ 0.16455078  0.7944336  -0.05844116 -0.35302734 -0.6401367  -0.36376953] 1   5 \n",
      "[-1.5761719   0.36889648 -0.08660889  0.40234375  0.75439453  0.74560547] 4   2 \n",
      "[-0.5151367   0.2364502   0.20947266  0.4970703  -0.11218262 -0.36987305] 3   0 \n",
      "[-0.9941406  -0.01187134  0.69873047  0.98876953  0.11431885 -0.8388672 ] 3   2 \n",
      "[-1.3095703   0.20214844 -0.07366943  0.39868164  0.47192383  0.58691406] 5   5 Match 25\n",
      "\n",
      "[-1.7880859 -0.28125   -0.4506836  0.5932617  1.2851562  1.0849609] 4   4 Match 26\n",
      "\n",
      "[-0.5727539   1.3349609  -0.50390625 -0.12805176 -0.5600586  -0.06408691] 1   1 Match 27\n",
      "\n",
      "[ 0.4387207   0.45532227  0.6484375   0.08483887 -0.71484375 -1.1230469 ] 2   0 \n",
      "[-0.79248047  0.68652344 -0.01107025  0.1394043   0.07946777  0.05474854] 1   5 \n",
      "[-1.6347656  -0.01737976 -0.04669189  0.73095703  0.8442383   0.4675293 ] 4   1 \n",
      "[ 0.5566406   0.63134766  0.22155762 -0.20483398 -1.1054688  -0.9423828 ] 1   1 Match 28\n",
      "\n",
      "[-1.3242188  -0.00348473  0.31591797  0.9897461   0.50683594 -0.37280273] 3   3 Match 29\n",
      "\n",
      "[-0.06549072  0.65283203  0.421875   -0.00782776 -0.3798828  -0.5649414 ] 1   2 \n",
      "[ 0.16906738  0.60498047  0.5385742   0.14294434 -0.6176758  -1.0634766 ] 1   2 \n",
      "[-1.7294922  -0.13171387 -0.47143555  0.6035156   1.1855469   1.0654297 ] 4   4 Match 30\n",
      "\n",
      "[-1.5634766  -0.08984375 -0.05990601  0.875       0.95947266  0.48779297] 4   4 Match 31\n",
      "\n",
      "[-1.4443359   0.36938477  0.38427734  0.6982422   0.55371094  0.01557922] 3   5 \n",
      "[-0.7363281   0.3857422   0.5932617   0.63964844  0.01983643 -0.5859375 ] 3   1 \n",
      "[-1.5429688   0.1472168  -0.14086914  0.59033203  0.7817383   0.6557617 ] 4   1 \n",
      "[-1.5673828e+00  1.2058020e-04  5.5859375e-01  9.6533203e-01\n",
      "  7.0410156e-01 -2.2619629e-01] 3   4 \n",
      "[-1.7294922  -0.15344238 -0.3449707   0.66503906  1.1679688   0.87060547] 4   5 \n",
      "[-1.0351562   0.5083008   0.3371582   0.32055664  0.40478516 -0.12091064] 1   1 Match 32\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.7001953  -0.08178711  0.14575195  1.0048828   0.8105469   0.2487793 ] 3   2 \n",
      "[ 0.0447998   0.84033203 -0.05731201 -0.20080566 -0.6699219  -0.44067383] 1   2 \n",
      "[ 0.3840332   0.5961914   0.22924805 -0.16699219 -0.69189453 -0.6479492 ] 1   1 Match 33\n",
      "\n",
      "[-0.41552734  0.51708984  0.07446289  0.15307617 -0.16552734 -0.12329102] 1   2 \n",
      "[-1.5361328   0.06719971  0.3930664   0.59228516  0.7026367  -0.00733566] 4   1 \n",
      "[-0.49121094  0.46801758  0.28149414  0.4819336  -0.14526367 -0.48950195] 3   2 \n",
      "[-1.6689453  -0.07092285 -0.5053711   0.5942383   0.9916992   1.0449219 ] 5   5 Match 34\n",
      "\n",
      "[-0.8623047   0.6245117  -0.15368652 -0.01268768  0.15576172  0.44335938] 1   5 \n",
      "[-1.8144531  -0.11853027 -0.29467773  0.6923828   0.99658203  1.0429688 ] 5   2 \n",
      "[-0.73535156  0.14001465  0.42016602  0.80322266 -0.03515625 -0.68847656] 3   3 Match 35\n",
      "\n",
      "[ 0.03643799  0.3684082   0.7685547   0.56591797 -0.69970703 -1.3632812 ] 2   1 \n",
      "[-1.796875   -0.29345703 -0.44335938  0.9140625   1.1972656   1.1298828 ] 4   2 \n",
      "[-0.43774414  0.66064453 -0.01412964  0.12792969 -0.1541748  -0.01419067] 1   1 Match 36\n",
      "\n",
      "[-0.6669922   0.34472656  0.36523438  0.5336914  -0.03933716 -0.69091797] 3   4 \n",
      "[-1.7167969  -0.42822266  0.09796143  1.3056641   0.8120117   0.13464355] 3   3 Match 37\n",
      "\n",
      "[-1.8105469  -0.1899414  -0.04373169  0.94433594  0.9863281   0.6621094 ] 4   4 Match 38\n",
      "\n",
      "[-1.7539062  -0.2788086  -0.16894531  1.0283203   1.0849609   0.5361328 ] 4   3 \n",
      "[ 0.38964844  0.34765625  0.50097656 -0.06451416 -0.68896484 -1.1015625 ] 2   3 \n",
      "[-1.6337891 -0.2175293 -0.5776367  0.6513672  1.1445312  0.9790039] 4   5 \n",
      "[ 0.45458984  0.71435547  0.21765137 -0.44995117 -0.79296875 -0.6074219 ] 1   4 \n",
      "[-0.06225586  0.65625    -0.00140381 -0.0061264  -0.56152344 -0.36669922] 1   1 Match 39\n",
      "\n",
      "[-1.265625    0.06933594  0.48999023  1.0126953   0.35546875 -0.46704102] 3   2 \n",
      "[-1.4345703   0.09936523  0.18408203  0.7133789   0.5527344   0.46142578] 3   1 \n",
      "[-1.5478516  -0.1439209   0.07562256  0.54541016  0.9667969   0.58496094] 4   3 \n",
      "[-0.40454102  0.6933594   0.11077881 -0.18029785 -0.11322021 -0.0051384 ] 1   2 \n",
      "[-0.5029297   0.42114258  0.5839844   0.50439453 -0.20422363 -0.53564453] 2   3 \n",
      "[-1.6435547  -0.36987305 -0.37353516  0.9121094   1.2070312   0.93847656] 4   5 \n",
      "[-1.8095703  -0.10943604 -0.24230957  0.6948242   1.1083984   0.98535156] 4   4 Match 40\n",
      "\n",
      "[-0.9116211   0.39868164  0.4482422   0.640625   -0.0791626  -0.80078125] 3   4 \n",
      "[-1.7783203  -0.00787354 -0.37597656  0.4116211   1.2285156   0.99121094] 4   5 \n",
      "[-1.5566406  -0.38598633  0.25170898  1.0332031   0.9589844   0.10522461] 3   5 \n",
      "[ 0.26635742  0.59375     0.58691406  0.1652832  -0.8432617  -1.1484375 ] 1   1 Match 41\n",
      "\n",
      "[-0.90283203  0.27514648  0.02436829  0.40771484  0.10321045  0.34765625] 3   4 \n",
      "[-0.64501953  1.2978516  -0.5644531  -0.18103027 -0.40551758  0.21606445] 1   3 \n",
      "[-1.5253906   0.1595459  -0.2548828   0.39404297  0.8022461   0.85302734] 5   3 \n",
      "[-1.1572266   0.14953613  0.2770996   0.8017578   0.3552246  -0.20800781] 3   5 \n",
      "[-1.1503906   0.74560547  0.05752563  0.33007812  0.17126465  0.05749512] 1   3 \n",
      "[-1.8984375  -0.31445312 -0.2993164   0.66503906  1.1835938   0.94970703] 4   2 \n",
      "[-0.27416992  0.64990234  0.15649414 -0.16369629 -0.2290039  -0.06500244] 1   3 \n",
      "[-0.06329346  0.71484375 -0.07501221 -0.21533203 -0.44555664 -0.14648438] 1   1 Match 42\n",
      "\n",
      "[-0.76464844  0.45776367  0.6459961   0.4892578   0.09942627 -0.6821289 ] 2   3 \n",
      "[-1.703125   -0.43774414 -0.17480469  1.1865234   1.0273438   0.5654297 ] 3   2 \n",
      "[-0.6323242   0.39697266 -0.12072754  0.07598877  0.01730347  0.42114258] 5   5 Match 43\n",
      "\n",
      "[-1.7802734  -0.21435547 -0.45654297  0.6591797   1.2421875   1.1025391 ] 4   5 \n",
      "[-0.02709961  1.1572266  -0.4321289  -0.23876953 -0.99560547 -0.29882812] 1   3 \n",
      "[-1.1376953   0.0196228   0.33374023  0.65722656  0.5444336   0.09057617] 3   5 \n",
      "[-1.1386719   0.69628906 -0.32202148  0.08569336  0.30126953  0.4753418 ] 1   3 \n",
      "[-0.48217773  0.4638672   0.8823242   0.5810547  -0.16821289 -1.1679688 ] 2   4 \n",
      "[-0.8544922   0.32006836  0.43286133  0.7163086   0.09545898 -0.6044922 ] 3   1 \n",
      "[-1.8427734  -0.10620117 -0.29248047  0.62841797  1.2421875   0.95410156] 4   2 \n",
      "[-1.4257812   0.46875     0.09960938  0.47875977  0.5317383   0.32470703] 4   5 \n",
      "[-1.703125    0.0456543  -0.17016602  0.4296875   1.0322266   0.68408203] 4   3 \n",
      "[-1.0947266   0.7607422   0.12298584  0.42895508  0.12420654 -0.2286377 ] 1   3 \n",
      "[ 0.28393555  0.8696289  -0.03256226 -0.4428711  -0.80908203 -0.2998047 ] 1   1 Match 44\n",
      "\n",
      "[-1.8251953  -0.22351074 -0.1854248   0.9838867   0.8930664   0.62597656] 3   1 \n",
      "[-0.13708496  0.48266602 -0.04272461 -0.09375    -0.39501953  0.07214355] 1   0 \n",
      "[-0.02639771  0.73535156  0.4086914   0.09179688 -0.6791992  -0.7363281 ] 1   0 \n",
      "[-1.3974609   0.27978516 -0.1706543   0.5854492   0.39526367  0.32861328] 3   4 \n",
      "[-1.7939453   0.05667114 -0.06210327  0.6176758   0.9501953   0.8852539 ] 4   1 \n",
      "[-1.328125    0.49072266  0.04498291  0.27026367  0.4465332   0.20910645] 1   5 \n",
      "[-1.0615234   0.24511719  0.55322266  0.99853516  0.28979492 -0.5996094 ] 3   2 \n",
      "[-1.6855469  -0.20581055 -0.53564453  0.5991211   1.2412109   1.0283203 ] 4   5 \n",
      "[-1.5449219   0.5786133  -0.20874023  0.34765625  0.4506836   0.6748047 ] 5   5 Match 45\n",
      "\n",
      "[-1.0263672   0.24682617 -0.17956543  0.7548828   0.25610352 -0.01158905] 3   0 \n",
      "[-0.05392456  0.61816406  0.43774414  0.11956787 -0.5546875  -0.85546875] 1   1 Match 46\n",
      "\n",
      "[-0.89453125  1.1640625  -0.58935547  0.02459717 -0.22302246  0.53808594] 1   1 Match 47\n",
      "\n",
      "[ 0.37231445  0.8535156  -0.25219727 -0.4128418  -0.828125   -0.375     ] 1   2 \n",
      "[-0.74316406  0.18151855  0.61572266  0.46264648 -0.04037476 -0.58740234] 2   3 \n",
      "[-1.1210938   0.23632812  0.58154297  0.6152344   0.34545898 -0.35668945] 3   3 Match 48\n",
      "\n",
      "[ 0.12493896  0.62597656  0.03878784 -0.36499023 -0.52441406 -0.05239868] 1   1 Match 49\n",
      "\n",
      "[ 0.3972168   0.7685547   0.0668335  -0.10430908 -0.8930664  -0.79052734] 1   1 Match 50\n",
      "\n",
      "[-1.5185547  -0.22839355  0.11187744  1.0878906   0.8339844   0.15148926] 3   2 \n",
      "[-0.56396484  0.06774902  0.34814453  0.01235199 -0.18859863 -0.07055664] 2   3 \n",
      "[-1.4199219   0.0635376   0.3095703   1.0537109   0.63964844 -0.01873779] 3   3 Match 51\n",
      "\n",
      "[-1.4130859  -0.05239868  0.24804688  0.8051758   0.8198242   0.05389404] 4   1 \n",
      "[-0.7011719   0.60791016  0.34887695  0.3232422  -0.1307373  -0.26733398] 1   1 Match 52\n",
      "\n",
      "[-1.5957031  -0.13537598 -0.0091629   1.09375     0.9589844   0.4465332 ] 3   4 \n",
      "[-1.7041016  -0.11529541 -0.2388916   0.58691406  1.1113281   0.8129883 ] 4   5 \n",
      "[-1.8115234   0.17199707  0.01233673  0.61621094  0.99853516  0.7216797 ] 4   3 \n",
      "[-1.4755859  -0.25170898 -0.07971191  1.0302734   0.8261719   0.45751953] 3   3 Match 53\n",
      "\n",
      "[-0.41015625  0.5566406   0.37597656  0.14147949 -0.23339844 -0.4020996 ] 1   1 Match 54\n",
      "\n",
      "[-0.9584961   0.5917969   0.07910156  0.0071907   0.01742554  0.14807129] 1   3 \n",
      "[-0.25097656  1.1923828  -0.2993164  -0.17883301 -0.6503906   0.0042038 ] 1   3 \n",
      "[ 0.44458008  0.8208008   0.10638428 -0.3779297  -0.8095703  -0.6411133 ] 1   0 \n",
      "[-0.5546875   0.8383789   0.02955627 -0.03112793 -0.18054199 -0.09344482] 1   2 \n",
      "[-0.46777344  0.31420898  0.49365234  0.34545898 -0.07037354 -0.52490234] 2   5 \n",
      "[-0.9160156   0.72021484  0.44677734  0.5908203  -0.2956543  -0.6074219 ] 1   2 \n",
      "[-1.7910156  -0.19494629 -0.50146484  0.5595703   1.2392578   1.0976562 ] 4   4 Match 55\n",
      "\n",
      "[ 0.28295898  0.83496094 -0.07574463 -0.3955078  -0.82470703 -0.32226562] 1   3 \n",
      "[ 0.01608276  0.92041016 -0.11474609 -0.27929688 -0.7475586  -0.41357422] 1   4 \n",
      "[-1.6142578  -0.27783203  0.38061523  1.0966797   0.91015625  0.09515381] 3   4 \n",
      "[-0.26489258  0.65527344 -0.28173828  0.08197021 -0.76416016 -0.24206543] 1   1 Match 56\n",
      "\n",
      "[-1.4736328   0.02528381  0.1977539   0.67578125  0.48535156  0.32763672] 3   4 \n",
      "[-0.07653809  0.24414062  0.39111328  0.35253906 -0.23791504 -0.6142578 ] 2   2 Match 57\n",
      "\n",
      "[-1.3808594   0.09057617  0.4560547   0.8730469   0.55371094 -0.09295654] 3   1 \n",
      "[-1.4960938   0.0508728   0.13574219  0.9667969   0.79052734  0.02377319] 3   2 \n",
      "[ 0.65722656  0.62646484  0.11273193 -0.45092773 -0.91015625 -0.5722656 ] 0   3 \n",
      "[-1.5419922e+00 -1.2588501e-04 -6.2072754e-02  4.1284180e-01\n",
      "  9.1503906e-01  6.6845703e-01] 4   1 \n",
      "[-1.6787109  -0.29223633  0.01576233  1.078125    0.8364258   0.42919922] 3   3 Match 58\n",
      "\n",
      "[-0.05615234  0.91845703  0.01940918 -0.23999023 -0.5336914  -0.2980957 ] 1   0 \n",
      "[-1.8818359  -0.12548828 -0.11987305  0.9916992   1.0390625   0.67626953] 4   3 \n",
      "[-1.5810547  -0.18737793  0.07269287  1.1259766   0.81591797  0.42358398] 3   1 \n",
      "[ 0.5229492   0.48168945  0.42333984 -0.1340332  -0.7988281  -1.0546875 ] 0   0 Match 59\n",
      "\n",
      "[-0.9667969   0.47924805 -0.0725708   0.1965332   0.22521973  0.5263672 ] 5   0 \n",
      "[-1.6582031   0.02853394  0.05133057  0.97802734  0.66503906  0.00901031] 3   1 \n",
      "[-0.72216797  0.5131836   0.4165039   0.33544922 -0.06726074 -0.4597168 ] 1   3 \n",
      "[-0.6328125   0.3869629   0.68896484  0.35083008 -0.0295105  -0.6484375 ] 2   5 \n",
      "[ 0.86865234  0.7006836  -0.03497314 -0.73583984 -0.9892578  -0.44677734] 0   0 Match 60\n",
      "\n",
      "[-1.7060547  -0.02993774 -0.46850586  0.58496094  1.2089844   1.0146484 ] 4   4 Match 61\n",
      "\n",
      "[-0.57666016  0.52490234  0.67529297  0.65625    -0.44970703 -1.2207031 ] 2   2 Match 62\n",
      "\n",
      "[ 0.7495117   0.7182617  -0.09051514 -0.46264648 -0.96875    -0.62060547] 0   2 \n",
      "[-1.1669922   0.5888672   0.02555847  0.3466797   0.28930664  0.38964844] 1   1 Match 63\n",
      "\n",
      "[-1.1777344   0.00927734  0.44091797  0.8515625   0.03375244 -0.44311523] 3   3 Match 64\n",
      "\n",
      "[-0.76904297  0.8979492   0.3486328   0.27148438 -0.22070312 -0.28222656] 1   5 \n",
      "[-0.9199219   0.48657227  0.05496216  0.26464844  0.10272217  0.15820312] 1   4 \n",
      "[-1.2529297   0.21887207  0.1529541   0.49853516  0.5498047   0.3383789 ] 4   1 \n",
      "[-1.2226562  -0.03952026  0.63183594  0.99609375  0.5317383  -0.35913086] 3   0 \n",
      "[-0.69433594  1.2460938  -0.5078125   0.05108643 -0.44311523  0.01104736] 1   1 Match 65\n",
      "\n",
      "[-0.54541016  0.21240234  0.5917969   0.7241211  -0.3330078  -0.9609375 ] 3   3 Match 66\n",
      "\n",
      "[-0.75927734  0.19177246  0.2824707   0.3239746  -0.03530884 -0.04067993] 3   2 \n",
      "[-0.08435059  0.5966797   0.25195312 -0.21374512 -0.4350586  -0.30200195] 1   2 \n",
      "[-0.30004883  0.6035156   0.01976013 -0.14538574 -0.17895508 -0.14233398] 1   5 \n",
      "[-0.7192383   0.6171875   0.14025879  0.21252441  0.04769897  0.0760498 ] 1   3 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6357422  -0.40307617 -0.10455322  1.0175781   1.2880859   0.86279297] 4   4 Match 67\n",
      "\n",
      "[-1.1503906   0.4033203   0.32836914  0.52197266  0.49853516  0.13928223] 3   1 \n",
      "[-0.5991211   0.28881836  0.18481445  0.5107422  -0.0871582  -0.23352051] 3   1 \n",
      "[-1.6650391   0.1229248  -0.06158447  0.79003906  0.9086914   0.5839844 ] 4   1 \n",
      "[-0.07580566  0.96728516 -0.08312988 -0.31811523 -0.5053711  -0.26782227] 1   0 \n",
      "[-0.9716797   0.45166016  0.35791016  0.4970703  -0.03019714 -0.5541992 ] 3   4 \n",
      "[-0.3798828   0.29052734  0.4050293   0.5957031  -0.41796875 -0.6958008 ] 3   1 \n",
      "[-1.3466797  -0.0068779   0.3088379   0.96435547  0.42773438 -0.32958984] 3   3 Match 68\n",
      "\n",
      "[ 0.08807373  0.49682617  0.4104004  -0.12133789 -0.3996582  -0.4189453 ] 1   1 Match 69\n",
      "\n",
      "[-0.6933594   0.25830078  0.07885742  0.1796875  -0.13708496  0.11566162] 1   1 Match 70\n",
      "\n",
      "[-1.7734375   0.04467773 -0.42236328  0.62353516  1.0664062   1.0576172 ] 4   3 \n",
      "[-0.35009766  0.4453125   0.35742188  0.02372742 -0.20349121 -0.25463867] 1   0 \n",
      "[-0.48510742  0.36572266 -0.02429199  0.31860352 -0.11639404 -0.00895691] 1   5 \n",
      "[-0.7363281   0.58496094  0.16479492  0.3881836  -0.13671875  0.09667969] 1   0 \n",
      "[-1.7792969  -0.13574219 -0.4724121   0.6098633   1.1806641   1.0009766 ] 4   3 \n",
      "[-1.7851562  -0.00535583 -0.01856995  0.79248047  0.8227539   0.45776367] 4   2 \n",
      "[-1.0888672   0.13659668  0.42797852  0.49267578  0.44018555  0.03781128] 3   0 \n",
      "[ 0.35253906  0.5883789   0.32714844 -0.09356689 -0.5859375  -0.69433594] 1   0 \n",
      "[-1.5029297  -0.16174316  0.43139648  1.1054688   0.6123047  -0.08868408] 3   4 \n",
      "[-0.3959961   0.61621094  0.66308594  0.34643555 -0.32714844 -0.89990234] 2   1 \n",
      "[-1.171875    0.2919922   0.77734375  0.7636719   0.29711914 -0.7109375 ] 2   2 Match 71\n",
      "\n",
      "[-0.15161133  0.7631836  -0.06237793 -0.20446777 -0.54248047  0.13317871] 1   3 \n",
      "[-1.3798828  -0.25732422  0.2619629   0.87841797  0.7919922   0.26098633] 3   2 \n",
      "[-1.7382812  -0.11682129 -0.46264648  0.59472656  1.2246094   1.0458984 ] 4   5 \n",
      "[-0.8574219   1.2460938  -0.57421875 -0.01701355 -0.26489258  0.3466797 ] 1   3 \n",
      "[-1.7822266   0.20422363 -0.1784668   0.71728516  0.8647461   0.6357422 ] 4   5 \n",
      "[-1.7177734  -0.18579102 -0.46313477  0.58935547  1.1982422   0.94189453] 4   4 Match 72\n",
      "\n",
      "[-1.7910156  -0.37597656 -0.36083984  0.8496094   1.1660156   1.0273438 ] 4   4 Match 73\n",
      "\n",
      "[-1.0576172   0.34350586 -0.02218628  0.3359375   0.43530273  0.20031738] 4   1 \n",
      "[-1.4902344   0.34887695  0.1116333   0.8666992   0.45703125 -0.15234375] 3   4 \n",
      "[-1.4570312  -0.23364258 -0.00803375  0.95458984  0.71484375  0.5571289 ] 3   3 Match 74\n",
      "\n",
      "[-0.46606445  0.6875      0.46850586  0.5703125  -0.41210938 -0.9243164 ] 1   0 \n",
      "[-1.5107422   0.29736328  0.42041016  0.8408203   0.6074219  -0.20776367] 3   3 Match 75\n",
      "\n",
      "[-1.5488281   0.10266113 -0.26245117  0.7338867   0.72558594  0.6010742 ] 3   3 Match 76\n",
      "\n",
      "[-1.0839844   1.0595703  -0.36889648  0.04827881 -0.056427    0.21289062] 1   3 \n",
      "[-1.6884766   0.06835938  0.02697754  0.9121094   0.8413086   0.15270996] 3   4 \n",
      "[-0.7729492   0.5493164   0.265625    0.6176758  -0.08190918 -0.6191406 ] 3   0 \n",
      "[-1.3652344   0.2619629   0.18444824  0.8852539   0.4033203  -0.26513672] 3   1 \n",
      "[-1.7470703  -0.06933594 -0.43969727  0.5185547   1.1289062   1.0449219 ] 4   4 Match 77\n",
      "\n",
      "[ 1.0205078   0.72216797 -0.0859375  -0.7260742  -1.1982422  -0.79589844] 0   1 \n",
      "[-0.48632812  1.1738281  -0.45263672 -0.29711914 -0.49316406  0.09124756] 1   1 Match 78\n",
      "\n",
      "[ 0.21362305  0.6582031  -0.01860046 -0.40527344 -0.70751953 -0.23754883] 1   0 \n",
      "[-1.4707031  -0.07440186  0.25878906  0.62939453  0.7036133   0.38232422] 4   2 \n",
      "[-1.3925781  -0.21325684  0.0243988   0.90771484  0.8833008   0.38671875] 3   4 \n",
      "[-1.6630859   0.18395996  0.16101074  0.79785156  0.67578125  0.09088135] 3   1 \n",
      "[-0.7192383   0.08996582  0.20129395  0.40307617  0.14562988 -0.04412842] 3   2 \n",
      "[-1.8056641  -0.1159668  -0.2331543   0.74072266  1.1328125   0.91259766] 4   4 Match 79\n",
      "\n",
      "[-1.5732422  -0.22998047 -0.5517578   0.62646484  1.1728516   0.96533203] 4   1 \n",
      "[-0.26293945  0.7260742   0.4362793   0.32836914 -0.6616211  -1.0839844 ] 1   1 Match 80\n",
      "\n",
      "[-1.7744141  -0.23095703 -0.26391602  0.7890625   1.1796875   0.9555664 ] 4   3 \n",
      "[-1.6875     -0.05053711 -0.36108398  0.5551758   1.15625     1.0253906 ] 4   5 \n",
      "[ 0.25463867  0.32714844  0.37182617  0.02960205 -0.5605469  -0.54296875] 2   5 \n",
      "[-1.1884766  -0.27563477  0.19677734  0.92529297  0.6152344   0.19006348] 3   4 \n",
      "[ 0.10650635  0.65234375  0.46777344  0.1038208  -0.6191406  -1.140625  ] 1   2 \n",
      "[-0.82714844  0.81152344  0.38378906  0.35473633 -0.0725708  -0.5996094 ] 1   5 \n",
      "[-1.8232422   0.13574219 -0.29663086  0.5185547   0.95654297  1.0839844 ] 5   3 \n",
      "[-1.6181641   0.02734375  0.01171112  0.6430664   1.015625    0.54589844] 4   4 Match 81\n",
      "\n",
      "[-0.6328125   0.66845703  0.47070312  0.2565918   0.01016998 -0.44018555] 1   1 Match 82\n",
      "\n",
      "[-1.5458984   0.05435181 -0.04629517  0.5180664   0.8901367   0.65234375] 4   3 \n",
      "[-0.7861328   0.5161133   0.09594727  0.0617981   0.04440308  0.31347656] 1   1 Match 83\n",
      "\n",
      "[-1.1806641   0.19018555  0.3552246   0.7241211   0.44604492 -0.02825928] 3   4 \n",
      "[-1.5703125   0.41577148  0.36694336  0.8051758   0.46875    -0.09118652] 3   0 \n",
      "[-1.7099609  -0.30200195  0.19628906  1.0507812   1.0664062   0.26757812] 4   5 \n",
      "[-1.8212891  -0.14440918 -0.2705078   0.6850586   1.234375    1.0117188 ] 4   5 \n",
      "[-0.7885742   0.31420898  0.27319336  0.453125    0.12078857 -0.43359375] 3   3 Match 84\n",
      "\n",
      "[-1.7851562  -0.02838135 -0.10498047  0.7597656   0.7871094   0.515625  ] 4   3 \n",
      "[-1.3125     -0.1628418  -0.29296875  0.89404297  0.87841797  0.4963379 ] 3   3 Match 85\n",
      "\n",
      "[-1.1064453   0.43652344  0.13879395  0.5332031   0.28881836 -0.20751953] 3   4 \n",
      "[-1.6875     -0.5620117  -0.17346191  1.1542969   1.2089844   0.7451172 ] 4   4 Match 86\n",
      "\n",
      "[-0.45922852 -0.03366089  0.43481445  0.40112305 -0.10614014 -0.24804688] 2   2 Match 87\n",
      "\n",
      "[-1.2998047   0.13549805  0.04376221  0.52197266  0.5292969   0.37963867] 4   1 \n",
      "[-0.16687012  0.8222656   0.16894531 -0.22741699 -0.37817383 -0.35351562] 1   1 Match 88\n",
      "\n",
      "[ 0.4753418   0.3154297   0.56689453 -0.09197998 -0.9111328  -1.1748047 ] 2   0 \n",
      "[-1.2490234  -0.02589417  0.16687012  0.47998047  0.63964844  0.5209961 ] 4   5 \n",
      "[-1.7236328   0.05258179 -0.14379883  0.6772461   0.87646484  0.5385742 ] 4   3 \n",
      "[-1.3691406   0.0355835  -0.32617188  0.43041992  0.7573242   0.7631836 ] 5   5 Match 89\n",
      "\n",
      "[-1.3613281   0.49731445  0.3930664   0.86083984  0.30419922 -0.3996582 ] 3   1 \n",
      "[-1.6767578   0.15686035 -0.328125    0.38330078  0.9863281   0.8823242 ] 4   5 \n",
      "[-1.046875   -0.00580597  0.3400879   0.9482422   0.21191406 -0.4645996 ] 3   4 \n",
      "[-1.5976562  -0.31347656 -0.13928223  0.96875     0.96240234  0.48632812] 3   3 Match 90\n",
      "\n",
      "[ 0.13476562  0.6352539   0.36767578 -0.02854919 -0.5991211  -0.85058594] 1   2 \n",
      "[-0.48510742  0.546875    0.24584961  0.03588867 -0.17260742 -0.01513672] 1   0 \n",
      "[ 0.48828125  0.28564453  0.35620117 -0.05352783 -0.6333008  -0.9345703 ] 0   1 \n",
      "[-1.0439453   0.16418457  0.01152802  0.46972656  0.48486328  0.14770508] 4   5 \n",
      "[-0.47802734  0.72314453  0.125       0.39331055 -0.33251953 -0.6098633 ] 1   3 \n",
      "[-1.3886719   0.14416504  0.11114502  0.46948242  0.6123047   0.57714844] 4   4 Match 91\n",
      "\n",
      "[-0.82470703  0.39990234  0.18701172  0.51464844 -0.2241211  -0.34887695] 3   1 \n",
      "[-1.8046875  -0.10424805 -0.328125    0.5283203   1.2041016   0.91015625] 4   1 \n",
      "[-1.1845703   0.10864258  0.4091797   0.9067383   0.4567871  -0.43798828] 3   3 Match 92\n",
      "\n",
      "[-1.7744141   0.03842163 -0.22753906  0.5966797   0.9506836   0.7368164 ] 4   4 Match 93\n",
      "\n",
      "[ 0.04324341  0.7993164   0.33398438 -0.10473633 -0.51660156 -0.61279297] 1   1 Match 94\n",
      "\n",
      "[-0.07458496  0.49023438  0.40625     0.31762695 -0.49853516 -0.90478516] 1   3 \n",
      "[-1.7861328   0.17126465 -0.28149414  0.71728516  0.95410156  0.8261719 ] 4   5 \n",
      "[-1.734375   -0.09661865 -0.17150879  0.70751953  0.96777344  0.8076172 ] 4   2 \n",
      "[-1.3330078   0.02046204  0.5571289   1.1064453   0.39672852 -0.4350586 ] 3   3 Match 95\n",
      "\n",
      "[-1.2714844   0.99658203 -0.13842773  0.34423828  0.23095703  0.18457031] 1   3 \n",
      "[ 0.3791504   0.37426758  0.46118164  0.0418396  -0.6904297  -0.85253906] 2   2 Match 96\n",
      "\n",
      "[ 0.19360352  0.8076172  -0.39501953 -0.37060547 -0.8564453  -0.46191406] 1   3 \n",
      "[-0.01288605  0.5336914   0.3725586   0.11096191 -0.4699707  -0.49951172] 1   0 \n",
      "[-1.7138672  -0.34399414 -0.41845703  0.8598633   1.2451172   1.0117188 ] 4   4 Match 97\n",
      "\n",
      "[ 0.0760498   0.61572266  0.27783203 -0.10076904 -0.5917969  -0.6191406 ] 1   1 Match 98\n",
      "\n",
      "[-1.6972656   0.1463623  -0.29296875  0.49951172  1.0107422   0.84228516] 4   3 \n",
      "[-1.4580078   0.40576172  0.11212158  0.75634766  0.62109375 -0.00283241] 3   3 Match 99\n",
      "\n",
      "[-1.2519531   0.96875    -0.25268555  0.22192383  0.05044556  0.04492188] 1   5 \n",
      "[-1.7070312  -0.29052734 -0.13085938  0.91259766  1.0849609   0.7006836 ] 4   4 Match 100\n",
      "\n",
      "[ 0.7705078   0.7758789   0.08532715 -0.53808594 -0.9291992  -0.6640625 ] 1   1 Match 101\n",
      "\n",
      "[-1.0175781   0.5541992   0.34643555  0.43945312  0.2998047  -0.21325684] 1   2 \n",
      "[-1.8164062  -0.18017578 -0.5371094   0.64746094  1.2763672   1.2666016 ] 4   4 Match 102\n",
      "\n",
      "[-0.53125     0.54052734  0.45092773  0.33251953 -0.05004883 -0.5917969 ] 1   1 Match 103\n",
      "\n",
      "[ 0.6298828   0.71240234 -0.0682373  -0.5996094  -0.75683594 -0.4909668 ] 1   1 Match 104\n",
      "\n",
      "[-0.77001953  1.2392578  -0.4572754  -0.10058594 -0.3881836   0.18981934] 1   1 Match 105\n",
      "\n",
      "[-0.46557617  0.1583252   0.31054688  0.1550293   0.02096558 -0.10198975] 2   2 Match 106\n",
      "\n",
      "[-1.4589844   0.30004883  0.112854    0.3671875   0.78027344  0.49780273] 4   0 \n",
      "[-0.5776367   0.48095703  0.3125      0.14660645  0.00483704 -0.1171875 ] 1   4 \n",
      "[-0.24499512  0.6459961   0.41870117  0.06628418 -0.3161621  -0.5996094 ] 1   1 Match 107\n",
      "\n",
      "[-1.8525391  -0.3659668  -0.36645508  0.90966797  1.2822266   1.1601562 ] 4   4 Match 108\n",
      "\n",
      "[-0.8671875   0.22607422  0.26367188  0.5986328   0.3046875  -0.13330078] 3   1 \n",
      "[-1.8613281  -0.19836426 -0.42041016  0.50683594  1.2548828   1.0322266 ] 4   0 \n",
      "[-0.39746094  0.75683594 -0.00676346 -0.04742432 -0.20092773 -0.01741028] 1   1 Match 109\n",
      "\n",
      "[-1.8261719  -0.06994629 -0.5253906   0.46777344  1.1972656   1.0771484 ] 4   5 \n",
      "[-1.5976562   0.17150879 -0.24194336  0.31982422  1.0263672   0.8461914 ] 4   4 Match 110\n",
      "\n",
      "[-1.7412109  -0.06115723 -0.35131836  0.7036133   1.1210938   0.96875   ] 4   5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1494141   0.21838379  0.38354492  0.8901367   0.32080078 -0.41430664] 3   1 \n",
      "[-1.6533203  -0.48046875 -0.29125977  0.8959961   1.2158203   0.92529297] 4   3 \n",
      "[-1.7646484  -0.29223633 -0.30786133  0.8442383   1.0810547   0.81396484] 4   2 \n",
      "[-1.6962891   0.0585022  -0.11743164  0.65234375  1.0214844   0.5571289 ] 4   2 \n",
      "[ 0.00727844  0.48120117  0.12463379  0.18737793 -0.2607422  -0.4560547 ] 1   1 Match 111\n",
      "\n",
      "[-1.6298828  -0.5185547   0.08239746  1.0029297   0.88427734  0.54248047] 3   4 \n",
      "[-1.4677734   0.3942871  -0.27905273  0.27001953  0.91259766  0.84521484] 4   1 \n",
      "[-1.2783203   0.55078125  0.45654297  0.828125    0.06768799 -0.7524414 ] 3   4 \n",
      "[-1.7666016  -0.20227051 -0.06018066  0.8647461   0.91503906  0.5644531 ] 4   3 \n",
      "[-0.47216797  0.77246094  0.07055664 -0.00107384 -0.2277832  -0.00348663] 1   3 \n",
      "[-1.7568359  -0.03515625 -0.06872559  0.79296875  1.0126953   0.46606445] 4   3 \n",
      "[-1.390625   -0.1973877   0.11303711  1.1367188   0.73095703  0.14428711] 3   3 Match 112\n",
      "\n",
      "[-1.0224609   0.27197266 -0.01180267  0.28125     0.23999023  0.48046875] 5   2 \n",
      "[-1.2529297   0.4338379   0.15307617  0.72216797  0.3137207  -0.11901855] 3   1 \n",
      "[-1.3125      0.3720703   0.27954102  0.73095703  0.17578125 -0.09088135] 3   3 Match 113\n",
      "\n",
      "[-0.9472656  -0.03479004  0.1829834   0.5097656   0.44702148  0.1114502 ] 3   3 Match 114\n",
      "\n",
      "[-0.68652344  0.4790039   0.50146484  0.88623047 -0.31103516 -1.0976562 ] 3   1 \n",
      "[-1.2568359   1.1015625  -0.42871094  0.19152832 -0.01573181  0.19665527] 1   1 Match 115\n",
      "\n",
      "[ 0.29125977  0.7597656   0.12597656 -0.38378906 -0.7270508  -0.51953125] 1   1 Match 116\n",
      "\n",
      "[-0.48999023  0.50341797  0.49194336  0.3149414  -0.07659912 -0.3762207 ] 1   4 \n",
      "[-0.97558594  0.1463623   0.71728516  0.99560547  0.20959473 -0.65966797] 3   3 Match 117\n",
      "\n",
      "[-1.6845703   0.07232666  0.06341553  0.7739258   0.98535156  0.47045898] 4   3 \n",
      "[-1.0976562   0.3935547  -0.34204102  0.39526367 -0.04684448 -0.13647461] 3   1 \n",
      "[-1.4121094   0.07641602  0.22253418  0.6015625   0.57666016  0.06610107] 3   4 \n",
      "[-1.75       -0.22009277 -0.54785156  0.6010742   1.1552734   1.0546875 ] 4   4 Match 118\n",
      "\n",
      "[-1.4042969   0.02290344  0.4020996   1.0400391   0.2446289  -0.4868164 ] 3   4 \n",
      "[-0.36669922  0.43920898 -0.02290344  0.5332031  -0.34204102 -0.3840332 ] 3   4 \n",
      "[ 0.60791016  0.75878906  0.08734131 -0.35595703 -0.86328125 -0.76953125] 1   0 \n",
      "[-0.84765625  0.6928711   0.13171387  0.5571289  -0.15319824 -0.4777832 ] 1   1 Match 119\n",
      "\n",
      "[-1.7861328  -0.29785156 -0.49438477  0.62597656  1.2460938   1.1171875 ] 4   4 Match 120\n",
      "\n",
      "[-1.7070312  -0.24108887 -0.20043945  0.8256836   1.1513672   0.9111328 ] 4   5 \n",
      "[-0.26464844  0.64990234  0.0294342  -0.21142578 -0.3305664  -0.11242676] 1   1 Match 121\n",
      "\n",
      "[-1.7832031  -0.3203125  -0.33544922  0.90625     1.1455078   0.9394531 ] 4   3 \n",
      "[-0.60058594  0.17382812  0.06677246  0.57666016 -0.28295898 -0.2553711 ] 3   3 Match 122\n",
      "\n",
      "[-1.7207031  -0.07855225 -0.34765625  0.5854492   1.1972656   1.0009766 ] 4   4 Match 123\n",
      "\n",
      "[-0.9321289   0.58984375  0.3581543   0.52734375  0.11523438 -0.14196777] 1   3 \n",
      "[-0.7001953   0.4741211   0.68115234  0.7314453  -0.10241699 -1.0546875 ] 3   1 \n",
      "[ 0.00199127  0.7524414   0.27319336 -0.29711914 -0.5371094  -0.35742188] 1   4 \n",
      "[-1.7646484  -0.21118164 -0.34326172  0.7285156   1.1503906   0.82421875] 4   2 \n",
      "[-1.1298828   0.7480469   0.17114258  0.5332031  -0.00543594 -0.21862793] 1   0 \n",
      "[-0.3166504   0.5966797   0.13134766  0.02159119 -0.17749023  0.01565552] 1   1 Match 124\n",
      "\n",
      "[-1.5458984   0.45361328  0.12878418  0.69091797  0.4975586   0.17541504] 3   1 \n",
      "[-1.7109375  -0.20983887 -0.1595459   0.71533203  1.1464844   0.6352539 ] 4   5 \n",
      "[-1.3691406   0.1315918   0.36669922  0.9199219   0.6582031  -0.16186523] 3   4 \n",
      "[-1.5732422   0.21862793 -0.29858398  0.30371094  0.8457031   0.9614258 ] 5   4 \n",
      "[-0.20458984  0.3869629   0.734375    0.26293945 -0.25048828 -0.8041992 ] 2   0 \n",
      "[-1.5986328  -0.08190918 -0.09008789  0.83447266  0.9746094   0.6621094 ] 4   5 \n",
      "[-1.7978516  -0.36083984 -0.47607422  0.94433594  1.1396484   0.99853516] 4   5 \n",
      "[ 0.29638672  0.75097656 -0.22827148 -0.42285156 -0.7578125  -0.5102539 ] 1   1 Match 125\n",
      "\n",
      "[ 0.7211914   0.74560547  0.23083496 -0.55371094 -0.9926758  -0.73095703] 1   3 \n",
      "[-1.5898438  -0.04837036 -0.11376953  0.7368164   0.98046875  0.66748047] 4   3 \n",
      "[-1.4794922   0.23303223  0.12231445  0.6430664   0.60009766  0.18334961] 3   1 \n",
      "[-1.0380859   0.23339844  0.5205078   1.0361328   0.23962402 -0.5727539 ] 3   3 Match 126\n",
      "\n",
      "[-1.8066406  -0.22265625  0.09637451  1.0302734   0.9453125   0.4951172 ] 3   4 \n",
      "[-1.6533203  -0.42456055 -0.32617188  0.8515625   1.1982422   0.8461914 ] 4   3 \n",
      "[-0.15039062  0.71435547  0.3671875   0.00084162 -0.38012695 -0.4423828 ] 1   5 \n",
      "[-1.4023438  -0.02456665  0.3491211   0.6796875   0.75439453  0.10559082] 4   1 \n",
      "[-1.1494141   0.4255371  -0.09020996  0.20996094  0.4934082   0.47875977] 4   2 \n",
      "[-1.40625     0.22265625  0.07672119  0.7470703   0.79589844  0.21289062] 4   4 Match 127\n",
      "\n",
      "[-1.0166016   0.60498047  0.4248047   0.5566406   0.03738403 -0.49804688] 1   3 \n",
      "[-1.671875   -0.16247559 -0.2524414   0.87939453  1.0244141   0.72509766] 4   2 \n",
      "[-1.0380859   0.5053711   0.6489258   0.7324219   0.14160156 -0.5839844 ] 3   4 \n",
      "[-0.7363281   0.38842773  0.47265625  0.4934082   0.09747314 -0.4777832 ] 3   3 Match 128\n",
      "\n",
      "[ 0.25952148  0.3828125   0.2854004  -0.22705078 -0.6044922  -0.4399414 ] 1   4 \n",
      "[-1.6240234   0.1315918  -0.35327148  0.37426758  1.1035156   0.8881836 ] 4   0 \n",
      "[-1.8300781  -0.2841797  -0.31274414  0.95996094  1.2109375   1.0742188 ] 4   5 \n",
      "[-1.6015625  -0.21459961  0.3798828   1.1484375   0.7714844  -0.08172607] 3   5 \n",
      "[-0.4465332   0.54296875  0.1541748   0.08538818 -0.30419922 -0.02281189] 1   2 \n",
      "[-0.8173828   0.3095703   0.5834961   0.7807617  -0.16760254 -0.9770508 ] 3   1 \n",
      "[-1.5986328   0.04223633 -0.16662598  0.7631836   0.92529297  0.5029297 ] 4   4 Match 129\n",
      "\n",
      "[ 0.074646    0.5136719   0.3857422  -0.25561523 -0.37817383 -0.42211914] 1   2 \n",
      "[-1.4248047  -0.27246094  0.14526367  1.0976562   0.7817383   0.00531769] 3   3 Match 130\n",
      "\n",
      "[-1.5019531   0.06567383  0.22473145  0.99316406  0.64941406  0.02424622] 3   2 \n",
      "[ 0.73876953  0.6254883   0.12585449 -0.39770508 -1.0859375  -0.8149414 ] 0   1 \n",
      "[-0.7236328   0.49926758  0.12164307  0.105896   -0.05184937  0.34106445] 1   5 \n",
      "[-0.36865234  0.36621094  0.64160156  0.57714844 -0.23461914 -1.1679688 ] 2   1 \n",
      "[ 0.19250488  0.53564453  0.32836914  0.05227661 -0.6845703  -0.6669922 ] 1   1 Match 131\n",
      "\n",
      "[-0.8203125   0.32836914  0.8129883   0.8701172  -0.30004883 -1.2294922 ] 3   3 Match 132\n",
      "\n",
      "[-1.6289062  -0.28222656  0.13439941  1.1269531   0.98095703  0.29223633] 3   3 Match 133\n",
      "\n",
      "[-1.2744141   0.02171326  0.14440918  0.98046875  0.6196289  -0.12280273] 3   4 \n",
      "[-1.578125   -0.5263672  -0.29248047  1.1162109   1.25        0.78564453] 4   4 Match 134\n",
      "\n",
      "[-0.8808594   0.4934082   0.31762695  0.5341797   0.21447754 -0.29785156] 3   5 \n",
      "[-1.5957031  -0.3828125  -0.09106445  1.0478516   1.0224609   0.47827148] 3   3 Match 135\n",
      "\n",
      "[-1.7851562  -0.30249023  0.01855469  1.0947266   1.0302734   0.49316406] 3   1 \n",
      "[-0.7397461   0.41748047  0.9038086   0.7944336  -0.1430664  -1.21875   ] 2   1 \n",
      "[-1.9619141   0.03997803  0.01400757  1.1992188   0.69677734  0.23913574] 3   5 \n",
      "[-0.54248047  0.38183594  0.8730469   0.7836914  -0.40283203 -1.1523438 ] 2   2 Match 136\n",
      "\n",
      "[-1.7841797  -0.22668457 -0.48291016  0.796875    1.1992188   1.1035156 ] 4   3 \n",
      "[-1.7519531  -0.17382812 -0.12524414  0.9472656   1.0849609   0.65722656] 4   4 Match 137\n",
      "\n",
      "[-1.5507812  -0.0378418   0.06872559  0.6176758   0.6777344   0.66308594] 4   1 \n",
      "[ 0.08117676  0.44311523  0.5834961   0.35595703 -0.4519043  -1.0615234 ] 2   3 \n",
      "[-1.5263672  -0.06115723 -0.03701782  0.73095703  0.8510742   0.39892578] 4   1 \n",
      "[-0.29785156  0.73339844  0.4584961   0.09844971 -0.21557617 -0.6591797 ] 1   3 \n",
      "[-0.5498047   0.67871094  0.6665039   0.5810547  -0.4086914  -1.1464844 ] 1   1 Match 138\n",
      "\n",
      "[-1.1474609   0.06945801  0.19360352  0.64697266  0.6123047   0.04772949] 3   3 Match 139\n",
      "\n",
      "[ 0.45214844  0.76123047  0.40942383 -0.12866211 -0.82714844 -0.85498047] 1   2 \n",
      "[-1.5439453   0.00401688  0.3149414   0.94970703  0.69140625 -0.29711914] 3   5 \n",
      "[-0.02607727  0.73535156 -0.06256104 -0.37548828 -0.41455078 -0.04431152] 1   1 Match 140\n",
      "\n",
      "[-1.7617188  -0.27001953 -0.3803711   0.64746094  1.2246094   1.0058594 ] 4   3 \n",
      "[-1.1455078   0.49853516  0.5258789   0.8852539   0.12011719 -0.8022461 ] 3   3 Match 141\n",
      "\n",
      "[-0.19885254  0.24780273  0.52197266  0.00245857 -0.19165039 -0.55078125] 2   3 \n",
      "[-1.0810547   0.04602051  0.61035156  0.88916016  0.33544922 -0.45898438] 3   3 Match 142\n",
      "\n",
      "[-1.5341797  -0.25878906  0.22558594  1.2001953   0.73828125 -0.01844788] 3   4 \n",
      "[-1.3818359  -0.02609253  0.18676758  0.5625      0.6435547   0.4675293 ] 4   3 \n",
      "[-0.83496094  0.3786621   0.7026367   1.0351562  -0.23657227 -1.1972656 ] 3   3 Match 143\n",
      "\n",
      "[-1.2626953   0.29077148  0.49780273  0.74365234  0.28564453 -0.3005371 ] 3   5 \n",
      "[-1.8310547  -0.10205078 -0.14013672  0.86035156  1.0761719   0.65625   ] 4   4 Match 144\n",
      "\n",
      "[-1.5537109  -0.24902344  0.04663086  0.8588867   1.0107422   0.34399414] 4   5 \n",
      "[-0.67089844  0.78759766 -0.06646729 -0.10620117 -0.1965332   0.18908691] 1   1 Match 145\n",
      "\n",
      "[-0.6948242   0.11962891  0.50878906  0.38110352  0.07446289 -0.3173828 ] 2   2 Match 146\n",
      "\n",
      "[-1.5058594  -0.2800293   0.02224731  0.8251953   0.8691406   0.68066406] 4   4 Match 147\n",
      "\n",
      "[-1.5205078   0.22265625  0.12915039  0.96533203  0.6665039   0.04702759] 3   2 \n",
      "[ 0.20251465  0.8154297   0.36108398 -0.25024414 -0.81933594 -0.62402344] 1   2 \n",
      "[-0.2565918   0.70166016  0.17578125 -0.25683594 -0.3959961  -0.13793945] 1   3 \n",
      "[-1.7607422  -0.1373291  -0.48632812  0.5625      1.2246094   1.2021484 ] 4   5 \n",
      "[-0.7529297   0.6069336   0.4885254   0.44360352 -0.04653931 -0.6660156 ] 1   2 \n",
      "[-1.7119141   0.03094482 -0.17150879  0.6791992   0.9873047   0.7636719 ] 4   0 \n",
      "[-1.671875   -0.3581543  -0.39819336  1.0878906   1.1298828   0.8491211 ] 4   4 Match 148\n",
      "\n",
      "[-1.0693359   0.32006836  0.48706055  1.0527344   0.14794922 -0.76464844] 3   1 \n",
      "[ 0.7163086   0.74365234  0.3684082  -0.36645508 -0.86865234 -0.84033203] 1   1 Match 149\n",
      "\n",
      "[ 0.8935547   0.7446289  -0.13269043 -0.70751953 -1.1630859  -0.5834961 ] 0   0 Match 150\n",
      "\n",
      "[-0.90625     0.24584961  0.5756836   1.0097656   0.08764648 -1.0253906 ] 3   3 Match 151\n",
      "\n",
      "[-1.8164062  -0.17663574 -0.42797852  0.6557617   1.15625     1.0058594 ] 4   4 Match 152\n",
      "\n",
      "[-0.7944336   0.42456055  0.4494629   0.58154297  0.1541748  -0.4711914 ] 3   0 \n",
      "[-1.7763672  -0.11798096 -0.35058594  0.69921875  1.0644531   0.93066406] 4   4 Match 153\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.30004883  0.84521484  0.04595947 -0.4013672  -0.7910156  -0.46679688] 1   3 \n",
      "[-1.3808594  -0.05239868  0.74560547  1.1005859   0.66845703 -0.5180664 ] 3   5 \n",
      "[-0.28100586  0.7060547   0.4946289   0.22668457 -0.3071289  -0.77001953] 1   1 Match 154\n",
      "\n",
      "[ 0.31445312  0.7788086   0.27661133 -0.08410645 -0.7290039  -0.7915039 ] 1   3 \n",
      "[-1.8691406  -0.2355957  -0.28320312  0.7817383   1.1972656   0.9824219 ] 4   5 \n",
      "[-1.1787109   0.2142334  -0.08850098  0.35961914  0.44482422  0.6958008 ] 5   3 \n",
      "[-0.78808594  0.7607422   0.47021484  0.4453125  -0.17297363 -0.85302734] 1   1 Match 155\n",
      "\n",
      "[-1.6865234  -0.10345459 -0.21533203  0.63378906  1.1484375   0.8745117 ] 4   4 Match 156\n",
      "\n",
      "[-1.7353516   0.02287292 -0.2166748   0.59521484  0.9511719   0.84277344] 4   3 \n",
      "[-1.5712891   0.05020142  0.21704102  0.94873047  0.6738281   0.11535645] 3   3 Match 157\n",
      "\n",
      "[-0.79052734  0.44018555  0.28320312  0.81152344 -0.12011719 -0.8808594 ] 3   3 Match 158\n",
      "\n",
      "[-1.7324219  -0.35742188 -0.46777344  0.79052734  1.2802734   1.0947266 ] 4   5 \n",
      "[ 0.26391602  0.7915039   0.55322266  0.06365967 -0.7553711  -1.2617188 ] 1   1 Match 159\n",
      "\n",
      "[-1.6611328   0.03509521  0.21826172  0.8774414   0.81347656  0.05889893] 3   1 \n",
      "[-1.7197266  -0.19165039 -0.22497559  0.56103516  0.9423828   0.89160156] 4   4 Match 160\n",
      "\n",
      "[-1.7480469  -0.16320801 -0.32202148  0.6118164   1.1865234   0.86279297] 4   5 \n",
      "[-0.5444336   0.82958984 -0.13000488  0.26733398 -0.42578125 -0.2932129 ] 1   3 \n",
      "[-1.3535156  -0.19677734  0.17834473  0.91552734  0.71777344  0.11700439] 3   5 \n",
      "[ 0.7866211   0.43920898 -0.03044128 -0.51904297 -0.89990234 -0.39111328] 0   1 \n",
      "[-1.6494141  -0.33544922 -0.5605469   0.82128906  1.2373047   0.99658203] 4   5 \n",
      "[-1.0957031   0.39648438  0.265625    0.8154297   0.19763184 -0.46533203] 3   1 \n",
      "[-0.8286133   0.6166992   0.26000977  0.31640625  0.02810669 -0.06506348] 1   3 \n",
      "[-1.2753906   0.28955078  0.6411133   0.7446289   0.18029785 -0.6035156 ] 3   5 \n",
      "[-1.7607422  -0.24035645 -0.29223633  0.75878906  1.1455078   0.8510742 ] 4   4 Match 161\n",
      "\n",
      "[-0.7993164   0.5073242   0.15600586  0.22912598  0.13378906  0.06616211] 1   1 Match 162\n",
      "\n",
      "[-1.2324219   0.4555664  -0.32080078  0.17370605  0.30639648  0.6010742 ] 5   5 Match 163\n",
      "\n",
      "[-1.2246094   0.6503906   0.1262207   0.5834961   0.27124023 -0.14147949] 1   2 \n",
      "[ 0.28759766  0.9243164   0.29785156 -0.29907227 -0.6425781  -0.6723633 ] 1   3 \n",
      "[-1.5107422  -0.07910156  0.11151123  1.0361328   0.6557617  -0.00410843] 3   3 Match 164\n",
      "\n",
      "[-1.3916016   0.18188477 -0.09399414  0.5966797   0.7446289   0.32739258] 4   5 \n",
      "[-1.6044922  -0.11169434 -0.1953125   0.69189453  0.96191406  0.69873047] 4   5 \n",
      "[ 0.50439453  0.50390625  0.16137695 -0.6152344  -0.66845703 -0.56396484] 0   2 \n",
      "[ 0.6821289   0.67578125  0.16003418 -0.53808594 -0.86279297 -0.53515625] 0   0 Match 165\n",
      "\n",
      "[-1.1035156   0.36987305 -0.15356445  0.12915039  0.43017578  0.3786621 ] 4   1 \n",
      "[ 0.38989258  0.7807617   0.38623047 -0.17797852 -0.7973633  -0.8017578 ] 1   1 Match 166\n",
      "\n",
      "[-1.7539062  -0.04873657  0.13757324  1.0566406   0.8417969   0.2590332 ] 3   3 Match 167\n",
      "\n",
      "[-1.6572266   0.46972656  0.02478027  0.6074219   0.8696289   0.48388672] 4   1 \n",
      "[-1.0087891   0.45874023 -0.03726196  0.4350586   0.27539062  0.21972656] 1   0 \n",
      "[-1.1611328   0.3564453  -0.2310791   0.3491211   0.4091797   0.47143555] 5   2 \n",
      "[-0.9160156   0.50878906 -0.07293701  0.00658035  0.12451172  0.6230469 ] 5   5 Match 168\n",
      "\n",
      "[-0.82910156  0.41088867  0.34887695  0.42822266  0.04443359 -0.6142578 ] 3   3 Match 169\n",
      "\n",
      "[-1.8291016  -0.32739258 -0.3564453   0.9482422   1.2109375   0.9394531 ] 4   5 \n",
      "[-1.7763672  -0.16809082 -0.32641602  0.66845703  1.1884766   1.0732422 ] 4   3 \n",
      "[-1.0810547   0.30004883  0.15478516  0.50390625  0.02908325  0.21179199] 3   2 \n",
      "[-1.3857422   0.11553955 -0.13208008  0.5439453   0.59716797  0.40014648] 4   1 \n",
      "[-1.7871094  -0.07281494 -0.42016602  0.45288086  1.1083984   1.0244141 ] 4   3 \n",
      "[-0.4267578   0.5761719   0.3256836   0.13537598 -0.14355469 -0.37280273] 1   0 \n",
      "[-1.3642578   0.3671875   0.29077148  0.82470703  0.5810547  -0.24267578] 3   5 \n",
      "[-1.84375    -0.20678711 -0.3713379   0.62597656  1.2207031   1.        ] 4   4 Match 170\n",
      "\n",
      "[-0.6245117   0.23828125  0.5209961   0.28222656  0.03256226 -0.25219727] 2   2 Match 171\n",
      "\n",
      "[-0.2734375   0.4958496   0.31079102 -0.04907227 -0.39916992 -0.3544922 ] 1   1 Match 172\n",
      "\n",
      "[-0.5288086   0.47631836  0.2163086   0.11340332 -0.06707764 -0.05853271] 1   3 \n",
      "[-0.9003906   0.15466309  0.7192383   0.88720703  0.06884766 -0.91259766] 3   1 \n",
      "[ 0.6201172   0.85302734  0.14086914 -0.5029297  -1.0107422  -0.68359375] 1   1 Match 173\n",
      "\n",
      "[ 0.73583984  0.5859375  -0.04040527 -0.45458984 -0.86376953 -0.47802734] 0   2 \n",
      "[-1.0371094   0.3798828   0.27416992  0.96875     0.12194824 -0.5961914 ] 3   3 Match 174\n",
      "\n",
      "[-0.5996094   0.4248047   0.7158203   0.47314453 -0.01835632 -0.81884766] 2   3 \n",
      "[-0.38134766  0.47802734  0.44506836  0.5058594  -0.32958984 -0.80029297] 3   2 \n",
      "[-1.7734375  -0.0446167  -0.34960938  0.85595703  0.91796875  0.70947266] 4   2 \n",
      "[-0.48266602  0.7441406   0.2878418  -0.17785645 -0.27319336 -0.1940918 ] 1   1 Match 175\n",
      "\n",
      "[-0.26611328  0.96728516 -0.33642578 -0.17492676 -0.87353516 -0.33935547] 1   5 \n",
      "[-1.734375   -0.28808594 -0.35986328  0.8647461   1.2177734   1.0498047 ] 4   5 \n",
      "[-0.07958984  0.68652344  0.38916016  0.03065491 -0.5048828  -0.6743164 ] 1   1 Match 176\n",
      "\n",
      "[-1.7597656  -0.11871338 -0.03851318  0.6699219   0.9868164   0.6533203 ] 4   1 \n",
      "[-0.43139648  0.33544922  0.3840332   0.13879395 -0.15478516 -0.23632812] 2   0 \n",
      "[-1.7597656  -0.12792969 -0.22045898  0.8154297   1.1064453   0.875     ] 4   1 \n",
      "[-1.7236328  -0.3190918  -0.5004883   0.59521484  1.2568359   1.0234375 ] 4   3 \n",
      "[-1.0771484   0.15942383  0.59033203  1.0371094   0.35327148 -0.60253906] 3   4 \n",
      "[-1.0488281   0.32958984  0.36376953  0.57714844  0.3005371  -0.24182129] 3   1 \n",
      "[-0.24182129  0.5019531   0.42700195  0.6484375  -0.40625    -0.89746094] 3   3 Match 177\n",
      "\n",
      "[-0.75878906  0.22802734  0.35498047  0.37060547  0.36206055 -0.24353027] 3   5 \n",
      "[-1.4238281  -0.21020508  0.15393066  1.0253906   0.56396484  0.09448242] 3   3 Match 178\n",
      "\n",
      "[-1.5898438  -0.26513672 -0.45141602  0.60009766  1.1318359   0.84765625] 4   3 \n",
      "[ 0.47485352  0.51708984  0.3227539  -0.21154785 -0.79003906 -0.75927734] 1   0 \n",
      "[-0.65625     0.60791016 -0.02026367  0.02342224 -0.04727173  0.22570801] 1   1 Match 179\n",
      "\n",
      "[-1.8105469  -0.3256836  -0.31176758  0.7714844   1.2773438   1.0449219 ] 4   2 \n",
      "[-1.7255859  -0.25878906 -0.26342773  0.8696289   1.0214844   0.5522461 ] 4   2 \n",
      "[-0.47045898  0.69433594 -0.04541016 -0.09185791 -0.2253418   0.10137939] 1   4 \n",
      "[ 0.34814453  0.59375     0.38793945 -0.11138916 -0.69970703 -0.8183594 ] 1   3 \n",
      "[-1.7666016  -0.3564453  -0.27807617  0.85595703  1.1201172   0.9223633 ] 4   4 Match 180\n",
      "\n",
      "[ 0.89453125  0.6767578   0.00257111 -0.66064453 -1.0751953  -0.6503906 ] 0   2 \n",
      "[ 0.12719727  0.43676758  0.57373047  0.01329041 -0.5083008  -0.7861328 ] 2   2 Match 181\n",
      "\n",
      "[-1.6494141  -0.07489014 -0.00273514  0.9794922   0.7089844   0.33203125] 3   1 \n",
      "[-0.7758789   0.76220703  0.47094727  0.35351562  0.04190063 -0.34887695] 1   0 \n",
      "[-1.3955078   0.5102539   0.40625     0.64208984  0.39916992 -0.20361328] 3   1 \n",
      "[-1.3710938   0.48291016 -0.15283203  0.34204102  0.6660156   0.4243164 ] 4   1 \n",
      "[-0.7807617   0.44873047  0.27734375  0.7944336  -0.1932373  -0.89941406] 3   2 \n",
      "[-1.1611328   0.30493164 -0.13085938  0.32421875  0.36987305  0.5175781 ] 5   5 Match 182\n",
      "\n",
      "[-1.6923828  -0.06628418 -0.2397461   0.56347656  0.9736328   0.8798828 ] 4   3 \n",
      "[-1.7958984  -0.25024414 -0.38964844  0.7783203   1.2568359   1.2109375 ] 4   2 \n",
      "[-1.5654297  -0.4296875  -0.16296387  1.1757812   0.87353516  0.4736328 ] 3   2 \n",
      "[ 0.7441406   0.71875     0.20922852 -0.52197266 -1.0273438  -0.66015625] 0   0 Match 183\n",
      "\n",
      "[-1.6103516   0.15808105 -0.21655273  0.5410156   0.88916016  0.4267578 ] 4   3 \n",
      "[-0.06262207  0.6816406   0.41674805 -0.05422974 -0.42529297 -0.61816406] 1   2 \n",
      "[-0.88671875  0.54589844  0.32421875  0.6171875  -0.10455322 -0.54785156] 3   3 Match 184\n",
      "\n",
      "[-0.26538086  0.69970703  0.5253906   0.22644043 -0.34960938 -0.8305664 ] 1   0 \n",
      "[-1.1191406   0.34228516  0.41259766  0.40844727  0.2548828  -0.43725586] 2   1 \n",
      "[-1.4267578   0.43798828  0.45898438  0.8984375   0.5253906  -0.28979492] 3   4 \n",
      "[-1.6523438  -0.19433594 -0.25854492  0.7651367   0.97753906  0.8564453 ] 4   3 \n",
      "[-1.890625   -0.36767578 -0.11462402  0.9526367   1.0976562   0.85595703] 4   1 \n",
      "[-0.49145508  0.5522461   0.8144531   0.60253906 -0.45922852 -1.3203125 ] 2   2 Match 185\n",
      "\n",
      "[-1.6855469   0.12756348 -0.30493164  0.6875      0.88916016  0.8383789 ] 4   5 \n",
      "[-0.49975586  0.6245117   0.6430664   0.63183594 -0.29370117 -1.0566406 ] 2   2 Match 186\n",
      "\n",
      "[-1.2958984   0.32226562 -0.07012939  0.25976562  0.2932129   0.52001953] 5   4 \n",
      "[-1.2880859   0.30566406  0.43041992  0.9223633   0.43603516 -0.3166504 ] 3   3 Match 187\n",
      "\n",
      "[-0.71533203  0.41064453 -0.02120972  0.12597656  0.0637207   0.22668457] 1   3 \n",
      "[-1.6523438  -0.10839844  0.18725586  0.8491211   0.97802734  0.40356445] 4   1 \n",
      "[-1.0244141   0.34301758  0.20373535  0.45288086  0.25927734  0.30273438] 3   3 Match 188\n",
      "\n",
      "[ 0.22216797  0.53808594  0.44335938 -0.01901245 -0.50634766 -0.82666016] 1   3 \n",
      "[-1.2929688   0.37109375  0.043396    0.33984375  0.61621094  0.4440918 ] 4   4 Match 189\n",
      "\n",
      "[ 0.05328369  0.6503906   0.42285156  0.16918945 -0.55371094 -0.85498047] 1   1 Match 190\n",
      "\n",
      "[-1.5146484   0.31079102  0.44482422  0.9423828   0.41333008 -0.31713867] 3   5 \n",
      "[-1.3496094   0.14685059  0.15649414  0.62158203  0.6386719   0.25390625] 4   3 \n",
      "[-0.18481445  0.35888672  0.43188477  0.47607422 -0.23291016 -0.6723633 ] 3   1 \n",
      "[-0.9448242   0.37402344  0.8730469   0.7285156   0.16845703 -0.7080078 ] 2   4 \n",
      "[-1.5214844  -0.11590576  0.04284668  0.64990234  0.8671875   0.65527344] 4   5 \n",
      "[-1.7167969  -0.08392334 -0.07995605  0.75634766  0.8984375   0.7446289 ] 4   5 \n",
      "[-1.5253906e+00 -9.9658966e-04  4.1137695e-01  1.0839844e+00\n",
      "  5.5908203e-01 -5.0445557e-02] 3   3 Match 191\n",
      "\n",
      "[-1.7324219  -0.18017578 -0.35668945  0.76660156  1.1201172   0.87646484] 4   3 \n",
      "[-0.10302734  0.44262695  0.5107422   0.35424805 -0.43725586 -0.80908203] 2   3 \n",
      "[-1.6523438  -0.2836914   0.44262695  1.1015625   1.0966797   0.06445312] 3   2 \n",
      "[-1.8173828  -0.15209961 -0.39794922  0.69091797  1.1943359   0.9946289 ] 4   4 Match 192\n",
      "\n",
      "[-0.79589844  0.3701172   0.02012634  0.1628418  -0.01482391  0.15917969] 1   5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.62646484  0.36401367  0.46020508  0.38867188 -0.01353455 -0.234375  ] 2   1 \n",
      "[-1.2871094   0.36157227 -0.10150146  0.27905273  0.37573242  0.7158203 ] 5   2 \n",
      "[-1.1494141   0.44067383 -0.04931641  0.28173828  0.17346191  0.51123047] 5   4 \n",
      "[-0.45214844  0.4650879  -0.03417969 -0.1640625  -0.17419434  0.09136963] 1   3 \n",
      "[-1.8398438  -0.04000854 -0.35766602  0.77685547  1.1630859   1.1572266 ] 4   5 \n",
      "[-1.3046875   0.54052734 -0.39453125  0.24108887  0.44189453  0.7348633 ] 5   0 \n",
      "[-0.33081055  0.56689453  0.5698242   0.14611816 -0.3857422  -0.7939453 ] 2   0 \n",
      "[-1.4609375   0.08227539  0.32739258  1.1572266   0.5727539  -0.2626953 ] 3   1 \n",
      "[-0.26342773  0.45141602  0.38793945  0.4494629  -0.38330078 -0.6616211 ] 1   1 Match 193\n",
      "\n",
      "[-0.5473633   0.6040039   0.41503906  0.28442383 -0.19824219 -0.44067383] 1   4 \n",
      "[-1.7089844  -0.03448486 -0.17370605  0.54003906  1.0214844   0.6953125 ] 4   5 \n",
      "[-1.4335938   0.37451172 -0.04983521  0.36010742  0.6411133   0.6074219 ] 4   0 \n",
      "[-0.32299805  0.48901367  0.22680664 -0.05532837 -0.2890625  -0.17602539] 1   4 \n",
      "[-0.7373047   0.6201172   0.36279297  0.4375     -0.07574463 -0.40161133] 1   2 \n",
      "[-0.5102539   0.03042603  0.6171875   0.61621094 -0.19995117 -0.74072266] 2   4 \n",
      "[-0.66845703  0.4609375   0.3154297   0.8100586  -0.04858398 -1.0029297 ] 3   2 \n",
      "[-0.33496094  0.58984375  0.6923828   0.29418945 -0.26757812 -0.80908203] 2   2 Match 194\n",
      "\n",
      "[-1.3212891   0.28710938  0.26708984  0.8173828   0.33398438 -0.3244629 ] 3   2 \n",
      "[-1.5449219   0.21154785  0.3347168   1.1894531   0.51171875 -0.23632812] 3   0 \n",
      "[-0.8046875   0.546875    0.6738281   0.8330078  -0.11181641 -0.84765625] 3   2 \n",
      "[-0.6118164   0.73339844 -0.22814941 -0.02516174 -0.10888672  0.19873047] 1   2 \n",
      "[-0.95947266  0.1574707   0.5131836   0.99853516  0.02398682 -0.8144531 ] 3   1 \n",
      "[-1.7080078  -0.10913086 -0.2932129   0.7729492   1.0693359   0.84814453] 4   4 Match 195\n",
      "\n",
      "[-1.9226074e-01  5.4345703e-01  4.7045898e-01  2.3531914e-04\n",
      " -3.7158203e-01 -5.8496094e-01] 1   2 \n",
      "[-1.6552734  -0.24108887  0.24987793  0.91552734  1.1074219   0.4362793 ] 4   3 \n",
      "[-1.6152344   0.30029297  0.01477814  0.84472656  0.50683594 -0.05548096] 3   3 Match 196\n",
      "\n",
      "[-1.6630859  -0.38256836 -0.03512573  0.89208984  1.1474609   0.5908203 ] 4   4 Match 197\n",
      "\n",
      "[-1.4492188   0.1607666   0.12866211  0.8364258   0.52246094  0.13720703] 3   2 \n",
      "[-1.6230469  -0.3095703   0.26367188  1.1044922   1.0703125   0.26489258] 3   2 \n",
      "[-1.8085938  -0.08306885 -0.4086914   0.6533203   1.140625    0.94091797] 4   3 \n",
      "[ 0.13952637  0.6923828   0.41210938  0.11883545 -0.61035156 -0.94628906] 1   2 \n",
      "[-1.5058594   0.18798828 -0.12310791  0.4807129   0.7519531   0.63964844] 4   4 Match 198\n",
      "\n",
      "[-0.5620117   0.45751953  0.57910156  0.44262695 -0.21936035 -0.80615234] 2   3 \n",
      "[ 0.40039062  0.54785156  0.4802246  -0.17224121 -0.6074219  -0.7397461 ] 1   1 Match 199\n",
      "\n",
      "[-1.6806641  -0.13781738 -0.19689941  0.82128906  1.0947266   0.6948242 ] 4   5 \n",
      "[-1.6396484   0.21923828 -0.23693848  0.5410156   0.78808594  0.7285156 ] 4   3 \n",
      "[-1.6816406  -0.12060547 -0.17700195  0.90185547  0.78515625  0.47753906] 3   4 \n",
      "[-1.5146484  -0.07983398  0.22729492  0.9707031   0.73339844  0.10064697] 3   5 \n",
      "[-0.0275116   0.7841797   0.01721191 -0.3955078  -0.546875   -0.21166992] 1   2 \n",
      "[-1.5419922   0.08477783  0.11633301  0.8618164   0.82958984  0.40820312] 3   3 Match 200\n",
      "\n",
      "[-1.4306641   0.14685059  0.1104126   0.82421875  0.55371094  0.02848816] 3   5 \n",
      "[-1.1699219   0.09594727  0.46533203  0.9526367   0.22033691 -0.53125   ] 3   2 \n",
      "[-1.3476562   0.37280273 -0.08813477  0.48999023  0.5678711   0.22058105] 4   5 \n",
      "[-1.5371094  -0.06185913 -0.10650635  0.9033203   0.88183594  0.47558594] 3   4 \n",
      "[-1.3134766   0.41064453  0.01741028  0.35986328  0.65478516  0.31030273] 4   2 \n",
      "[-1.5361328   0.29467773  0.07507324  0.6557617   0.93408203  0.29541016] 4   3 \n",
      "[ 0.5527344   0.58691406  0.5317383  -0.10791016 -0.7832031  -1.1328125 ] 1   0 \n",
      "[-1.3232422   0.3017578   0.15759277  0.65185547  0.5283203   0.00708389] 3   2 \n",
      "[-1.8515625  -0.12841797 -0.43188477  0.6176758   1.2011719   1.0839844 ] 4   5 \n",
      "[-1.7626953   0.4416504  -0.43115234  0.33740234  0.8779297   1.0380859 ] 5   5 Match 201\n",
      "\n",
      "[-1.4882812   0.17578125 -0.01223755  0.7661133   0.7788086   0.25048828] 4   3 \n",
      "[-1.3212891   0.2800293  -0.03759766  0.47558594  0.57470703  0.41381836] 4   2 \n",
      "[-1.0488281   0.5522461   0.1895752   0.47143555  0.08276367 -0.05044556] 1   4 \n",
      "[-1.4589844   0.43188477  0.20227051  0.67089844  0.5654297   0.08197021] 3   1 \n",
      "[-1.7958984  -0.26000977 -0.4729004   0.61376953  1.2060547   1.0273438 ] 4   4 Match 202\n",
      "\n",
      "[-1.7695312   0.11035156 -0.18945312  0.70654297  0.89941406  0.6958008 ] 4   3 \n",
      "[-1.6181641  -0.3605957  -0.20324707  0.9013672   1.0966797   0.73095703] 4   3 \n",
      "[-1.8349609   0.02813721 -0.25952148  0.58740234  1.09375     0.96435547] 4   1 \n",
      "[-1.5644531  -0.15649414 -0.42700195  0.5073242   1.0830078   0.82470703] 4   4 Match 203\n",
      "\n",
      "[-1.6123047  -0.3408203   0.03479004  0.9760742   1.0830078   0.39648438] 4   5 \n",
      "[-1.2080078   0.72265625 -0.01985168  0.17553711  0.13232422  0.08074951] 1   2 \n",
      "[-0.9736328   0.36669922  0.7402344   0.84472656  0.0456543  -0.7636719 ] 3   0 \n",
      "[-0.59033203  0.5019531   0.37402344  0.14794922 -0.0072937  -0.12963867] 1   1 Match 204\n",
      "\n",
      "[-1.8212891  -0.34936523 -0.17529297  0.88964844  1.1035156   0.80029297] 4   3 \n",
      "[-1.7529297  -0.02973938 -0.3930664   0.58251953  1.2138672   1.0351562 ] 4   3 \n",
      "[-1.4746094   0.3774414  -0.04217529  0.5595703   0.6542969   0.2932129 ] 4   2 \n",
      "[-1.5605469   0.01154327 -0.13391113  0.54589844  0.7026367   0.59521484] 4   5 \n",
      "[-1.6630859   0.3305664  -0.10626221  0.62646484  0.8852539   0.6274414 ] 4   2 \n",
      "[-1.7314453  -0.18701172 -0.35888672  0.4868164   1.2890625   0.89501953] 4   5 \n",
      "[ 0.1665039   0.8378906   0.36376953 -0.1060791  -0.70654297 -0.6689453 ] 1   3 \n",
      "[-1.7294922   0.07788086 -0.22192383  0.5517578   1.140625    0.86376953] 4   4 Match 205\n",
      "\n",
      "[-1.6904297   0.27783203 -0.18237305  0.33911133  0.78125     0.91064453] 5   2 \n",
      "[-1.8125     -0.19921875 -0.25        0.8535156   1.1259766   0.92578125] 4   4 Match 206\n",
      "\n",
      "[-0.9707031   0.7597656  -0.20080566 -0.04425049 -0.04174805  0.53564453] 1   2 \n",
      "[-1.6992188   0.20544434 -0.01459503  0.70996094  0.6689453   0.6035156 ] 3   4 \n",
      "[-1.5654297   0.4128418   0.10943604  0.5473633   0.78125     0.20117188] 4   4 Match 207\n",
      "\n",
      "[-1.7832031  -0.13134766 -0.3642578   0.6430664   1.1894531   1.1621094 ] 4   0 \n",
      "[-0.18017578  0.5229492   0.31323242  0.11315918 -0.22509766 -0.49267578] 1   2 \n",
      "[-1.1748047   0.38891602 -0.20031738  0.26953125  0.45117188  0.5463867 ] 5   5 Match 208\n",
      "\n",
      "[-1.7744141  -0.14416504 -0.3684082   0.72021484  1.2324219   1.0791016 ] 4   5 \n",
      "[ 0.5908203   0.67089844  0.36865234 -0.47729492 -0.82177734 -0.76464844] 1   2 \n",
      "[ 0.09436035  0.89501953  0.5576172   0.0206604  -0.7919922  -0.9790039 ] 1   3 \n",
      "[ 0.0736084   0.80126953  0.23132324 -0.21289062 -0.5205078  -0.45898438] 1   1 Match 209\n",
      "\n",
      "[-1.6962891  -0.2890625   0.04266357  1.0244141   0.89501953  0.3737793 ] 3   3 Match 210\n",
      "\n",
      "[-1.7167969  -0.12536621 -0.12249756  0.94140625  0.9760742   0.5854492 ] 4   3 \n",
      "[-1.3945312  -0.07556152  0.31274414  0.9448242   0.72558594  0.02233887] 3   0 \n",
      "[-1.4384766  -0.02552795  0.28173828  0.76904297  0.6098633   0.13952637] 3   2 \n",
      "[-1.4462891   0.11639404  0.32788086  0.90185547  0.59033203 -0.22485352] 3   4 \n",
      "[-1.5712891  -0.2133789   0.07312012  0.80322266  0.8779297   0.48535156] 4   4 Match 211\n",
      "\n",
      "[ 0.23632812  0.83251953  0.3786621  -0.01615906 -0.5517578  -0.73339844] 1   4 \n",
      "[-1.7587891  -0.07177734 -0.25219727  0.54296875  1.1552734   0.93310547] 4   4 Match 212\n",
      "\n",
      "[-1.8291016  -0.02883911 -0.09393311  0.7661133   1.0068359   0.74902344] 4   3 \n",
      "[-1.2910156   0.31445312  0.47924805  0.7783203   0.4091797  -0.3076172 ] 3   0 \n",
      "[-1.6542969  -0.0173645  -0.20202637  0.671875    1.078125    0.7363281 ] 4   5 \n",
      "[-1.3554688   0.04959106  0.26708984  0.72558594  0.64941406  0.11914062] 3   2 \n",
      "[-0.47436523  0.2692871   0.19006348  0.37182617  0.0680542  -0.20092773] 3   2 \n",
      "[-0.10357666  0.66064453  0.39794922  0.09991455 -0.38598633 -0.55371094] 1   4 \n",
      "[-1.7714844  -0.18322754 -0.42358398  0.68310547  1.1357422   1.0507812 ] 4   1 \n",
      "[-1.6503906   0.15185547 -0.13891602  0.5209961   0.98339844  0.57470703] 4   4 Match 213\n",
      "\n",
      "[ 0.2644043   0.5498047   0.22058105 -0.14624023 -0.5131836  -0.59277344] 1   3 \n",
      "[-1.3642578   0.15600586 -0.05526733  0.5019531   0.59814453  0.01277924] 4   4 Match 214\n",
      "\n",
      "[-0.48120117  1.1230469  -0.33374023 -0.24157715 -0.5053711   0.06445312] 1   5 \n",
      "[-1.7021484   0.01313782  0.00671005  0.7026367   0.94384766  0.43920898] 4   2 \n",
      "[-1.8369141   0.0136261  -0.25634766  0.68408203  1.0673828   0.96191406] 4   5 \n",
      "[-1.8095703  -0.12091064 -0.18688965  0.8051758   1.0683594   0.8413086 ] 4   3 \n",
      "[-1.6064453   0.05822754 -0.0534668   0.80371094  0.8286133   0.4790039 ] 4   3 \n",
      "[-0.75439453  0.5444336   0.08685303  0.07354736  0.02062988  0.14562988] 1   1 Match 215\n",
      "\n",
      "[-1.9052734   0.04602051 -0.35327148  0.58984375  1.0800781   0.9794922 ] 4   4 Match 216\n",
      "\n",
      "[-0.7421875   0.26293945  0.50390625  0.5703125  -0.0092392  -0.78466797] 3   2 \n",
      "[-0.9873047   0.88183594  0.17871094  0.31054688 -0.03793335 -0.2565918 ] 1   1 Match 217\n",
      "\n",
      "[-0.20117188  0.88378906  0.06787109 -0.29516602 -0.34277344 -0.02861023] 1   4 \n",
      "[-1.1044922   0.5527344   0.5161133   0.6767578   0.3022461  -0.53759766] 3   4 \n",
      "[-1.4902344   0.24243164 -0.10375977  0.36865234  0.76660156  0.5776367 ] 4   3 \n",
      "[-1.7705078  -0.37353516 -0.50878906  0.6875      1.1855469   1.0048828 ] 4   3 \n",
      "[-1.1738281   0.10205078  0.15698242  0.5751953   0.33447266  0.35839844] 3   2 \n",
      "[ 0.84765625  0.69628906  0.03863525 -0.69384766 -1.0361328  -0.6459961 ] 0   0 Match 218\n",
      "\n",
      "[-0.4580078   0.6616211   0.64941406  0.80078125 -0.59765625 -1.1054688 ] 3   1 \n",
      "[-1.7138672 -0.1529541 -0.3876953  0.6269531  1.1621094  1.0517578] 4   4 Match 219\n",
      "\n",
      "[-1.6015625  -0.2277832   0.11804199  1.1660156   0.81103516  0.25634766] 3   4 \n",
      "[-1.7597656   0.00429916 -0.28881836  0.6118164   1.1884766   0.9189453 ] 4   4 Match 220\n",
      "\n",
      "[-1.671875   -0.19018555 -0.35131836  0.6171875   1.2431641   0.96875   ] 4   5 \n",
      "[-0.7553711   0.2241211   0.25854492  0.7451172   0.12658691 -0.46191406] 3   3 Match 221\n",
      "\n",
      "[-1.609375    0.31176758  0.15795898  0.6191406   0.67578125  0.21252441] 4   2 \n",
      "[-0.74658203  0.7114258  -0.140625   -0.01282501 -0.01687622  0.25683594] 1   5 \n",
      "[-0.13586426  0.7675781   0.1381836  -0.17626953 -0.29492188 -0.18640137] 1   0 \n",
      "[-0.47070312  0.28125     0.3762207   0.07055664 -0.28076172 -0.18713379] 2   3 \n",
      "[-0.06848145  0.7441406   0.18188477 -0.16479492 -0.36010742 -0.39086914] 1   0 \n",
      "[-1.84375    -0.17456055 -0.2861328   0.67626953  1.1386719   0.9003906 ] 4   4 Match 222\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1162109   0.40551758  0.1307373   0.39697266  0.40014648  0.078125  ] 1   4 \n",
      "[-1.2832031   0.39819336 -0.3251953   0.2409668   0.37182617  0.8808594 ] 5   4 \n",
      "[-0.22070312  0.5498047   0.40820312  0.31933594 -0.4177246  -0.9121094 ] 1   3 \n",
      "[-1.4863281   0.14526367  0.45751953  1.0615234   0.45263672 -0.23913574] 3   2 \n",
      "[-0.21594238  0.29760742  0.2619629   0.21350098 -0.0406189  -0.11126709] 1   5 \n",
      "[-1.4482422   0.08325195 -0.10723877  0.609375    0.8461914   0.45410156] 4   5 \n",
      "[-0.5961914   0.6386719  -0.16296387  0.01985168 -0.04605103  0.25439453] 1   1 Match 223\n",
      "\n",
      "[-1.4101562   0.32006836  0.04788208  0.63720703  0.61376953  0.18334961] 3   1 \n",
      "[-1.6103516   0.31176758 -0.20178223  0.30859375  0.7631836   0.7368164 ] 4   3 \n",
      "[-1.7724609  -0.03881836 -0.36767578  0.53808594  1.1669922   0.91259766] 4   2 \n",
      "[-1.8486328  -0.09692383 -0.49853516  0.53808594  1.2070312   1.0849609 ] 4   5 \n",
      "[-0.5654297   0.55126953  0.25146484  0.42651367 -0.03311157 -0.4885254 ] 1   2 \n",
      "[-0.6220703   0.47875977 -0.04528809  0.07189941  0.00881958  0.11077881] 1   1 Match 224\n",
      "\n",
      "[-0.8251953   1.1015625  -0.72265625  0.05020142 -0.29589844 -0.0847168 ] 1   3 \n",
      "[-1.7783203  -0.4831543  -0.45166016  0.6020508   1.234375    0.86279297] 4   3 \n",
      "[-0.94921875  0.49243164 -0.10418701  0.07507324  0.15795898  0.43920898] 1   1 Match 225\n",
      "\n",
      "[-1.2900391   0.09283447  0.2644043   1.0292969   0.5180664  -0.23339844] 3   3 Match 226\n",
      "\n",
      "[-1.6689453  -0.34887695 -0.4020996   0.7685547   1.1181641   0.82910156] 4   4 Match 227\n",
      "\n",
      "[-0.6977539   0.50634766  0.6147461   0.57470703 -0.15246582 -0.82177734] 2   5 \n",
      "[-1.8115234  -0.13720703 -0.30249023  0.7084961   1.0859375   1.0605469 ] 4   4 Match 228\n",
      "\n",
      "[-1.7617188  -0.28686523 -0.37231445  0.68408203  1.2177734   0.8544922 ] 4   1 \n",
      "[-1.7050781 -0.1385498 -0.2097168  0.7207031  1.0771484  0.7138672] 4   3 \n",
      "[-0.9394531   0.6611328  -0.10473633  0.06610107  0.19763184  0.2993164 ] 1   4 \n",
      "[ 0.8178711   0.7363281   0.18652344 -0.390625   -0.9941406  -0.98291016] 0   0 Match 229\n",
      "\n",
      "[-1.6191406   0.54296875 -0.30493164  0.41967773  0.74560547  0.8876953 ] 5   2 \n",
      "[-1.7949219   0.3112793  -0.3581543   0.44555664  1.0029297   0.92285156] 4   1 \n",
      "[-0.72998047  0.68896484 -0.2220459  -0.01722717 -0.15441895  0.48388672] 1   5 \n",
      "[-1.4746094   0.00746918  0.32250977  1.125       0.58447266 -0.03311157] 3   2 \n",
      "[-1.7910156  -0.29956055 -0.45532227  0.7402344   1.1835938   0.93066406] 4   4 Match 230\n",
      "\n",
      "[-1.6552734  -0.11022949  0.22216797  1.0859375   0.8696289   0.12512207] 3   3 Match 231\n",
      "\n",
      "[-1.1699219   0.23950195  0.3137207   0.6845703   0.48364258 -0.15771484] 3   4 \n",
      "[-1.4482422  -0.19519043 -0.03826904  1.0449219   0.69921875  0.3227539 ] 3   5 \n",
      "[-0.47558594  0.3449707   0.74365234  0.48901367  0.00146389 -0.82714844] 2   1 \n",
      "[ 0.66503906  0.67333984 -0.19360352 -0.42871094 -1.1435547  -0.5395508 ] 1   0 \n",
      "[-0.70751953  0.41015625  0.04443359  0.14050293  0.09222412  0.27612305] 1   2 \n",
      "[-0.12768555  0.32421875  0.34692383 -0.03442383 -0.43969727 -0.57177734] 2   2 Match 232\n",
      "\n",
      "[-1.2910156   0.19384766  0.28833008  0.72753906  0.4116211  -0.16894531] 3   1 \n",
      "[-0.6196289   0.60595703  0.41748047  0.46533203 -0.23400879 -0.72021484] 1   1 Match 233\n",
      "\n",
      "[-1.7480469  -0.14111328 -0.09539795  0.8251953   1.0625      0.6401367 ] 4   4 Match 234\n",
      "\n",
      "[-1.1396484  -0.07684326  0.5390625   0.72558594  0.5517578  -0.1619873 ] 3   2 \n",
      "[-1.5068359   0.44604492 -0.46948242  0.35424805  0.8041992   0.81396484] 5   1 \n",
      "[-0.9238281   0.6484375   0.33447266  0.57666016  0.15661621 -0.6142578 ] 1   5 \n",
      "[-1.7001953   0.01179504 -0.11846924  0.625       1.0537109   0.8095703 ] 4   4 Match 235\n",
      "\n",
      "[-1.4628906  -0.08709717 -0.15100098  0.58984375  0.6689453   0.8696289 ] 5   5 Match 236\n",
      "\n",
      "[-1.5712891  -0.00524902  0.06976318  0.8613281   0.76708984  0.2154541 ] 3   3 Match 237\n",
      "\n",
      "[-0.875       0.1538086   0.4404297   0.6694336   0.24316406 -0.34936523] 3   1 \n",
      "[-1.1884766   0.34936523 -0.10162354  0.22094727  0.49560547  0.5048828 ] 5   2 \n",
      "[-1.7236328  -0.0625     -0.22106934  0.82373047  0.95410156  0.81640625] 4   2 \n",
      "[ 0.31640625  0.6821289   0.3972168  -0.16369629 -0.6176758  -0.5439453 ] 1   3 \n",
      "[-1.625       0.19897461  0.02244568  0.48461914  0.8486328   0.63134766] 4   5 \n",
      "[-1.6699219   0.23217773 -0.37402344  0.515625    1.0498047   0.89501953] 4   1 \n",
      "[-0.23217773 -0.04397583  0.06027222  0.63378906 -0.20361328 -0.2915039 ] 3   3 Match 238\n",
      "\n",
      "[ 0.2565918   0.5083008   0.5395508   0.04187012 -0.66845703 -0.7866211 ] 2   3 \n",
      "[-1.3583984   0.32714844 -0.19274902  0.35253906  0.7060547   0.5336914 ] 4   5 \n",
      "[-0.5371094   0.29174805  0.26293945  0.18432617  0.00457001 -0.19140625] 1   2 \n",
      "[ 0.16174316  0.5961914   0.45263672 -0.18847656 -0.46362305 -0.6557617 ] 1   1 Match 239\n",
      "\n",
      "[-1.4208984  -0.08117676  0.01208496  0.7578125   0.83203125  0.43530273] 4   3 \n",
      "[-0.83447266  1.0595703  -0.1640625   0.21447754 -0.13720703 -0.08416748] 1   3 \n",
      "[-1.1455078   0.43554688 -0.0826416   0.36035156  0.35742188  0.39233398] 1   5 \n",
      "[-0.45361328  0.7709961   0.02583313 -0.08551025 -0.19543457 -0.11383057] 1   5 \n",
      "[-1.421875    0.33129883  0.40234375  0.64160156  0.63134766  0.1685791 ] 3   2 \n",
      "[-1.7421875  -0.23779297 -0.14611816  0.86865234  1.1201172   0.8203125 ] 4   5 \n",
      "[-1.7714844  -0.33666992 -0.39575195  0.8051758   1.2861328   1.0673828 ] 4   5 \n",
      "[ 0.19042969  0.7050781  -0.10858154 -0.4543457  -0.7368164  -0.15649414] 1   3 \n",
      "[-0.16235352  0.11193848  0.28222656  0.15600586 -0.16015625 -0.24499512] 2   1 \n",
      "[-1.7919922 -0.0147934 -0.4663086  0.6616211  1.1943359  1.0615234] 4   4 Match 240\n",
      "\n",
      "[-1.5615234  -0.1217041  -0.05459595  0.84228516  0.9633789   0.35595703] 4   3 \n",
      "[-7.7685547e-01  4.8046875e-01  1.5075684e-01  1.8872070e-01\n",
      "  1.4672852e-01 -6.3419342e-05] 1   1 Match 241\n",
      "\n",
      "[-1.3798828  -0.14208984  0.21264648  1.1064453   0.5317383  -0.06158447] 3   4 \n",
      "[-1.3916016   0.40283203 -0.12512207  0.38256836  0.33569336  0.26904297] 1   1 Match 242\n",
      "\n",
      "[-1.8457031  -0.22766113 -0.421875    0.69140625  1.203125    1.1152344 ] 4   5 \n",
      "[-1.828125   -0.2076416  -0.51220703  0.78759766  1.2128906   1.09375   ] 4   3 \n",
      "[-1.7265625   0.03701782  0.05804443  0.6171875   0.90625     0.49316406] 4   4 Match 243\n",
      "\n",
      "[-1.6826172   0.15014648 -0.13781738  0.51464844  0.91259766  0.8569336 ] 4   3 \n",
      "[-1.65625     0.06030273  0.04190063  0.9038086   0.7495117   0.04766846] 3   1 \n",
      "[-1.1064453   0.08978271 -0.32543945  0.78759766  0.5341797   0.43603516] 3   3 Match 244\n",
      "\n",
      "[-0.9995117   0.22851562 -0.08544922  0.24487305  0.17333984  0.5463867 ] 5   4 \n",
      "[-1.6552734  -0.06799316 -0.01901245  0.8144531   1.078125    0.5810547 ] 4   4 Match 245\n",
      "\n",
      "[-0.9501953   0.1303711  -0.09820557  0.8696289   0.44921875 -0.11663818] 3   5 \n",
      "[-0.4086914   0.82128906  0.02560425 -0.17407227 -0.27978516 -0.06549072] 1   4 \n",
      "[-1.4267578  -0.19555664 -0.00151253  1.1337891   0.7338867   0.16821289] 3   4 \n",
      "[-0.25341797  0.45263672  0.4033203   0.20483398 -0.31958008 -0.6459961 ] 1   5 \n",
      "[ 0.5566406   0.69921875  0.20214844 -0.203125   -0.8457031  -0.66015625] 1   5 \n",
      "[-0.61816406  0.05645752  0.5307617   0.6694336   0.12091064 -0.5708008 ] 3   4 \n",
      "[-0.9819336  -0.24365234  0.27612305  0.7753906   0.2902832  -0.12536621] 3   5 \n",
      "[-1.5986328  -0.16503906 -0.35839844  0.82177734  0.9584961   0.67578125] 4   4 Match 246\n",
      "\n",
      "[-1.4082031   0.21801758  0.17810059  1.0576172   0.5390625  -0.25097656] 3   3 Match 247\n",
      "\n",
      "[ 0.35009766  0.578125    0.33618164 -0.08966064 -0.54785156 -0.63134766] 1   0 \n",
      "[-0.13891602  0.66748047  0.60791016  0.5253906  -0.6074219  -1.1943359 ] 1   2 \n",
      "[-1.4013672   0.36547852 -0.12249756  0.515625    0.7558594   0.34204102] 4   4 Match 248\n",
      "\n",
      "[-0.46435547  1.015625    0.13366699 -0.00267792 -0.33398438 -0.2980957 ] 1   4 \n",
      "[-1.7978516  -0.20715332 -0.44140625  0.67285156  1.2265625   1.0117188 ] 4   2 \n",
      "[-1.8837891  -0.1459961  -0.24304199  0.68359375  1.1757812   1.0087891 ] 4   3 \n",
      "[-1.7392578  -0.25878906 -0.23962402  0.8144531   1.2167969   0.9326172 ] 4   5 \n",
      "[-0.43945312  0.44555664  0.5234375   0.34228516 -0.17529297 -0.5463867 ] 2   4 \n",
      "[-1.625       0.01218414 -0.1829834   0.7060547   0.8510742   0.5810547 ] 4   4 Match 249\n",
      "\n",
      "[ 0.00987244  0.16992188  0.49169922  0.37353516 -0.28051758 -0.80078125] 2   0 \n",
      "[-1.0048828   0.11993408  0.3984375   0.54296875  0.22961426 -0.04284668] 3   4 \n",
      "[-0.96533203  0.51904297  0.09350586  0.07141113  0.21569824  0.26953125] 1   5 \n",
      "[-1.7001953  -0.17810059 -0.02078247  1.0722656   1.          0.50146484] 3   1 \n",
      "[-1.3154297   0.2310791   0.04550171  0.47998047  0.5361328   0.06002808] 4   2 \n",
      "[-0.36010742  0.7163086   0.50146484  0.45361328 -0.42089844 -1.2246094 ] 1   2 \n",
      "[-1.3408203  -0.44262695  0.00319481  0.7158203   0.6665039   0.42358398] 3   0 \n",
      "[-1.8046875  -0.15124512 -0.25561523  0.79833984  0.9921875   0.80126953] 4   4 Match 250\n",
      "\n",
      "[-0.56347656  0.66503906  0.40600586  0.4099121  -0.09509277 -0.8779297 ] 1   3 \n",
      "[-1.6738281   0.11254883 -0.17468262  0.53222656  0.88134766  0.81347656] 4   5 \n",
      "[-0.765625    0.50439453  0.5722656   0.67089844 -0.11462402 -0.8725586 ] 3   2 \n",
      "[-0.5571289   0.42285156 -0.15795898 -0.00391006 -0.21740723  0.12561035] 1   1 Match 251\n",
      "\n",
      "[ 0.04327393  0.60791016  0.25927734  0.01229095 -0.39672852 -0.47973633] 1   2 \n",
      "[-1.8144531  -0.3828125  -0.34570312  0.859375    1.2167969   0.9453125 ] 4   4 Match 252\n",
      "\n",
      "[-0.04425049  0.7729492   0.18554688 -0.28344727 -0.4777832  -0.30664062] 1   5 \n",
      "[-0.6738281   0.9067383   0.01535797  0.06048584 -0.39672852 -0.12597656] 1   5 \n",
      "[-1.7607422  -0.1427002  -0.25317383  0.6948242   1.1855469   0.99365234] 4   1 \n",
      "[-1.65625     0.25976562 -0.17602539  0.58203125  0.8334961   0.46118164] 4   3 \n",
      "[-1.0410156   0.37573242  0.17089844  0.4790039   0.56933594  0.04330444] 4   1 \n",
      "[-0.89404297  0.25170898  0.4477539   0.5444336   0.25439453 -0.2199707 ] 3   2 \n",
      "[-1.6953125   0.18078613  0.0317688   0.7133789   0.69628906  0.37329102] 3   3 Match 253\n",
      "\n",
      "[-1.7841797  -0.19506836 -0.25976562  0.85791016  1.1513672   0.8676758 ] 4   1 \n",
      "[ 0.5234375   0.6845703   0.41088867 -0.15234375 -0.8701172  -0.9321289 ] 1   0 \n",
      "[-0.6069336   0.7338867  -0.05993652 -0.06951904 -0.08551025  0.26171875] 1   5 \n",
      "[-1.8398438  -0.10296631 -0.26464844  0.5800781   1.1298828   0.82177734] 4   5 \n",
      "[-1.7099609  -0.03359985 -0.43408203  0.6166992   0.9946289   0.6923828 ] 4   1 \n",
      "[-0.8798828   0.57470703 -0.16711426 -0.01461029  0.14013672  0.5644531 ] 1   1 Match 254\n",
      "\n",
      "[-0.71728516  0.3395996   0.8222656   0.8886719  -0.12976074 -1.1005859 ] 3   0 \n",
      "[ 0.3425293   0.6347656   0.19177246 -0.38989258 -0.7236328  -0.50439453] 1   1 Match 255\n",
      "\n",
      "[-1.7441406  -0.08203125 -0.22338867  0.8588867   0.96435547  0.73876953] 4   3 \n",
      "[ 0.7109375   0.58251953  0.3010254  -0.3791504  -0.8486328  -0.64208984] 0   0 Match 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-0.7919922   0.30371094  0.4897461   0.7368164   0.10314941 -0.82421875] 3   4 \n",
      "[-1.5742188   0.08978271 -0.234375    0.39916992  0.7006836   0.8354492 ] 5   4 \n",
      "[-0.87402344  0.34204102  0.1977539   0.14196777  0.0094986   0.25854492] 1   5 \n",
      "[-0.22619629  0.72509766 -0.03201294 -0.10919189 -0.38232422  0.13916016] 1   3 \n",
      "[-0.43530273  0.5913086   0.43164062  0.26757812 -0.20117188 -0.6479492 ] 1   2 \n",
      "[ 0.890625    0.5957031   0.22167969 -0.33862305 -0.96777344 -0.8183594 ] 0   0 Match 257\n",
      "\n",
      "[-1.4736328   0.074646    0.24841309  0.7036133   0.57128906  0.38452148] 3   4 \n",
      "[-1.4316406  -0.02362061  0.25317383  1.015625    0.69970703 -0.14123535] 3   2 \n",
      "[-0.56591797  0.10516357  0.32958984  0.29589844 -0.02992249 -0.0526123 ] 2   2 Match 258\n",
      "\n",
      "[-1.3056641   0.50878906  0.2890625   0.58447266  0.44335938  0.18835449] 3   2 \n",
      "[-1.5498047  -0.36865234  0.26000977  1.0263672   1.0576172   0.18054199] 4   4 Match 259\n",
      "\n",
      "[-0.19189453  0.3317871   0.51464844  0.34033203 -0.33813477 -0.8256836 ] 2   5 \n",
      "[-1.7216797   0.10375977  0.09942627  0.75341797  0.7636719   0.40795898] 4   4 Match 260\n",
      "\n",
      "[ 0.12127686  0.70410156  0.5083008  -0.24633789 -0.5239258  -0.57666016] 1   2 \n",
      "[-0.9814453   0.24230957  0.5991211   0.79785156  0.17138672 -0.36083984] 3   4 \n",
      "[-1.5703125  -0.62109375 -0.27490234  1.3554688   0.9584961   0.42993164] 3   4 \n",
      "[-1.1347656   0.07263184  0.4897461   0.68408203  0.4831543  -0.19641113] 3   2 \n",
      "[ 0.265625    0.8125      0.00193119 -0.38305664 -0.61621094 -0.3334961 ] 1   3 \n",
      "[ 0.01753235  0.47216797  0.62646484  0.32592773 -0.48583984 -0.84033203] 2   2 Match 261\n",
      "\n",
      "[-1.6787109  -0.02391052 -0.03179932  0.61816406  0.89453125  0.86621094] 4   4 Match 262\n",
      "\n",
      "[-1.7724609  -0.11920166 -0.30786133  0.55615234  1.1386719   0.90527344] 4   3 \n",
      "[-1.328125    0.17053223 -0.35986328  0.39868164  0.7734375   0.6723633 ] 4   2 \n",
      "[-0.21362305  0.3869629   0.33862305  0.06494141 -0.24157715 -0.41796875] 1   1 Match 263\n",
      "\n",
      "[-1.6025391   0.20263672 -0.2841797   0.4177246   0.8183594   0.7573242 ] 4   4 Match 264\n",
      "\n",
      "[-1.2753906   0.2878418   0.35009766  0.703125    0.51904297 -0.1640625 ] 3   0 \n",
      "[ 0.03549194  0.7763672   0.03009033 -0.34472656 -0.55078125 -0.3395996 ] 1   2 \n",
      "[-1.2275391   0.20727539  0.12359619  0.6875      0.45166016  0.12780762] 3   3 Match 265\n",
      "\n",
      "[-0.31762695  0.44580078  0.19458008 -0.18823242 -0.1640625  -0.01856995] 1   0 \n",
      "[ 0.42895508  0.6435547   0.2607422  -0.29345703 -0.71533203 -0.60009766] 1   2 \n",
      "[ 0.7866211   0.5288086   0.3112793  -0.25317383 -1.0019531  -0.9086914 ] 0   1 \n",
      "[-1.6640625  -0.36450195 -0.41503906  0.9482422   1.2783203   0.96533203] 4   4 Match 266\n",
      "\n",
      "[-1.3242188   0.01737976  0.1182251   0.6254883   0.60253906  0.20629883] 3   1 \n",
      "[-0.29296875  0.32055664  0.35229492 -0.09399414 -0.12756348 -0.25878906] 2   3 \n",
      "[-2.0654297e-01  6.5527344e-01  4.9972534e-04 -1.2445068e-01\n",
      " -3.8208008e-01 -2.0812988e-01] 1   0 \n",
      "[-0.32177734  0.7270508   0.03988647 -0.24926758 -0.31518555  0.02313232] 1   1 Match 267\n",
      "\n",
      "[-0.9482422   0.07421875  0.5         0.7392578   0.27172852 -0.3466797 ] 3   3 Match 268\n",
      "\n",
      "[-1.6582031   0.17468262 -0.32592773  0.31713867  1.0234375   0.8730469 ] 4   5 \n",
      "[-1.0009766   0.20349121  0.40307617  0.71435547  0.21972656 -0.25512695] 3   4 \n",
      "[-1.7304688  -0.10516357 -0.09368896  0.7636719   1.0410156   0.6665039 ] 4   4 Match 269\n",
      "\n",
      "[-0.13891602  0.5410156   0.2915039   0.0670166  -0.3088379  -0.47729492] 1   0 \n",
      "[ 0.06317139  0.6333008  -0.16577148 -0.44726562 -0.4091797   0.02268982] 1   0 \n",
      "[-0.04690552  0.7246094   0.41137695  0.04675293 -0.5214844  -0.6777344 ] 1   2 \n",
      "[-1.8525391  -0.05148315 -0.2368164   0.8208008   1.0644531   0.8808594 ] 4   0 \n",
      "[-0.93115234  0.57421875 -0.03573608  0.1583252   0.0680542   0.31811523] 1   2 \n",
      "[-1.6865234   0.44433594 -0.28955078  0.57958984  0.82714844  0.7421875 ] 4   1 \n",
      "[-1.6972656  -0.05041504 -0.3100586   0.49853516  1.1142578   0.8442383 ] 4   3 \n",
      "[-0.6254883   0.3215332   0.24780273  0.10827637  0.02561951 -0.13928223] 1   3 \n",
      "[-1.1396484   0.3647461   0.2998047   0.8120117   0.2565918  -0.38793945] 3   5 \n",
      "[-1.5917969  -0.2783203   0.16235352  0.8779297   1.0087891   0.42626953] 4   3 \n",
      "[-0.15856934  0.55078125  0.54248047  0.37304688 -0.4152832  -0.7529297 ] 1   1 Match 270\n",
      "\n",
      "[-1.2041016   0.18591309  0.47802734  1.0244141   0.25610352 -0.47045898] 3   0 \n",
      "[-1.5634766   0.29711914 -0.3095703   0.24560547  0.82421875  0.92285156] 5   2 \n",
      "[-1.0498047   0.1796875   0.27270508  0.0838623   0.22888184  0.12286377] 2   2 Match 271\n",
      "\n",
      "[-0.36376953  0.10412598  0.4855957   0.36206055 -0.08203125 -0.38256836] 2   1 \n",
      "[-1.1230469   0.00791168  0.31030273  1.0664062   0.4440918  -0.32666016] 3   5 \n",
      "[-1.5976562   0.2800293  -0.31518555  0.3544922   0.95654297  0.87646484] 4   2 \n",
      "[-0.43798828  0.20898438  0.3864746   0.32470703 -0.03485107 -0.4633789 ] 2   1 \n",
      "[-1.6357422   0.04397583 -0.11279297  0.7548828   0.72558594  0.22644043] 3   4 \n",
      "[-1.5419922   0.17675781  0.06231689  0.47607422  0.9160156   0.4650879 ] 4   1 \n",
      "[ 0.04290771  0.60498047  0.2668457  -0.05657959 -0.4753418  -0.51416016] 1   1 Match 272\n",
      "\n",
      "[-1.6289062  -0.28271484 -0.06689453  0.8652344   1.0302734   0.63916016] 4   4 Match 273\n",
      "\n",
      "[-0.22619629  0.6245117   0.13647461 -0.04446411 -0.2854004  -0.1138916 ] 1   3 \n",
      "[-1.6328125  -0.33374023  0.43164062  1.1044922   0.9038086   0.11077881] 3   4 \n",
      "[-0.6376953   0.03262329  0.23059082  0.7290039   0.01464844 -0.12207031] 3   4 \n",
      "[-0.68896484  0.4428711   0.59716797  0.6201172  -0.08374023 -0.71435547] 3   2 \n",
      "[-1.5439453   0.25512695 -0.105896    0.5131836   0.8046875   0.57910156] 4   4 Match 274\n",
      "\n",
      "[-8.0761719e-01  6.2597656e-01  2.4926758e-01  2.9638672e-01\n",
      " -2.8800964e-04 -1.5917969e-01] 1   4 \n",
      "[-1.8134766   0.02548218 -0.4243164   0.5703125   1.1201172   1.0146484 ] 4   4 Match 275\n",
      "\n",
      "[-1.6318359   0.05218506 -0.33325195  0.54003906  0.87890625  0.8105469 ] 4   5 \n",
      "[-1.796875    0.11474609 -0.40063477  0.45117188  1.0957031   1.1347656 ] 5   5 Match 276\n",
      "\n",
      "[-1.7167969  -0.31689453 -0.45141602  0.84765625  1.0839844   0.8544922 ] 4   0 \n",
      "[-1.6191406  -0.12335205 -0.05197144  0.8491211   1.0605469   0.42993164] 4   5 \n",
      "[-1.6582031  -0.23950195 -0.21057129  0.74072266  0.9716797   0.57958984] 4   3 \n",
      "[-1.7255859   0.02261353 -0.37426758  0.5839844   0.99609375  0.84277344] 4   4 Match 277\n",
      "\n",
      "[-0.5649414   0.60253906  0.04632568 -0.05899048 -0.07366943  0.19897461] 1   3 \n",
      "[-1.125      -0.01443481  0.51660156  1.0126953   0.23803711 -0.42504883] 3   3 Match 278\n",
      "\n",
      "[-0.9711914  -0.18579102  0.36035156  0.7763672   0.27563477 -0.0838623 ] 3   4 \n",
      "[-0.6533203   0.4814453   0.51953125  0.8808594  -0.16259766 -1.0205078 ] 3   3 Match 279\n",
      "\n",
      "[-0.13635254  0.60302734  0.5864258   0.15649414 -0.46533203 -0.8022461 ] 1   1 Match 280\n",
      "\n",
      "[ 0.54541016  0.5756836  -0.14746094 -0.06030273 -1.0351562  -0.56933594] 1   1 Match 281\n",
      "\n",
      "[-1.5410156   0.37109375 -0.15014648  0.41064453  0.72509766  0.73779297] 5   1 \n",
      "[-1.6933594   0.04092407 -0.2849121   0.39038086  1.0498047   0.93066406] 4   5 \n",
      "[-0.01048279  0.45458984  0.27172852  0.11395264 -0.37353516 -0.50878906] 1   0 \n",
      "[-1.6816406  -0.58203125 -0.1496582   1.1220703   1.1699219   0.6557617 ] 4   1 \n",
      "[-0.90185547  0.13415527  0.38134766  0.5913086   0.06732178 -0.18054199] 3   3 Match 282\n",
      "\n",
      "[-0.5078125   0.4309082   0.14086914  0.08361816 -0.0625     -0.04089355] 1   1 Match 283\n",
      "\n",
      "[-0.62890625  1.0712891  -0.17138672  0.2331543  -0.43115234 -0.5       ] 1   1 Match 284\n",
      "\n",
      "[-1.6757812   0.17468262 -0.13098145  0.37402344  0.8413086   0.83740234] 4   5 \n",
      "[-1.3867188   0.05203247 -0.10748291  0.60791016  0.5209961   0.5361328 ] 3   4 \n",
      "[ 0.12023926  0.52001953  0.3557129   0.3347168  -0.67089844 -1.0322266 ] 1   2 \n",
      "[-1.0302734   0.25146484  0.29736328  0.8144531   0.35229492 -0.4699707 ] 3   3 Match 285\n",
      "\n",
      "[-0.5576172   0.2788086   0.56689453  0.7763672  -0.11962891 -0.8457031 ] 3   1 \n",
      "[-0.01425934  0.8613281  -0.00383949  0.03640747 -0.61083984 -0.5180664 ] 1   2 \n",
      "[-1.7773438  -0.15515137 -0.2524414   0.80029297  0.8955078   0.75634766] 4   1 \n",
      "[-0.16687012  0.91503906  0.14929199 -0.04779053 -0.45507812 -0.25463867] 1   2 \n",
      "[-0.71972656  0.53125     0.30078125  0.2529297  -0.0506897  -0.09490967] 1   4 \n",
      "[ 0.6850586   0.8017578   0.01715088 -0.6010742  -0.9472656  -0.4309082 ] 1   0 \n",
      "[-1.8339844  -0.34057617 -0.41210938  0.7626953   1.2275391   1.1669922 ] 4   4 Match 286\n",
      "\n",
      "[-0.40722656  0.3618164   0.7451172   0.56640625 -0.5102539  -1.1455078 ] 2   4 \n",
      "[-0.5830078   0.44580078  0.30273438  0.28857422 -0.13867188 -0.28344727] 1   4 \n",
      "[-1.5830078   0.2800293  -0.26391602  0.34814453  0.69189453  0.9448242 ] 5   4 \n",
      "[-1.8544922  -0.22094727 -0.10583496  0.95751953  1.0898438   0.7910156 ] 4   5 \n",
      "[-1.8291016  -0.34179688 -0.0980835   0.8330078   1.1748047   0.8261719 ] 4   5 \n",
      "[-0.18054199  0.46191406  0.56689453  0.23986816 -0.34570312 -0.7817383 ] 2   2 Match 287\n",
      "\n",
      "[-1.6494141   0.29882812 -0.22680664  0.3244629   0.91552734  0.6508789 ] 4   1 \n",
      "[-1.6757812   0.052948   -0.06463623  0.73876953  0.9765625   0.4567871 ] 4   4 Match 288\n",
      "\n",
      "[-1.6416016   0.34448242 -0.35791016  0.48608398  0.8564453   0.85253906] 4   4 Match 289\n",
      "\n",
      "[-1.7421875  -0.14624023 -0.27416992  0.76464844  1.0742188   0.7866211 ] 4   3 \n",
      "[-1.6132812   0.10845947  0.15283203  0.94140625  0.7416992   0.13513184] 3   3 Match 290\n",
      "\n",
      "[-0.89453125  0.41430664  0.53564453  0.70410156  0.15808105 -0.6376953 ] 3   3 Match 291\n",
      "\n",
      "[-0.5883789   0.39794922  0.11730957 -0.02394104 -0.11871338  0.14672852] 1   2 \n",
      "[-1.6767578  -0.27514648 -0.30395508  0.8666992   1.203125    0.99560547] 4   3 \n",
      "[-1.2626953   0.35546875  0.1809082   0.3942871   0.5341797   0.23950195] 4   3 \n",
      "[ 0.20837402  0.8144531  -0.08459473 -0.30615234 -0.7138672  -0.46313477] 1   4 \n",
      "[-1.7490234  -0.33642578 -0.22888184  0.7836914   1.2236328   0.91015625] 4   1 \n",
      "[-1.7451172   0.10369873 -0.41503906  0.47558594  1.1982422   1.1904297 ] 4   4 Match 292\n",
      "\n",
      "[-1.8623047  -0.23486328 -0.44458008  0.7163086   1.2216797   1.03125   ] 4   3 \n",
      "[-1.6533203   0.22644043 -0.10137939  0.5058594   0.7373047   0.6791992 ] 4   5 \n",
      "[-0.6977539   1.0791016  -0.37109375  0.18469238 -0.34350586 -0.09661865] 1   1 Match 293\n",
      "\n",
      "[-0.7441406   0.61816406 -0.11218262  0.19055176 -0.00407028  0.28076172] 1   1 Match 294\n",
      "\n",
      "[-1.7910156  -0.21484375 -0.28833008  0.80566406  1.1230469   0.8564453 ] 4   5 \n",
      "[ 0.33447266  0.4794922   0.42651367  0.10192871 -0.7919922  -1.1582031 ] 1   3 \n",
      "[-1.2441406   0.32983398 -0.01504517  0.63720703  0.6665039   0.3251953 ] 4   1 \n",
      "[-0.5810547   0.22607422  0.5834961   0.5488281   0.04974365 -0.5830078 ] 2   2 Match 295\n",
      "\n",
      "[-1.5605469   0.24829102 -0.08935547  0.74658203  0.79248047  0.5053711 ] 4   4 Match 296\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.34106445  0.59521484  0.09753418 -0.01676941 -0.29833984  0.01161957] 1   1 Match 297\n",
      "\n",
      "[-0.33032227  0.36376953  0.35229492  0.140625   -0.17089844 -0.22058105] 1   1 Match 298\n",
      "\n",
      "[-0.70166016  0.13122559  0.7182617   0.58691406  0.090271   -0.7631836 ] 2   3 \n",
      "[-1.2304688   0.51220703  0.34350586  0.6328125  -0.02798462 -0.6040039 ] 3   0 \n",
      "[-1.6123047   0.10900879 -0.24938965  0.64746094  1.0566406   0.68408203] 4   1 \n",
      "[-1.46875    -0.06237793 -0.4855957   0.3840332   1.0527344   0.7685547 ] 4   5 \n",
      "[-0.8598633   0.36694336  0.5708008   0.5961914   0.08300781 -0.32958984] 3   2 \n",
      "[-0.3959961   1.0625     -0.25048828 -0.1072998  -0.4645996  -0.26049805] 1   3 \n",
      "[-0.7133789   0.25512695  0.21008301  0.15429688 -0.04989624  0.29589844] 5   4 \n",
      "[-0.50683594  0.6298828   0.43847656  0.19311523  0.04266357 -0.58740234] 1   2 \n",
      "[-0.31079102  0.8515625   0.36938477  0.28857422 -0.3918457  -0.7841797 ] 1   1 Match 299\n",
      "\n",
      "[-1.3916016   0.33691406 -0.15283203  0.32250977  0.5678711   0.6816406 ] 5   3 \n",
      "[-1.7119141  -0.29858398 -0.17651367  0.83203125  1.1103516   0.8203125 ] 4   4 Match 300\n",
      "\n",
      "[-0.8486328   0.4501953   0.6665039   0.67578125 -0.06658936 -1.0332031 ] 3   1 \n",
      "[ 0.1459961   0.49194336  0.40039062  0.05667114 -0.54345703 -0.7939453 ] 1   3 \n",
      "[-1.7783203  -0.11212158 -0.42114258  0.6166992   1.1894531   1.0957031 ] 4   5 \n",
      "[-0.47875977  0.6176758  -0.00249481 -0.04550171 -0.2927246   0.17895508] 1   1 Match 301\n",
      "\n",
      "[-1.2119141   0.5498047   0.28466797  0.29418945  0.55371094  0.06835938] 4   1 \n",
      "[-1.6210938  -0.3581543  -0.18334961  0.9604492   1.0917969   0.7207031 ] 4   0 \n",
      "[-1.5771484   0.20056152 -0.22424316  0.39648438  0.86376953  0.8276367 ] 4   5 \n",
      "[-1.7109375  -0.23901367 -0.33154297  0.72998047  1.1943359   1.0009766 ] 4   3 \n",
      "[-1.203125    0.25463867  0.22058105  0.80126953  0.3935547  -0.18762207] 3   4 \n",
      "[-1.7158203  -0.02546692 -0.13134766  0.5415039   0.9892578   0.70751953] 4   1 \n",
      "[-1.8037109  -0.2746582  -0.26489258  0.9526367   1.0947266   0.875     ] 4   4 Match 302\n",
      "\n",
      "[-1.7578125   0.07733154 -0.30151367  0.31469727  1.2011719   1.0332031 ] 4   5 \n",
      "[-0.5654297   0.37841797  0.49780273  0.26293945 -0.03991699 -0.33154297] 2   2 Match 303\n",
      "\n",
      "[-0.19873047  0.14782715  0.3647461   0.22338867 -0.37475586 -0.4560547 ] 2   0 \n",
      "[ 0.112854    1.0322266  -0.39648438 -0.36401367 -0.90283203 -0.17712402] 1   1 Match 304\n",
      "\n",
      "[-1.7246094  -0.3779297  -0.20422363  1.0263672   1.2089844   0.77783203] 4   4 Match 305\n",
      "\n",
      "[-1.8466797  -0.11468506 -0.34155273  0.5488281   1.1972656   0.9433594 ] 4   5 \n",
      "[-0.96875     0.16113281  0.14587402  0.5830078   0.3623047   0.02482605] 3   1 \n",
      "[-1.7167969  -0.04156494 -0.07836914  0.57128906  0.87158203  0.73583984] 4   5 \n",
      "[-0.13830566  0.53222656  0.5839844   0.37304688 -0.58984375 -1.1855469 ] 2   3 \n",
      "[-1.6972656  -0.13684082 -0.3515625   0.5102539   1.1582031   0.9169922 ] 4   5 \n",
      "[ 0.08837891  0.8232422   0.22485352 -0.1887207  -0.5541992  -0.4111328 ] 1   1 Match 306\n",
      "\n",
      "[-0.8310547   0.03887939  0.13134766  0.5341797   0.33544922  0.03967285] 3   5 \n",
      "[-1.1572266   0.30932617  0.07775879  0.48095703  0.5097656   0.11669922] 4   1 \n",
      "[-0.64453125  0.25756836  0.4711914   0.2524414   0.10046387 -0.19152832] 2   4 \n",
      "[-1.6953125  -0.31152344 -0.37231445  0.8208008   1.1787109   0.99609375] 4   2 \n",
      "[-0.02993774  0.56884766  0.09442139 -0.14245605 -0.39892578 -0.03952026] 1   5 \n",
      "[ 0.43530273  0.60839844  0.40966797 -0.32177734 -0.7963867  -0.80126953] 1   1 Match 307\n",
      "\n",
      "[-1.6806641  -0.29077148 -0.23339844  0.6899414   1.0908203   0.85498047] 4   5 \n",
      "[-1.6816406  -0.11352539 -0.28930664  0.6713867   1.0566406   0.86621094] 4   4 Match 308\n",
      "\n",
      "[-0.67871094  0.47827148  0.16259766  0.19812012  0.14318848 -0.27197266] 1   5 \n",
      "[-0.8569336   0.3942871   0.0713501   0.29052734  0.0390625   0.36914062] 1   4 \n",
      "[-0.921875    0.25708008 -0.01965332  0.3708496   0.29370117  0.38891602] 5   1 \n",
      "[-0.6777344   0.48901367  0.3618164   0.65234375  0.03717041 -0.56591797] 3   2 \n",
      "[-0.35229492  0.25976562  0.2578125   0.16479492 -0.03451538 -0.1821289 ] 1   5 \n",
      "[-1.7099609  -0.25439453 -0.06427002  0.9448242   0.98583984  0.5283203 ] 4   1 \n",
      "[-1.6650391  -0.10992432  0.00544739  0.9067383   0.9121094   0.46923828] 4   4 Match 309\n",
      "\n",
      "[-1.7119141  -0.00537491 -0.06658936  0.74560547  0.7993164   0.2944336 ] 4   4 Match 310\n",
      "\n",
      "[-1.7314453   0.11419678 -0.21069336  0.22583008  0.91748047  0.95751953] 5   4 \n",
      "[-1.7460938  -0.18981934 -0.33251953  0.8828125   0.9267578   0.82714844] 4   5 \n",
      "[ 0.10992432  0.6875      0.6074219   0.2932129  -0.62402344 -1.2460938 ] 1   3 \n",
      "[-1.8173828  -0.11358643 -0.35253906  0.7519531   1.0732422   1.0966797 ] 5   5 Match 311\n",
      "\n",
      "[-0.67822266  0.41381836  0.2783203   0.5341797  -0.05621338 -0.2619629 ] 3   4 \n",
      "[-0.60498047  0.09301758  0.6928711   0.92041016 -0.1730957  -0.9667969 ] 3   4 \n",
      "[-1.5107422   0.10150146 -0.02125549  0.72558594  0.58691406  0.11181641] 3   1 \n",
      "[ 0.45922852  0.87646484 -0.22644043 -0.4111328  -0.9394531  -0.34985352] 1   0 \n",
      "[-1.2714844   0.53515625 -0.34326172  0.18395996  0.3395996   0.7285156 ] 5   4 \n",
      "[-1.5751953  -0.2541504  -0.26611328  0.7714844   0.9477539   0.7583008 ] 4   3 \n",
      "[-0.39916992  0.43237305  0.640625    0.4909668  -0.17810059 -0.7026367 ] 2   3 \n",
      "[ 0.67089844  0.45825195  0.15649414 -0.37768555 -0.69433594 -0.54052734] 0   1 \n",
      "[-1.7929688  -0.10351562 -0.25878906  0.6904297   1.1767578   0.84716797] 4   4 Match 312\n",
      "\n",
      "[-1.0322266   0.35498047  0.5053711   0.69970703  0.29077148 -0.4765625 ] 3   2 \n",
      "[-0.51171875  0.5029297   0.22546387  0.02854919 -0.04696655 -0.31079102] 1   3 \n",
      "[ 0.11602783  0.8691406   0.00591278 -0.30029297 -0.63134766 -0.39257812] 1   4 \n",
      "[-0.30493164  0.55322266  0.51416016  0.08526611 -0.17138672 -0.46142578] 1   4 \n",
      "[-1.3798828   0.09338379  0.4741211   0.9375      0.52734375 -0.17321777] 3   4 \n",
      "[-1.3642578   0.19067383  0.19689941  0.49731445  0.71875     0.42138672] 4   3 \n",
      "[-1.3417969  -0.04208374 -0.11645508  0.7607422   0.73046875  0.33911133] 3   4 \n",
      "[-1.6142578  -0.01858521 -0.25585938  0.65527344  0.8027344   0.6953125 ] 4   4 Match 313\n",
      "\n",
      "[-0.75390625  0.19543457  0.4501953   0.69091797  0.0904541  -0.5727539 ] 3   2 \n",
      "[-1.7070312  -0.1977539  -0.20361328  0.67333984  1.1640625   0.8066406 ] 4   5 \n",
      "[-0.9375      0.30786133  0.640625    0.8569336  -0.01109314 -0.81347656] 3   4 \n",
      "[ 0.79248047  0.70410156  0.04708862 -0.65283203 -0.9892578  -0.55029297] 0   1 \n",
      "[-0.55126953  0.39868164  0.2763672   0.08618164 -0.05145264  0.03726196] 1   1 Match 314\n",
      "\n",
      "[-1.6367188   0.22583008  0.04034424  0.6254883   0.81884766  0.3564453 ] 4   2 \n",
      "[-1.921875   -0.13720703 -0.3725586   0.7734375   1.1425781   1.0878906 ] 4   5 \n",
      "[-0.87158203  0.16992188  0.2770996   0.7397461  -0.12145996 -0.5366211 ] 3   3 Match 315\n",
      "\n",
      "[-1.7021484   0.0280304  -0.23730469  0.47875977  1.0927734   0.6748047 ] 4   4 Match 316\n",
      "\n",
      "[-1.8955078e+00 -3.6215782e-04 -3.0224609e-01  6.1181641e-01\n",
      "  1.1220703e+00  1.0244141e+00] 4   3 \n",
      "[-1.5605469   0.13183594  0.2902832   1.1630859   0.4152832  -0.38208008] 3   4 \n",
      "[ 0.39233398  0.68066406  0.17346191 -0.24206543 -0.68310547 -0.54052734] 1   0 \n",
      "[-1.7851562  -0.1151123  -0.28759766  0.5341797   1.1601562   0.8618164 ] 4   3 \n",
      "[-0.8144531   0.2685547   0.22839355  0.4741211   0.23205566 -0.23400879] 3   1 \n",
      "[-1.6992188  -0.40698242 -0.13232422  0.8417969   1.0625      0.6455078 ] 4   4 Match 317\n",
      "\n",
      "[-0.3540039   0.86816406 -0.16625977 -0.33862305 -0.33447266  0.12231445] 1   4 \n",
      "[-1.6992188  -0.28515625 -0.07373047  0.9921875   1.1591797   0.6928711 ] 4   4 Match 318\n",
      "\n",
      "[-1.7626953  -0.17773438 -0.32495117  0.78222656  1.0693359   0.9970703 ] 4   4 Match 319\n",
      "\n",
      "[-0.8535156   0.5366211   0.17199707  0.39111328 -0.050354   -0.20483398] 1   1 Match 320\n",
      "\n",
      "[-1.5644531  -0.23913574 -0.03890991  1.1220703   0.8935547   0.29101562] 3   2 \n",
      "[-1.0566406   0.6826172   0.4260254   0.46948242  0.12805176 -0.43701172] 1   1 Match 321\n",
      "\n",
      "[-0.1262207   0.49267578  0.05508423 -0.14416504 -0.3461914   0.04296875] 1   4 \n",
      "[-0.56591797  0.61621094  0.54833984  0.71972656 -0.38330078 -1.1757812 ] 3   1 \n",
      "[ 0.29785156  0.72998047  0.42822266 -0.06115723 -0.6118164  -0.7050781 ] 1   2 \n",
      "[-0.5239258   0.6191406   0.03393555  0.00865173 -0.04031372 -0.03726196] 1   5 \n",
      "[-1.6796875  -0.37695312 -0.2175293   0.8261719   1.1894531   0.9086914 ] 4   2 \n",
      "[-1.3369141  -0.18029785  0.2409668   0.94873047  0.75341797  0.08996582] 3   3 Match 322\n",
      "\n",
      "[-1.7177734  -0.19189453 -0.38427734  0.8100586   1.1796875   0.9682617 ] 4   4 Match 323\n",
      "\n",
      "[-1.1845703   0.3725586   0.6977539   1.0986328   0.15588379 -0.76904297] 3   5 \n",
      "[-1.5986328  -0.12585449 -0.15600586  0.90478516  0.79052734  0.25195312] 3   0 \n",
      "[-1.6777344   0.24108887 -0.31469727  0.34643555  1.0664062   0.88134766] 4   3 \n",
      "[-1.4667969   0.08221436  0.01358795  0.6503906   0.84033203  0.37670898] 4   5 \n",
      "[ 0.4951172   0.6875     -0.18164062 -0.6201172  -0.80908203 -0.4638672 ] 1   2 \n",
      "[ 0.40820312  0.4963379   0.32299805 -0.42626953 -0.7421875  -0.5366211 ] 1   3 \n",
      "[-0.18139648  0.66796875  0.33325195  0.19616699 -0.32373047 -0.38842773] 1   4 \n",
      "[-1.7822266  -0.29052734 -0.26220703  1.0322266   0.94677734  0.8027344 ] 3   5 \n",
      "[-0.4885254   0.3544922   0.234375    0.3059082  -0.05667114 -0.3400879 ] 1   3 \n",
      "[-0.20654297  0.7392578   0.14562988 -0.2607422  -0.41308594 -0.23461914] 1   0 \n",
      "[-0.94189453  0.25170898  0.89160156  0.6772461   0.23022461 -0.69433594] 2   2 Match 324\n",
      "\n",
      "[ 0.27978516  0.6274414   0.28686523 -0.24279785 -0.5888672  -0.4194336 ] 1   1 Match 325\n",
      "\n",
      "[-0.41601562  0.55126953 -0.0423584   0.01832581 -0.1628418   0.11566162] 1   2 \n",
      "[-0.9453125   0.01039124  0.22827148  0.51220703  0.29003906  0.22302246] 3   1 \n",
      "[-0.33618164  0.23291016  0.22363281  0.17333984 -0.07092285 -0.11096191] 1   5 \n",
      "[-0.5629883   0.6767578   0.6196289   0.6455078  -0.23046875 -1.0498047 ] 1   2 \n",
      "[-0.24060059  0.46118164  0.171875    0.27148438 -0.59765625 -0.6567383 ] 1   0 \n",
      "[-1.6240234  -0.06137085 -0.16101074  0.4567871   0.86083984  0.8339844 ] 4   2 \n",
      "[-0.07446289  0.7709961  -0.03204346 -0.2121582  -0.54589844 -0.42407227] 1   5 \n",
      "[-1.5195312  -0.08013916  0.3334961   1.1357422   0.68847656 -0.059021  ] 3   3 Match 326\n",
      "\n",
      "[-0.81591797  0.35107422  0.09576416  0.35595703  0.19799805 -0.08026123] 3   5 \n",
      "[-0.13232422  0.296875    0.53466797  0.39990234 -0.37670898 -0.94140625] 2   1 \n",
      "[-0.46069336  0.37304688  0.4802246   0.16601562 -0.23034668 -0.24267578] 2   1 \n",
      "[-0.4934082   0.7573242   0.27929688  0.03399658 -0.3334961  -0.44580078] 1   5 \n",
      "[-0.5830078   0.74902344 -0.10095215 -0.07299805 -0.13647461  0.27783203] 1   1 Match 327\n",
      "\n",
      "[-1.1103516   0.9794922  -0.3942871   0.31347656 -0.02111816  0.31176758] 1   1 Match 328\n",
      "\n",
      "[-1.5380859   0.43017578 -0.19018555  0.4091797   0.61865234  0.5571289 ] 4   3 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.96484375  0.3395996  -0.03305054  0.21484375  0.35327148  0.07873535] 4   4 Match 329\n",
      "\n",
      "[-0.7207031   1.0869141  -0.38745117 -0.10748291 -0.2998047  -0.21520996] 1   3 \n",
      "[ 0.39282227  0.7285156   0.30639648 -0.21728516 -0.7866211  -0.78027344] 1   3 \n",
      "[-1.4394531   0.17126465 -0.13879395  0.5336914   0.3630371   0.78222656] 5   4 \n",
      "[-1.7841797  -0.05825806 -0.42700195  0.3149414   1.2177734   1.0107422 ] 4   5 \n",
      "[-0.09637451  0.69189453  0.21130371 -0.20788574 -0.4519043  -0.28710938] 1   5 \n",
      "[ 0.70458984  0.59765625  0.02883911 -0.5644531  -0.96875    -0.60253906] 0   5 \n",
      "[-0.9326172   0.6123047  -0.11621094  0.38110352  0.25439453  0.09747314] 1   2 \n",
      "[-1.8076172  -0.34545898 -0.37939453  0.890625    1.2578125   1.0097656 ] 4   4 Match 330\n",
      "\n",
      "[-1.6748047   0.02682495 -0.33398438  0.53222656  0.92041016  1.0498047 ] 5   2 \n",
      "[-1.0449219   0.03262329  0.48364258  0.5546875   0.1907959  -0.28393555] 3   4 \n",
      "[-0.5786133   0.54003906  0.38598633  0.3972168  -0.02575684 -0.47485352] 1   4 \n",
      "[ 0.33618164  0.48413086  0.3720703   0.09716797 -0.89990234 -1.1308594 ] 1   2 \n",
      "[-1.6708984   0.04907227 -0.18591309  0.63427734  0.88183594  0.67626953] 4   2 \n",
      "[ 0.24987793  0.4897461   0.2076416  -0.25805664 -0.46191406 -0.4831543 ] 1   2 \n",
      "[ 0.44140625  0.7895508   0.40820312 -0.4621582  -0.62841797 -0.67529297] 1   2 \n",
      "[ 0.09967041  0.6464844   0.39990234 -0.24572754 -0.52001953 -0.6298828 ] 1   3 \n",
      "[-1.1220703   0.2364502  -0.22607422  0.28295898  0.41015625  0.46166992] 5   4 \n",
      "[ 0.921875    0.6357422   0.02114868 -0.5263672  -1.1279297  -0.8208008 ] 0   0 Match 331\n",
      "\n",
      "[ 0.0609436   0.53564453  0.67529297  0.2644043  -0.47485352 -1.1328125 ] 2   3 \n",
      "[-0.96435547  0.5175781  -0.02157593  0.2487793   0.38208008  0.1965332 ] 1   5 \n",
      "[-0.70654297  0.5654297   0.70751953  0.546875   -0.3005371  -1.0117188 ] 2   3 \n",
      "[-0.5048828   0.46362305  0.23693848  0.08520508 -0.04067993 -0.08050537] 1   1 Match 332\n",
      "\n",
      "[-1.8447266  -0.17224121 -0.35424805  0.7294922   1.1113281   0.9169922 ] 4   4 Match 333\n",
      "\n",
      "[-1.2441406  -0.03012085  0.49902344  1.046875    0.33764648 -0.4724121 ] 3   4 \n",
      "[-1.8232422  -0.06561279 -0.4567871   0.6147461   1.2871094   1.0712891 ] 4   3 \n",
      "[-1.4951172   0.27319336 -0.16503906  0.48413086  0.7246094   0.6875    ] 4   5 \n",
      "[-0.5180664   0.15466309  0.23852539  0.39404297 -0.04403687 -0.1652832 ] 3   4 \n",
      "[-1.5712891  -0.02394104  0.0869751   0.69873047  0.9326172   0.44335938] 4   4 Match 334\n",
      "\n",
      "[-1.2832031   0.23425293 -0.29907227  0.31469727  0.71777344  0.4338379 ] 4   4 Match 335\n",
      "\n",
      "[-0.65283203  0.4440918  -0.05114746  0.01629639  0.12280273  0.4116211 ] 1   2 \n",
      "[-0.6459961   0.64990234 -0.04919434 -0.05444336 -0.13635254  0.40795898] 1   4 \n",
      "[-0.72265625  0.92089844  0.22607422  0.40844727 -0.27441406 -0.5390625 ] 1   2 \n",
      "[-1.6884766  -0.32226562 -0.4189453   0.84375     1.2597656   1.0546875 ] 4   5 \n",
      "[-0.95458984  0.27319336  0.5629883   0.875       0.02430725 -0.7211914 ] 3   2 \n",
      "[-1.46875     0.12261963 -0.20776367  0.48046875  0.7734375   0.6669922 ] 4   4 Match 336\n",
      "\n",
      "[-1.6367188   0.23986816  0.11260986  0.7597656   0.7836914   0.3557129 ] 4   2 \n",
      "[-1.7333984  0.0418396 -0.5830078  0.4333496  1.1572266  1.0917969] 4   5 \n",
      "[-1.5947266   0.14196777 -0.31274414  0.40454102  0.9560547   0.66748047] 4   5 \n",
      "[-0.80859375 -0.05288696  0.4477539   0.6425781   0.08843994 -0.19238281] 3   2 \n",
      "[-0.26757812  0.33764648  0.0368042   0.12005615 -0.21008301  0.07269287] 1   0 \n",
      "[-0.7548828   0.35839844  0.30151367  0.5698242   0.140625   -0.23815918] 3   1 \n",
      "[-1.7480469  -0.1652832  -0.14782715  0.82910156  0.9921875   0.9716797 ] 4   4 Match 337\n",
      "\n",
      "[-0.07543945  0.6533203   0.42993164  0.01705933 -0.5292969  -0.77441406] 1   2 \n",
      "[-1.2636719   0.4741211  -0.00634003  0.3798828   0.3395996   0.44726562] 1   4 \n",
      "[-1.5576172  -0.09112549 -0.04074097  0.8618164   0.68896484  0.52490234] 3   2 \n",
      "[ 0.6557617   0.6020508   0.19567871 -0.5185547  -0.7602539  -0.68359375] 0   5 \n",
      "[-1.5673828   0.1427002  -0.14733887  0.7207031   0.71240234  0.33666992] 3   2 \n",
      "[-1.7724609  -0.20129395 -0.12902832  0.9892578   0.97558594  0.5263672 ] 3   2 \n",
      "[-0.00159264  0.81152344  0.00369644 -0.3491211  -0.5644531  -0.30517578] 1   1 Match 338\n",
      "\n",
      "[-0.640625    0.24206543  0.38891602  0.7661133  -0.04251099 -0.7446289 ] 3   5 \n",
      "[ 0.5571289   0.6411133   0.4296875  -0.40625    -0.80029297 -0.8413086 ] 1   4 \n",
      "[ 0.2800293   0.7939453   0.4794922  -0.12597656 -0.6894531  -0.8203125 ] 1   1 Match 339\n",
      "\n",
      "[-0.67333984  0.5751953  -0.21020508  0.00879669  0.07836914  0.39233398] 1   2 \n",
      "[-1.1191406   0.11767578 -0.08325195  0.38134766  0.3605957   0.69189453] 5   3 \n",
      "[ 0.28222656  0.58447266  0.62841797  0.02342224 -0.6455078  -0.9682617 ] 2   1 \n",
      "[-0.4428711   0.6616211  -0.20581055 -0.30688477 -0.1875      0.27172852] 1   4 \n",
      "[-0.98828125  0.30200195  0.54296875  0.55566406  0.32348633 -0.5957031 ] 3   2 \n",
      "[-1.7958984  -0.17260742 -0.38671875  0.6582031   1.1435547   0.9765625 ] 4   5 \n",
      "[-0.67285156  0.58154297  0.5004883   0.32788086  0.02700806 -0.28686523] 1   5 \n",
      "[-0.65527344 -0.05569458  0.47338867  0.70703125 -0.03083801 -0.52783203] 3   2 \n",
      "[-0.32177734  0.37646484  0.2446289   0.05255127 -0.30981445 -0.265625  ] 1   2 \n",
      "[-1.4492188  -0.16345215  0.11968994  1.140625    0.734375    0.02436829] 3   4 \n",
      "[-1.7861328  -0.18530273 -0.5292969   0.6767578   1.2324219   1.046875  ] 4   4 Match 340\n",
      "\n",
      "[ 0.37036133  0.57714844  0.14758301 -0.21875    -0.61328125 -0.5966797 ] 1   1 Match 341\n",
      "\n",
      "[ 0.3166504   0.84472656 -0.02316284 -0.5488281  -0.7705078  -0.46484375] 1   0 \n",
      "[-0.90722656  0.37329102  0.3671875   0.58740234  0.17871094 -0.6503906 ] 3   4 \n",
      "[-0.93847656 -0.09289551  0.13378906  0.3540039   0.22155762  0.22607422] 3   3 Match 342\n",
      "\n",
      "[-1.0478516   0.78125    -0.0194397   0.01756287  0.05999756  0.04876709] 1   1 Match 343\n",
      "\n",
      "[ 0.03497314  0.29223633  0.33789062  0.14953613 -0.4099121  -0.4584961 ] 2   2 Match 344\n",
      "\n",
      "[-1.4433594   0.23828125  0.33764648  0.95947266  0.65234375 -0.19934082] 3   3 Match 345\n",
      "\n",
      "[-1.7294922  -0.2607422  -0.45092773  0.7055664   1.1826172   1.0263672 ] 4   3 \n",
      "[-1.2529297   0.6333008  -0.01593018  0.20471191  0.42138672  0.29101562] 1   4 \n",
      "[-0.2244873   0.15112305  0.25317383  0.17053223 -0.19018555 -0.4489746 ] 2   5 \n",
      "[-0.18896484  0.45581055  0.17663574 -0.05148315 -0.5292969  -0.1628418 ] 1   3 \n",
      "[-1.2285156   0.29956055 -0.04733276  0.54248047  0.41430664  0.50439453] 3   2 \n",
      "[-0.9008789   0.18395996  0.27734375  0.48632812  0.40039062  0.10253906] 3   2 \n",
      "[-1.6689453  -0.02716064 -0.49926758  0.36743164  1.0810547   0.93408203] 4   4 Match 346\n",
      "\n",
      "[-0.9980469   0.32788086 -0.0453186   0.18786621  0.3203125   0.40307617] 5   1 \n",
      "[-0.5517578   1.1376953  -0.4267578   0.07525635 -0.42578125  0.03086853] 1   1 Match 347\n",
      "\n",
      "[-0.20239258  0.6040039   0.28637695  0.08099365 -0.3852539  -0.2548828 ] 1   5 \n",
      "[-1.7041016 -0.4453125 -0.2746582  1.0849609  1.2089844  0.8730469] 4   5 \n",
      "[-0.32958984  0.1776123   0.50146484  0.6196289  -0.30273438 -0.9838867 ] 3   3 Match 348\n",
      "\n",
      "[-1.8115234   0.22351074 -0.06384277  0.8989258   0.89990234  0.53466797] 4   2 \n",
      "[-1.7207031  -0.02754211 -0.3010254   0.6503906   0.9736328   0.7836914 ] 4   0 \n",
      "[-0.7163086   0.33691406  0.3479004   0.56591797 -0.0758667  -0.46875   ] 3   3 Match 349\n",
      "\n",
      "[-0.7573242   0.8300781   0.390625    0.33007812 -0.01327515 -0.40673828] 1   5 \n",
      "[ 0.47485352  0.5317383   0.4165039  -0.03341675 -0.63623047 -0.75878906] 1   4 \n",
      "[-1.6679688  -0.25634766 -0.09228516  1.1171875   1.0107422   0.6347656 ] 3   5 \n",
      "[ 0.12597656  0.50439453  0.49560547  0.1394043  -0.5644531  -1.0908203 ] 1   3 \n",
      "[-0.7680664   0.1809082   0.34692383  0.60595703  0.0531311  -0.41455078] 3   2 \n",
      "[ 0.20214844  0.64746094  0.19726562  0.0463562  -0.6323242  -0.8833008 ] 1   2 \n",
      "[-1.1132812   0.49609375  0.37353516  0.72021484  0.14660645 -0.36035156] 3   1 \n",
      "[-1.3330078   0.5449219  -0.15515137  0.2388916   0.6225586   0.546875  ] 4   1 \n",
      "[-0.5761719   0.6430664   0.16882324  0.08996582 -0.09594727  0.03308105] 1   3 \n",
      "[-0.7163086   0.78466797  0.0904541   0.04919434 -0.00839996  0.04653931] 1   5 \n",
      "[ 0.05426025  0.44726562  0.23791504  0.16491699 -0.5151367  -0.6503906 ] 1   4 \n",
      "[-1.6005859   0.21252441 -0.10174561  0.5566406   0.58740234  0.30737305] 4   0 \n",
      "[-0.5239258   0.52197266  0.3203125   0.21057129 -0.24938965 -0.3942871 ] 1   2 \n",
      "[-1.7597656  -0.3569336  -0.33325195  1.0048828   1.1220703   0.8823242 ] 4   5 \n",
      "[-1.2392578  -0.03314209  0.31713867  0.75390625  0.5097656   0.10455322] 3   4 \n",
      "[ 0.2199707   0.6098633   0.03085327 -0.2709961  -0.5263672  -0.10723877] 1   2 \n",
      "[ 0.4963379   0.38916016  0.0737915  -0.22460938 -0.8881836  -0.6723633 ] 0   3 \n",
      "[-1.1044922   0.39086914  0.3310547   0.9970703   0.08874512 -0.6015625 ] 3   2 \n",
      "[-0.99853516  0.47070312  0.63671875  0.7675781   0.07092285 -0.69628906] 3   3 Match 350\n",
      "\n",
      "[-1.5654297   0.17321777 -0.1932373   0.5644531   0.9970703   0.68066406] 4   1 \n",
      "[-1.3818359   0.08135986  0.06048584  0.75927734  0.4777832   0.42333984] 3   2 \n",
      "[-1.2783203   0.04205322  0.6225586   0.99121094  0.34887695 -0.5527344 ] 3   5 \n",
      "[-0.34545898  0.02378845  0.47045898  0.33276367 -0.07897949 -0.42211914] 2   5 \n",
      "[-1.4414062  -0.23608398  0.32348633  0.9707031   0.8305664   0.21704102] 3   3 Match 351\n",
      "\n",
      "[-0.58740234  0.6254883   0.42163086  0.5185547  -0.27734375 -0.7890625 ] 1   3 \n",
      "[ 0.16125488  0.68066406  0.0440979  -0.32666016 -0.62841797 -0.36401367] 1   2 \n",
      "[-1.4951172  -0.08001709 -0.03146362  0.6582031   0.79833984  0.6875    ] 4   4 Match 352\n",
      "\n",
      "[-0.6669922   0.06878662  0.1081543   0.20239258 -0.03096008  0.2626953 ] 5   4 \n",
      "[-0.47485352  0.39819336  0.32836914 -0.03007507 -0.16015625 -0.03044128] 1   3 \n",
      "[-1.1142578   0.02537537  0.29101562  0.65478516  0.42016602  0.07720947] 3   1 \n",
      "[-0.5366211   0.40673828  0.5683594   0.63378906 -0.26635742 -1.0107422 ] 3   3 Match 353\n",
      "\n",
      "[-1.390625    0.16552734  0.59814453  1.0605469   0.4440918  -0.52197266] 3   4 \n",
      "[-1.7314453  -0.2692871   0.12127686  1.0615234   0.88427734  0.34375   ] 3   3 Match 354\n",
      "\n",
      "[-0.44604492  0.62060547  0.72314453  0.5385742  -0.34472656 -1.2148438 ] 2   2 Match 355\n",
      "\n",
      "[ 0.49829102  0.5571289   0.35864258  0.0019474  -0.75       -0.9609375 ] 1   1 Match 356\n",
      "\n",
      "[-1.4355469   0.37548828  0.06848145  0.4807129   0.4560547   0.5600586 ] 5   4 \n",
      "[-1.7470703   0.00816345 -0.23034668  0.6513672   0.85839844  0.8691406 ] 5   3 \n",
      "[-1.8173828  -0.3112793  -0.5205078   0.69091797  1.2558594   1.0693359 ] 4   4 Match 357\n",
      "\n",
      "[ 0.64501953  0.39624023  0.14550781 -0.0803833  -1.0410156  -0.9511719 ] 0   1 \n",
      "[-0.00995636  0.63134766  0.4423828  -0.19177246 -0.43603516 -0.55078125] 1   2 \n",
      "[-0.38476562  0.6425781   0.15661621  0.07659912 -0.27905273 -0.02935791] 1   2 \n",
      "[-0.9326172   0.2607422   0.31103516  0.25878906  0.16040039 -0.10058594] 2   0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3691406   0.3112793  -0.00666809  0.42407227  0.5810547   0.52197266] 4   2 \n",
      "[-0.7026367   0.47753906  0.01100922 -0.03244019 -0.09014893  0.21594238] 1   2 \n",
      "[-0.43554688  0.3322754   0.35839844  0.3840332   0.00083256 -0.4711914 ] 3   3 Match 358\n",
      "\n",
      "[-0.25024414  0.8623047   0.22314453 -0.00201035 -0.54345703 -0.7265625 ] 1   5 \n",
      "[-0.07122803  0.8305664   0.20117188 -0.21313477 -0.5673828  -0.4182129 ] 1   4 \n",
      "[-1.1884766   0.1126709   0.58984375  0.88720703  0.25878906 -0.5708008 ] 3   2 \n",
      "[-1.5791016  -0.08703613  0.12103271  0.9243164   0.8251953   0.44433594] 3   3 Match 359\n",
      "\n",
      "[ 0.07250977  0.8666992  -0.06240845 -0.4777832  -0.63671875 -0.11029053] 1   3 \n",
      "[-0.34228516  0.13513184  0.28051758  0.16723633 -0.28466797 -0.14453125] 2   4 \n",
      "[-1.6289062   0.14465332 -0.17089844  0.81640625  0.6113281   0.45898438] 3   4 \n",
      "[-0.9916992   0.01139069  0.43798828  0.6333008   0.3581543  -0.02700806] 3   2 \n",
      "[-1.7470703  -0.18493652 -0.40356445  0.4206543   1.3105469   1.015625  ] 4   3 \n",
      "[-1.7792969   0.06481934 -0.08746338  0.7783203   0.9711914   0.64501953] 4   2 \n",
      "[-0.45825195  0.10510254  0.18664551  0.46020508 -0.0401001  -0.10839844] 3   5 \n",
      "[ 0.21398926  0.90625    -0.08050537 -0.56103516 -0.79248047 -0.3017578 ] 1   1 Match 360\n",
      "\n",
      "[-1.2529297   0.1303711   0.57666016  1.0537109   0.29882812 -0.49194336] 3   2 \n",
      "[ 0.94140625  0.6508789   0.05007935 -0.8095703  -0.98876953 -0.64160156] 0   0 Match 361\n",
      "\n",
      "[-0.92333984  0.33007812 -0.00798798  0.24780273  0.22509766  0.08837891] 1   0 \n",
      "[-0.8066406   0.61035156 -0.05255127 -0.02471924 -0.02005005  0.44458008] 1   3 \n",
      "[-1.1835938   0.140625    0.52441406  0.99609375  0.26098633 -0.5644531 ] 3   2 \n",
      "[-1.921875   -0.24865723 -0.48779297  0.66064453  1.2890625   1.2265625 ] 4   1 \n",
      "[-0.8066406   0.7182617   0.4375      0.50878906 -0.08227539 -0.53515625] 1   4 \n",
      "[ 0.7055664   0.7324219   0.20532227 -0.44555664 -1.0009766  -0.7314453 ] 1   2 \n",
      "[ 0.27685547  0.6381836   0.27270508 -0.04309082 -0.63671875 -0.51220703] 1   0 \n",
      "[-0.6845703   0.6616211   0.08099365  0.06219482 -0.03338623  0.08435059] 1   1 Match 362\n",
      "\n",
      "362\n"
     ]
    }
   ],
   "source": [
    "Pred=[]\n",
    "\n",
    "countCorrect=0\n",
    "\n",
    "for row in range(TestModel_outputs.shape[0]):\n",
    "    outputs=TestModel_outputs[row]\n",
    "    #print(test.iloc[row,0])\n",
    "    print(outputs, end=' ')\n",
    "    \n",
    "    result=0\n",
    "    if outputs[0]<outputs[1]:result=1\n",
    "    if outputs[result]<outputs[2]:result=2\n",
    "    if outputs[result]<outputs[3]:result=3\n",
    "    if outputs[result]<outputs[4]:result=4\n",
    "    if outputs[result]<outputs[5]:result=5\n",
    "    Pred.append(result)\n",
    "    print(result, ' ',test.iloc[row,1], end=' ')\n",
    "    if result==test.iloc[row,1]:\n",
    "        countCorrect+=1\n",
    "        print('Match',countCorrect)\n",
    "    print('')\n",
    "\n",
    "print(countCorrect)\n",
    "#Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11  43   8  17  11   2]\n",
      " [  8 113  13  57  52   7]\n",
      " [  4  73  23  66  39   9]\n",
      " [  2  73  21  85  79   7]\n",
      " [  0  45   8  66 113  17]\n",
      " [  2  47   8  44  93  17]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    " \n",
    "print(metrics.confusion_matrix(test['labels'],Pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Pants       0.41      0.12      0.18        92\n",
      "       False       0.29      0.45      0.35       250\n",
      " Barely-True       0.28      0.11      0.16       214\n",
      "   Hlaf-True       0.25      0.32      0.28       267\n",
      " Mostly-True       0.29      0.45      0.36       249\n",
      "        True       0.29      0.08      0.13       211\n",
      "\n",
      "    accuracy                           0.28      1283\n",
      "   macro avg       0.30      0.26      0.24      1283\n",
      "weighted avg       0.29      0.28      0.26      1283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Pants', 'False', 'Barely-True','Hlaf-True','Mostly-True','True']\n",
    "\n",
    "print(metrics.classification_report(test['labels'], Pred,target_names =target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2821512081060016"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(test['labels'],Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Saving Complete on 2020-03-28 19:26:48.683056 in: ./TunedModels/albert/albert-large-v2/Saves/\n"
     ]
    }
   ],
   "source": [
    "# saving the output of the models to CSVs\n",
    "#these are 1X6 classification vectors\n",
    "\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Saves/\"\n",
    "print('Saving...')\n",
    "trainOut = pd.DataFrame(data= TrainModel_outputs )\n",
    "trainOut.to_csv(SavesDirectory+'trainOut.tsv', sep='\\t',  index=False)\n",
    "\n",
    "evalOut = pd.DataFrame(data= EvalModel_outputs )\n",
    "evalOut.to_csv(SavesDirectory+'evalOut.tsv', sep='\\t',  index=False)\n",
    "\n",
    "testOut = pd.DataFrame(data= TestModel_outputs )\n",
    "testOut.to_csv(SavesDirectory+'testOut.tsv', sep='\\t',  index=False)\n",
    "\n",
    "print('Saving Complete on',datetime.now() ,'in:', SavesDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)\n",
    "del(train,Eval,test)\n",
    "del(trainOut,evalOut,testOut)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Adding the reputation vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section takes the output results from the transformer used above and uses it together with the speaker's reputation to enhance the classification.\n",
    "\n",
    "Before running this section it is suggested that you halt the program and start running it again from this cell. The neural net will likely have an error caused by some unreleased variable used by thr simple transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PantsTotal</th>\n",
       "      <th>NotRealTotal</th>\n",
       "      <th>BarelyTotal</th>\n",
       "      <th>HalfTotal</th>\n",
       "      <th>MostlyTotal</th>\n",
       "      <th>Truths</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.042389</td>\n",
       "      <td>0.749023</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.281494</td>\n",
       "      <td>-0.387207</td>\n",
       "      <td>0.003132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-1.463867</td>\n",
       "      <td>-0.130737</td>\n",
       "      <td>0.175659</td>\n",
       "      <td>0.963379</td>\n",
       "      <td>0.816895</td>\n",
       "      <td>0.170044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-1.208984</td>\n",
       "      <td>0.086731</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>1.009766</td>\n",
       "      <td>0.365723</td>\n",
       "      <td>-0.464844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-1.789062</td>\n",
       "      <td>-0.198242</td>\n",
       "      <td>-0.172974</td>\n",
       "      <td>0.847656</td>\n",
       "      <td>1.080078</td>\n",
       "      <td>0.759277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-1.682617</td>\n",
       "      <td>-0.299805</td>\n",
       "      <td>-0.057770</td>\n",
       "      <td>1.031250</td>\n",
       "      <td>1.160156</td>\n",
       "      <td>0.654785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10264</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-1.038086</td>\n",
       "      <td>0.406006</td>\n",
       "      <td>0.380127</td>\n",
       "      <td>0.839355</td>\n",
       "      <td>0.057983</td>\n",
       "      <td>-0.520996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10265</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-1.597656</td>\n",
       "      <td>-0.008057</td>\n",
       "      <td>0.088074</td>\n",
       "      <td>0.793457</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.486328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-1.671875</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>0.019196</td>\n",
       "      <td>0.981445</td>\n",
       "      <td>0.876465</td>\n",
       "      <td>0.415039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10267</th>\n",
       "      <td>0.305</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.340088</td>\n",
       "      <td>0.747070</td>\n",
       "      <td>0.042603</td>\n",
       "      <td>-0.229980</td>\n",
       "      <td>-0.360107</td>\n",
       "      <td>-0.011665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10268</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.380859</td>\n",
       "      <td>0.415527</td>\n",
       "      <td>0.596680</td>\n",
       "      <td>0.670898</td>\n",
       "      <td>-0.180786</td>\n",
       "      <td>-1.003906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10269 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PantsTotal  NotRealTotal  BarelyTotal  HalfTotal  MostlyTotal  Truths  \\\n",
       "0           0.005         0.000        0.000      0.000        0.000   0.000   \n",
       "1           0.095         0.160        0.170      0.290        0.165   0.165   \n",
       "2           0.005         0.010        0.005      0.015        0.040   0.010   \n",
       "3           0.005         0.010        0.005      0.015        0.040   0.010   \n",
       "4           0.035         0.145        0.200      0.345        0.380   0.365   \n",
       "...           ...           ...          ...        ...          ...     ...   \n",
       "10264       0.005         0.030        0.070      0.050        0.050   0.020   \n",
       "10265       0.055         0.075        0.080      0.100        0.050   0.035   \n",
       "10266       0.035         0.115        0.140      0.190        0.170   0.075   \n",
       "10267       0.305         0.570        0.315      0.255        0.185   0.070   \n",
       "10268       0.000         0.005        0.000      0.000        0.000   0.000   \n",
       "\n",
       "              0         1         2         3         4         5  \n",
       "0     -0.042389  0.749023 -0.128662 -0.281494 -0.387207  0.003132  \n",
       "1     -1.463867 -0.130737  0.175659  0.963379  0.816895  0.170044  \n",
       "2     -1.208984  0.086731  0.572754  1.009766  0.365723 -0.464844  \n",
       "3     -1.789062 -0.198242 -0.172974  0.847656  1.080078  0.759277  \n",
       "4     -1.682617 -0.299805 -0.057770  1.031250  1.160156  0.654785  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "10264 -1.038086  0.406006  0.380127  0.839355  0.057983 -0.520996  \n",
       "10265 -1.597656 -0.008057  0.088074  0.793457  0.835938  0.486328  \n",
       "10266 -1.671875 -0.160645  0.019196  0.981445  0.876465  0.415039  \n",
       "10267 -0.340088  0.747070  0.042603 -0.229980 -0.360107 -0.011665  \n",
       "10268 -0.380859  0.415527  0.596680  0.670898 -0.180786 -1.003906  \n",
       "\n",
       "[10269 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train=pd.read_excel('trainReputation.xlsx' )\n",
    "train=train.iloc[:,:-2].astype(float)\n",
    "train=train/200  #for scaling\n",
    "#train\n",
    "\n",
    "model_class='albert'  # bert or roberta or albert\n",
    "model_version='albert-large-V2' #bert-base-cased, roberta-base, roberta-large, albert-base-v2 OR albert-large-v2\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Saves/\"\n",
    "TF_Output=pd.read_csv( SavesDirectory+'trainOut.tsv', sep='\\t')\n",
    "\n",
    "train=pd.concat([train,TF_Output], axis=1)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10264</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10265</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10267</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10268</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10269 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4  5\n",
       "0      1  0  0  0  0  0\n",
       "1      0  0  0  1  0  0\n",
       "2      0  0  0  0  1  0\n",
       "3      0  0  0  0  1  0\n",
       "4      0  0  0  0  0  1\n",
       "...   .. .. .. .. .. ..\n",
       "10264  0  0  0  0  1  0\n",
       "10265  0  0  0  0  0  1\n",
       "10266  0  0  0  1  0  0\n",
       "10267  0  1  0  0  0  0\n",
       "10268  0  1  0  0  0  0\n",
       "\n",
       "[10269 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLables=pd.read_excel('trainReputation.xlsx' )\n",
    "TrainLables=TrainLables.iloc[:,-1] \n",
    "\n",
    "TrainLables=pd.get_dummies(TrainLables)\n",
    "TrainLables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0050,  0.0000,  0.0000,  ..., -0.2815, -0.3872,  0.0031],\n",
       "        [ 0.0950,  0.1600,  0.1700,  ...,  0.9634,  0.8169,  0.1700],\n",
       "        [ 0.0050,  0.0100,  0.0050,  ...,  1.0098,  0.3657, -0.4648],\n",
       "        ...,\n",
       "        [ 0.0350,  0.1150,  0.1400,  ...,  0.9814,  0.8765,  0.4150],\n",
       "        [ 0.3050,  0.5700,  0.3150,  ..., -0.2300, -0.3601, -0.0117],\n",
       "        [ 0.0000,  0.0050,  0.0000,  ...,  0.6709, -0.1808, -1.0039]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.tensor(train.values)\n",
    " \n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets=torch.tensor(TrainLables.astype(float).values)\n",
    "\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: 12\n",
      "output size: 6\n"
     ]
    }
   ],
   "source": [
    " \n",
    "size= torch.tensor(input[0].size())\n",
    "InputSize=size.item()\n",
    "\n",
    "OutputSize=torch.tensor(targets[0].size()).item()\n",
    "\n",
    "print('input size:', InputSize)\n",
    "print('output size:', OutputSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "         \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(InputSize, 24)  # input size 32\n",
    "        self.fc2 = nn.Linear(24, 12)\n",
    "        self.fc3 = nn.Linear(12, OutputSize)  #classifies 'outputsize' different classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x)) \n",
    "        x = torch.tanh(self.fc3(x)).double()\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "#now we use it\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we  setup the neural network parameters\n",
    "# pick an optimizer (Simple Gradient Descent)\n",
    "\n",
    "learning_rate = 9e-4\n",
    "criterion = nn.MSELoss()  #computes the loss Function\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# creating optimizer\n",
    "#optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0965, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 0\n",
      "Loss: tensor(0.0965, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1\n",
      "Loss: tensor(0.0965, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2\n",
      "Loss: tensor(0.0965, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 3\n",
      "Loss: tensor(0.0965, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 4\n",
      "Loss: tensor(0.0965, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 5\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 6\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 7\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 8\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 9\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 10\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 11\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 12\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 13\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 14\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 15\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 16\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 17\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 18\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 19\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 20\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 21\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 22\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 23\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 24\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 25\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 26\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 27\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 28\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 29\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 30\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 31\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 32\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 33\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 34\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 35\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 36\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 37\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 38\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 39\n",
      "Loss: tensor(0.0964, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 40\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 41\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 42\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 43\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 44\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 45\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 46\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 47\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 48\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 49\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 50\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 51\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 52\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 53\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 54\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 55\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 56\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 57\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 58\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 59\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 60\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 61\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 62\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 63\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 64\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 65\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 66\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 67\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 68\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 69\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 70\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 71\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 72\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 73\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 74\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 75\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 76\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 77\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 78\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 79\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 80\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 81\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 82\n",
      "Loss: tensor(0.0963, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 83\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 84\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 85\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 86\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 87\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 88\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 89\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 90\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 91\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 92\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 93\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 94\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 95\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 96\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 97\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 98\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 99\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 100\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 101\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 102\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 103\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 105\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 106\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 107\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 108\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 109\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 110\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 111\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 112\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 113\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 114\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 115\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 116\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 117\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 118\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 119\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 120\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 121\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 122\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 123\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 124\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 125\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 126\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 127\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 128\n",
      "Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 129\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 130\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 131\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 132\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 133\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 134\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 135\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 136\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 137\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 138\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 139\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 140\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 141\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 142\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 143\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 144\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 145\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 146\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 147\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 148\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 149\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 150\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 151\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 152\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 153\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 154\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 155\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 156\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 157\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 158\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 159\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 160\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 161\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 162\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 163\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 164\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 165\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 166\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 167\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 168\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 169\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 170\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 171\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 172\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 173\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 174\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 175\n",
      "Loss: tensor(0.0961, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 176\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 177\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 178\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 179\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 180\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 181\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 182\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 183\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 184\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 185\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 186\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 187\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 188\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 189\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 190\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 191\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 192\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 193\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 194\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 195\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 196\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 197\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 198\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 199\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 200\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 201\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 203\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 204\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 205\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 206\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 207\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 208\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 209\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 210\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 211\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 212\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 213\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 214\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 215\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 216\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 217\n",
      "Loss: tensor(0.0960, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 218\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 219\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 220\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 221\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 222\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 223\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 224\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 225\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 226\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 227\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 228\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 229\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 230\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 231\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 232\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 233\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 234\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 235\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 236\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 237\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 238\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 239\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 240\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 241\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 242\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 243\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 244\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 245\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 246\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 247\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 248\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 249\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 250\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 251\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 252\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 253\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 254\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 255\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 256\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 257\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 258\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 259\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 260\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 261\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 262\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 263\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 264\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 265\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 266\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 267\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 268\n",
      "Loss: tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 269\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 270\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 271\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 272\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 273\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 274\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 275\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 276\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 277\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 278\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 279\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 280\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 281\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 282\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 283\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 284\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 285\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 286\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 287\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 288\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 289\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 290\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 291\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 292\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 293\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 294\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 295\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 296\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 297\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 298\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 299\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 300\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 301\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 302\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 303\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 304\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 305\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 306\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 307\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 308\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 309\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 310\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 311\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 312\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 313\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 315\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 316\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 317\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 318\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 319\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 320\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 321\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 322\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 323\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 324\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 325\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 326\n",
      "Loss: tensor(0.0958, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 327\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 328\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 329\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 330\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 331\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 332\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 333\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 334\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 335\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 336\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 337\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 338\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 339\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 340\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 341\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 342\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 343\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 344\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 345\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 346\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 347\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 348\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 349\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 350\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 351\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 352\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 353\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 354\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 355\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 356\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 357\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 358\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 359\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 360\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 361\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 362\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 363\n",
      "Loss: tensor(0.0957, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 364\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 365\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 366\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 367\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 368\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 369\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 370\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 371\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 372\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 373\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 374\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 375\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 376\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 377\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 378\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 379\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 380\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 381\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 382\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 383\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 384\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 385\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 386\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 387\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 388\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 389\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 390\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 391\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 392\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 393\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 394\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 395\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 396\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 397\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 398\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 399\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 400\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 401\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 402\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 403\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 404\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 405\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 406\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 407\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 408\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 409\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 410\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 411\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 412\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 413\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 414\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 415\n",
      "Loss: tensor(0.0956, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 416\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 417\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 418\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 419\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 420\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 421\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 423\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 424\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 425\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 426\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 427\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 428\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 429\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 430\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 431\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 432\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 433\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 434\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 435\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 436\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 437\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 438\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 439\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 440\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 441\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 442\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 443\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 444\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 445\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 446\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 447\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 448\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 449\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 450\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 451\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 452\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 453\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 454\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 455\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 456\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 457\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 458\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 459\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 460\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 461\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 462\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 463\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 464\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 465\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 466\n",
      "Loss: tensor(0.0955, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 467\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 468\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 469\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 470\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 471\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 472\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 473\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 474\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 475\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 476\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 477\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 478\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 479\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 480\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 481\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 482\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 483\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 484\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 485\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 486\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 487\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 488\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 489\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 490\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 491\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 492\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 493\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 494\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 495\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 496\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 497\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 498\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 499\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 500\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 501\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 502\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 503\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 504\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 505\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 506\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 507\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 508\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 509\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 510\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 511\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 512\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 513\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 514\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 515\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 516\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 517\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 518\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 519\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 520\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 521\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 522\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 523\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 524\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 525\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 526\n",
      "Loss: tensor(0.0954, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 527\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 528\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 529\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 530\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 531\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 532\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 533\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 534\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 535\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 536\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 537\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 538\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 539\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 541\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 542\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 543\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 544\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 545\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 546\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 547\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 548\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 549\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 550\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 551\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 552\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 553\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 554\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 555\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 556\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 557\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 558\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 559\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 560\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 561\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 562\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 563\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 564\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 565\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 566\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 567\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 568\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 569\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 570\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 571\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 572\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 573\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 574\n",
      "Loss: tensor(0.0953, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 575\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 576\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 577\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 578\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 579\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 580\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 581\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 582\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 583\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 584\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 585\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 586\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 587\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 588\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 589\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 590\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 591\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 592\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 593\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 594\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 595\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 596\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 597\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 598\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 599\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 600\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 601\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 602\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 603\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 604\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 605\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 606\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 607\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 608\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 609\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 610\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 611\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 612\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 613\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 614\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 615\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 616\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 617\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 618\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 619\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 620\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 621\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 622\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 623\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 624\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 625\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 626\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 627\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 628\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 629\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 630\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 631\n",
      "Loss: tensor(0.0952, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 632\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 633\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 634\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 635\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 636\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 637\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 638\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 639\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 641\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 642\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 643\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 644\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 645\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 646\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 647\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 648\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 649\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 650\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 651\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 652\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 653\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 654\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 655\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 656\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 657\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 658\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 659\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 660\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 661\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 662\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 663\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 664\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 665\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 666\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 667\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 668\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 669\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 670\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 671\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 672\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 673\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 674\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 675\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 676\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 677\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 678\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 679\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 680\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 681\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 682\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 683\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 684\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 685\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 686\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 687\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 688\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 689\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 690\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 691\n",
      "Loss: tensor(0.0951, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 692\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 693\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 694\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 695\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 696\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 697\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 698\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 699\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 700\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 701\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 702\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 703\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 704\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 705\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 706\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 707\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 708\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 709\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 710\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 711\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 712\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 713\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 714\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 715\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 716\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 717\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 718\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 719\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 720\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 721\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 722\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 723\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 724\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 725\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 726\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 727\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 728\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 729\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 730\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 731\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 732\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 733\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 734\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 735\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 736\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 737\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 738\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 739\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 740\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 741\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 742\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 743\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 744\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 745\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 746\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 748\n",
      "Loss: tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 749\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 750\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 751\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 752\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 753\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 754\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 755\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 756\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 757\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 758\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 759\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 760\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 761\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 762\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 763\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 764\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 765\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 766\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 767\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 768\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 769\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 770\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 771\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 772\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 773\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 774\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 775\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 776\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 777\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 778\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 779\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 780\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 781\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 782\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 783\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 784\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 785\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 786\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 787\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 788\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 789\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 790\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 791\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 792\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 793\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 794\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 795\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 796\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 797\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 798\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 799\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 800\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 801\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 802\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 803\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 804\n",
      "Loss: tensor(0.0949, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 805\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 806\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 807\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 808\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 809\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 810\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 811\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 812\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 813\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 814\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 815\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 816\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 817\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 818\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 819\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 820\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 821\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 822\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 823\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 824\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 825\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 826\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 827\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 828\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 829\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 830\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 831\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 832\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 833\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 834\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 835\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 836\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 837\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 838\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 839\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 840\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 841\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 842\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 843\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 844\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 845\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 846\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 847\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 848\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 849\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 850\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 851\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 852\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 853\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 854\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 855\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 856\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 857\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 858\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 859\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 860\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 861\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 862\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 864\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 865\n",
      "Loss: tensor(0.0948, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 866\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 867\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 868\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 869\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 870\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 871\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 872\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 873\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 874\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 875\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 876\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 877\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 878\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 879\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 880\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 881\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 882\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 883\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 884\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 885\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 886\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 887\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 888\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 889\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 890\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 891\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 892\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 893\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 894\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 895\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 896\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 897\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 898\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 899\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 900\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 901\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 902\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 903\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 904\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 905\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 906\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 907\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 908\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 909\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 910\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 911\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 912\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 913\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 914\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 915\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 916\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 917\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 918\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 919\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 920\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 921\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 922\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 923\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 924\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 925\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 926\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 927\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 928\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 929\n",
      "Loss: tensor(0.0947, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 930\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 931\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 932\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 933\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 934\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 935\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 936\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 937\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 938\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 939\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 940\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 941\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 942\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 943\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 944\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 945\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 946\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 947\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 948\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 949\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 950\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 951\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 952\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 953\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 954\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 955\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 956\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 957\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 958\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 959\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 960\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 961\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 962\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 963\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 964\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 965\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 966\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 967\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 968\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 969\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 970\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 971\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 972\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 973\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 974\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 975\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 976\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 978\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 979\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 980\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 981\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 982\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 983\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 984\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 985\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 986\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 987\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 988\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 989\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 990\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 991\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 992\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 993\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 994\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 995\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 996\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 997\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 998\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 999\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1000\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1001\n",
      "Loss: tensor(0.0946, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1002\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1003\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1004\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1005\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1006\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1007\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1008\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1009\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1010\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1011\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1012\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1013\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1014\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1015\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1016\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1017\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1018\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1019\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1020\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1021\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1022\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1023\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1024\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1025\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1026\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1027\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1028\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1029\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1030\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1031\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1032\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1033\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1034\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1035\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1036\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1037\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1038\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1039\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1040\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1041\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1042\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1043\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1044\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1045\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1046\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1047\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1048\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1049\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1050\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1051\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1052\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1053\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1054\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1055\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1056\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1057\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1058\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1059\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1060\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1061\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1062\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1063\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1064\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1065\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1066\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1067\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1068\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1069\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1070\n",
      "Loss: tensor(0.0945, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1071\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1072\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1073\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1074\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1075\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1076\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1077\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1078\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1079\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1080\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1081\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1082\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1083\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1084\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1085\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1086\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1087\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1088\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1089\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1090\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1091\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1092\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1094\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1095\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1096\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1097\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1098\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1099\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1100\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1101\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1102\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1103\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1104\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1105\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1106\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1107\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1108\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1109\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1110\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1111\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1112\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1113\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1114\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1115\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1116\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1117\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1118\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1119\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1120\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1121\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1122\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1123\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1124\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1125\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1126\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1127\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1128\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1129\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1130\n",
      "Loss: tensor(0.0944, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1131\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1132\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1133\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1134\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1135\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1136\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1137\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1138\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1139\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1140\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1141\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1142\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1143\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1144\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1145\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1146\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1147\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1148\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1149\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1150\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1151\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1152\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1153\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1154\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1155\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1156\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1157\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1158\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1159\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1160\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1161\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1162\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1163\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1164\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1165\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1166\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1167\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1168\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1169\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1170\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1171\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1172\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1173\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1174\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1175\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1176\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1177\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1178\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1179\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1180\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1181\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1182\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1183\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1184\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1185\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1186\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1187\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1188\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1189\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1190\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1191\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1192\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1193\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1194\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1195\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1196\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1197\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1198\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1199\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1200\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1201\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1202\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1203\n",
      "Loss: tensor(0.0943, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1204\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1205\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1206\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1207\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1208\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1209\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1211\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1212\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1213\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1214\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1215\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1216\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1217\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1218\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1219\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1220\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1221\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1222\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1223\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1224\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1225\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1226\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1227\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1228\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1229\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1230\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1231\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1232\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1233\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1234\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1235\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1236\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1237\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1238\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1239\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1240\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1241\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1242\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1243\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1244\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1245\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1246\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1247\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1248\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1249\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1250\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1251\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1252\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1253\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1254\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1255\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1256\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1257\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1258\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1259\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1260\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1261\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1262\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1263\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1264\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1265\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1266\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1267\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1268\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1269\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1270\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1271\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1272\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1273\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1274\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1275\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1276\n",
      "Loss: tensor(0.0942, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1277\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1278\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1279\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1280\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1281\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1282\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1283\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1284\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1285\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1286\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1287\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1288\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1289\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1290\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1291\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1292\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1293\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1294\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1295\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1296\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1297\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1298\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1299\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1300\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1301\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1302\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1303\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1304\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1305\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1306\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1307\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1308\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1309\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1310\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1311\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1312\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1313\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1314\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1315\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1316\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1317\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1318\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1319\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1320\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1321\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1322\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1323\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1324\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1325\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1326\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1327\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1328\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1329\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1330\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1331\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1332\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1333\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1334\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1336\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1337\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1338\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1339\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1340\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1341\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1342\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1343\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1344\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1345\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1346\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1347\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1348\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1349\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1350\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1351\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1352\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1353\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1354\n",
      "Loss: tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1355\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1356\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1357\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1358\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1359\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1360\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1361\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1362\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1363\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1364\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1365\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1366\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1367\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1368\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1369\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1370\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1371\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1372\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1373\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1374\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1375\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1376\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1377\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1378\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1379\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1380\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1381\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1382\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1383\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1384\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1385\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1386\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1387\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1388\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1389\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1390\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1391\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1392\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1393\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1394\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1395\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1396\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1397\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1398\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1399\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1400\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1401\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1402\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1403\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1404\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1405\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1406\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1407\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1408\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1409\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1410\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1411\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1412\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1413\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1414\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1415\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1416\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1417\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1418\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1419\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1420\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1421\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1422\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1423\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1424\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1425\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1426\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1427\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1428\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1429\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1430\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1431\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1432\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1433\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1434\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1435\n",
      "Loss: tensor(0.0940, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1436\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1437\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1438\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1439\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1440\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1441\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1442\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1443\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1444\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1445\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1446\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1447\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1448\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1449\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1450\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1451\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1452\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1453\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1454\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1456\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1457\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1458\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1459\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1460\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1461\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1462\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1463\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1464\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1465\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1466\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1467\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1468\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1469\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1470\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1471\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1472\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1473\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1474\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1475\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1476\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1477\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1478\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1479\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1480\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1481\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1482\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1483\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1484\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1485\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1486\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1487\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1488\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1489\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1490\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1491\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1492\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1493\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1494\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1495\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1496\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1497\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1498\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1499\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1500\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1501\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1502\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1503\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1504\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1505\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1506\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1507\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1508\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1509\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1510\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1511\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1512\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1513\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1514\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1515\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1516\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1517\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1518\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1519\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1520\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1521\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1522\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1523\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1524\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1525\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1526\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1527\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1528\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1529\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1530\n",
      "Loss: tensor(0.0939, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1531\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1532\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1533\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1534\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1535\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1536\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1537\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1538\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1539\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1540\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1541\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1542\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1543\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1544\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1545\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1546\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1547\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1548\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1549\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1550\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1551\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1552\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1553\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1554\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1555\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1556\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1557\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1558\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1559\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1560\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1561\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1562\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1563\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1564\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1565\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1566\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1567\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1568\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1569\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1570\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1571\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1572\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1573\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1574\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1575\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1576\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1577\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1578\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1579\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1581\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1582\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1583\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1584\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1585\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1586\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1587\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1588\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1589\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1590\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1591\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1592\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1593\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1594\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1595\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1596\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1597\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1598\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1599\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1600\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1601\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1602\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1603\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1604\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1605\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1606\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1607\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1608\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1609\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1610\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1611\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1612\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1613\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1614\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1615\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1616\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1617\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1618\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1619\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1620\n",
      "Loss: tensor(0.0938, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1621\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1622\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1623\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1624\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1625\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1626\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1627\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1628\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1629\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1630\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1631\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1632\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1633\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1634\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1635\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1636\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1637\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1638\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1639\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1640\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1641\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1642\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1643\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1644\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1645\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1646\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1647\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1648\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1649\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1650\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1651\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1652\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1653\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1654\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1655\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1656\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1657\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1658\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1659\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1660\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1661\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1662\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1663\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1664\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1665\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1666\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1667\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1668\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1669\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1670\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1671\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1672\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1673\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1674\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1675\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1676\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1677\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1678\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1679\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1680\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1681\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1682\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1683\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1684\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1685\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1686\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1687\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1688\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1689\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1690\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1691\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1692\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1693\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1694\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1695\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1696\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1697\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1698\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1699\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1700\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1701\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1702\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1703\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1705\n",
      "Loss: tensor(0.0937, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1706\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1707\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1708\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1709\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1710\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1711\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1712\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1713\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1714\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1715\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1716\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1717\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1718\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1719\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1720\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1721\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1722\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1723\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1724\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1725\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1726\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1727\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1728\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1729\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1730\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1731\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1732\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1733\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1734\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1735\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1736\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1737\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1738\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1739\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1740\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1741\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1742\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1743\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1744\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1745\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1746\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1747\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1748\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1749\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1750\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1751\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1752\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1753\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1754\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1755\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1756\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1757\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1758\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1759\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1760\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1761\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1762\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1763\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1764\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1765\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1766\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1767\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1768\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1769\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1770\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1771\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1772\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1773\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1774\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1775\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1776\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1777\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1778\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1779\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1780\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1781\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1782\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1783\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1784\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1785\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1786\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1787\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1788\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1789\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1790\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1791\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1792\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1793\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1794\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1795\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1796\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1797\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1798\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1799\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1800\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1801\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1802\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1803\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1804\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1805\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1806\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1807\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1808\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1809\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1810\n",
      "Loss: tensor(0.0936, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1811\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1812\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1813\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1814\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1815\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1816\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1817\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1818\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1819\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1820\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1821\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1822\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1823\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1824\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1825\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1827\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1828\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1829\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1830\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1831\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1832\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1833\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1834\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1835\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1836\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1837\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1838\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1839\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1840\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1841\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1842\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1843\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1844\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1845\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1846\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1847\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1848\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1849\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1850\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1851\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1852\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1853\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1854\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1855\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1856\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1857\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1858\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1859\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1860\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1861\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1862\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1863\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1864\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1865\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1866\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1867\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1868\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1869\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1870\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1871\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1872\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1873\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1874\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1875\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1876\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1877\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1878\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1879\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1880\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1881\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1882\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1883\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1884\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1885\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1886\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1887\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1888\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1889\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1890\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1891\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1892\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1893\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1894\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1895\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1896\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1897\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1898\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1899\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1900\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1901\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1902\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1903\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1904\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1905\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1906\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1907\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1908\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1909\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1910\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1911\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1912\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1913\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1914\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1915\n",
      "Loss: tensor(0.0935, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1916\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1917\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1918\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1919\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1920\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1921\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1922\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1923\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1924\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1925\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1926\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1927\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1928\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1929\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1930\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1931\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1932\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1933\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1934\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1935\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1936\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1937\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1938\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1939\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1940\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1941\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1942\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1943\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1944\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1945\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1946\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1947\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1948\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1950\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1951\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1952\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1953\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1954\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1955\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1956\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1957\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1958\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1959\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1960\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1961\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1962\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1963\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1964\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1965\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1966\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1967\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1968\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1969\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1970\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1971\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1972\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1973\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1974\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1975\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1976\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1977\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1978\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1979\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1980\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1981\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1982\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1983\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1984\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1985\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1986\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1987\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1988\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1989\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1990\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1991\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1992\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1993\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1994\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1995\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1996\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1997\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1998\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 1999\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2000\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2001\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2002\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2003\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2004\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2005\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2006\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2007\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2008\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2009\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2010\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2011\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2012\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2013\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2014\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2015\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2016\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2017\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2018\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2019\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2020\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2021\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2022\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2023\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2024\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2025\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2026\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2027\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2028\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2029\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2030\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2031\n",
      "Loss: tensor(0.0934, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2032\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2033\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2034\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2035\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2036\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2037\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2038\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2039\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2040\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2041\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2042\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2043\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2044\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2045\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2046\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2047\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2048\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2049\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2050\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2051\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2052\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2053\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2054\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2055\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2056\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2057\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2058\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2059\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2060\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2061\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2062\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2063\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2064\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2065\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2066\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2067\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2068\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2069\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2070\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2071\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2072\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2074\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2075\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2076\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2077\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2078\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2079\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2080\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2081\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2082\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2083\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2084\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2085\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2086\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2087\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2088\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2089\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2090\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2091\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2092\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2093\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2094\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2095\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2096\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2097\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2098\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2099\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2100\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2101\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2102\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2103\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2104\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2105\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2106\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2107\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2108\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2109\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2110\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2111\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2112\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2113\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2114\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2115\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2116\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2117\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2118\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2119\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2120\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2121\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2122\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2123\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2124\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2125\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2126\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2127\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2128\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2129\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2130\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2131\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2132\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2133\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2134\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2135\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2136\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2137\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2138\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2139\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2140\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2141\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2142\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2143\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2144\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2145\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2146\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2147\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2148\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2149\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2150\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2151\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2152\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2153\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2154\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2155\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2156\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2157\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2158\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2159\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2160\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2161\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2162\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2163\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2164\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2165\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2166\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2167\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2168\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2169\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2170\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2171\n",
      "Loss: tensor(0.0933, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2172\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2173\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2174\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2175\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2176\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2177\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2178\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2179\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2180\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2181\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2182\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2183\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2184\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2185\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2186\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2187\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2188\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2189\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2190\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2191\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2192\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2193\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2194\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2195\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2196\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2198\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2199\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2200\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2201\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2202\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2203\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2204\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2205\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2206\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2207\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2208\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2209\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2210\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2211\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2212\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2213\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2214\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2215\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2216\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2217\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2218\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2219\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2220\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2221\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2222\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2223\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2224\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2225\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2226\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2227\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2228\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2229\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2230\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2231\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2232\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2233\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2234\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2235\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2236\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2237\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2238\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2239\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2240\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2241\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2242\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2243\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2244\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2245\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2246\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2247\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2248\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2249\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2250\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2251\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2252\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2253\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2254\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2255\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2256\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2257\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2258\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2259\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2260\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2261\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2262\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2263\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2264\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2265\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2266\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2267\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2268\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2269\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2270\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2271\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2272\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2273\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2274\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2275\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2276\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2277\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2278\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2279\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2280\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2281\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2282\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2283\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2284\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2285\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2286\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2287\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2288\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2289\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2290\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2291\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2292\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2293\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2294\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2295\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2296\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2297\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2298\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2299\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2300\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2301\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2302\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2303\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2304\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2305\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2306\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2307\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2308\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2309\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2310\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2311\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2312\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2313\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2314\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2315\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2316\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2317\n",
      "Loss: tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2318\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2319\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2320\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2322\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2323\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2324\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2325\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2326\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2327\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2328\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2329\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2330\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2331\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2332\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2333\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2334\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2335\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2336\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2337\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2338\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2339\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2340\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2341\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2342\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2343\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2344\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2345\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2346\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2347\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2348\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2349\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2350\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2351\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2352\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2353\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2354\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2355\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2356\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2357\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2358\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2359\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2360\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2361\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2362\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2363\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2364\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2365\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2366\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2367\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2368\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2369\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2370\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2371\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2372\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2373\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2374\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2375\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2376\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2377\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2378\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2379\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2380\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2381\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2382\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2383\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2384\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2385\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2386\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2387\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2388\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2389\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2390\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2391\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2392\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2393\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2394\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2395\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2396\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2397\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2398\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2399\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2400\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2401\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2402\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2403\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2404\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2405\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2406\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2407\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2408\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2409\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2410\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2411\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2412\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2413\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2414\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2415\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2416\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2417\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2418\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2419\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2420\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2421\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2422\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2423\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2424\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2425\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2426\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2427\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2428\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2429\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2430\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2431\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2432\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2433\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2434\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2435\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2436\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2437\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2438\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2439\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2440\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2441\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2442\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2443\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2445\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2446\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2447\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2448\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2449\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2450\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2451\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2452\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2453\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2454\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2455\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2456\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2457\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2458\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2459\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2460\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2461\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2462\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2463\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2464\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2465\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2466\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2467\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2468\n",
      "Loss: tensor(0.0931, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2469\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2470\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2471\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2472\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2473\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2474\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2475\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2476\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2477\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2478\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2479\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2480\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2481\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2482\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2483\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2484\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2485\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2486\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2487\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2488\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2489\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2490\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2491\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2492\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2493\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2494\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2495\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2496\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2497\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2498\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2499\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2500\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2501\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2502\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2503\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2504\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2505\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2506\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2507\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2508\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2509\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2510\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2511\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2512\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2513\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2514\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2515\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2516\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2517\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2518\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2519\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2520\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2521\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2522\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2523\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2524\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2525\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2526\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2527\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2528\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2529\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2530\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2531\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2532\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2533\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2534\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2535\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2536\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2537\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2538\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2539\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2540\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2541\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2542\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2543\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2544\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2545\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2546\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2547\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2548\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2549\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2550\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2551\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2552\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2553\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2554\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2555\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2556\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2557\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2558\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2559\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2560\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2561\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2562\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2563\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2564\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2565\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2566\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2567\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2569\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2570\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2571\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2572\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2573\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2574\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2575\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2576\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2577\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2578\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2579\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2580\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2581\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2582\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2583\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2584\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2585\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2586\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2587\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2588\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2589\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2590\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2591\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2592\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2593\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2594\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2595\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2596\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2597\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2598\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2599\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2600\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2601\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2602\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2603\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2604\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2605\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2606\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2607\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2608\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2609\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2610\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2611\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2612\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2613\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2614\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2615\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2616\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2617\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2618\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2619\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2620\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2621\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2622\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2623\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2624\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2625\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2626\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2627\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2628\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2629\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2630\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2631\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2632\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2633\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2634\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2635\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2636\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2637\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2638\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2639\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2640\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2641\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2642\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2643\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2644\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2645\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2646\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2647\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2648\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2649\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2650\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2651\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2652\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2653\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2654\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2655\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2656\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2657\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2658\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2659\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2660\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2661\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2662\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2663\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2664\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2665\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2666\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2667\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2668\n",
      "Loss: tensor(0.0930, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2669\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2670\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2671\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2672\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2673\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2674\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2675\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2676\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2677\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2678\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2679\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2680\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2681\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2682\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2683\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2684\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2685\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2686\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2687\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2688\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2689\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2690\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2691\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2693\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2694\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2695\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2696\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2697\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2698\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2699\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2700\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2701\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2702\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2703\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2704\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2705\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2706\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2707\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2708\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2709\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2710\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2711\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2712\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2713\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2714\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2715\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2716\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2717\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2718\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2719\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2720\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2721\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2722\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2723\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2724\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2725\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2726\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2727\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2728\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2729\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2730\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2731\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2732\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2733\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2734\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2735\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2736\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2737\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2738\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2739\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2740\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2741\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2742\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2743\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2744\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2745\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2746\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2747\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2748\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2749\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2750\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2751\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2752\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2753\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2754\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2755\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2756\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2757\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2758\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2759\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2760\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2761\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2762\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2763\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2764\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2765\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2766\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2767\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2768\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2769\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2770\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2771\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2772\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2773\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2774\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2775\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2776\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2777\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2778\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2779\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2780\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2781\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2782\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2783\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2784\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2785\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2786\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2787\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2788\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2789\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2790\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2791\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2792\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2793\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2794\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2795\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2796\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2797\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2798\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2799\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2800\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2801\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2802\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2803\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2804\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2805\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2806\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2807\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2808\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2809\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2810\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2811\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2812\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2813\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2814\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2816\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2817\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2818\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2819\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2820\n",
      "Loss: tensor(0.0929, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2821\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2822\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2823\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2824\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2825\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2826\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2827\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2828\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2829\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2830\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2831\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2832\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2833\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2834\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2835\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2836\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2837\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2838\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2839\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2840\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2841\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2842\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2843\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2844\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2845\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2846\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2847\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2848\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2849\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2850\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2851\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2852\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2853\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2854\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2855\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2856\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2857\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2858\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2859\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2860\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2861\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2862\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2863\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2864\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2865\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2866\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2867\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2868\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2869\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2870\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2871\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2872\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2873\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2874\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2875\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2876\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2877\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2878\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2879\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2880\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2881\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2882\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2883\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2884\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2885\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2886\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2887\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2888\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2889\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2890\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2891\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2892\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2893\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2894\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2895\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2896\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2897\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2898\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2899\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2900\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2901\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2902\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2903\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2904\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2905\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2906\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2907\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2908\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2909\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2910\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2911\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2912\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2913\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2914\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2915\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2916\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2917\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2918\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2919\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2920\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2921\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2922\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2923\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2924\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2925\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2926\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2927\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2928\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2929\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2930\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2931\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2932\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2933\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2934\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2935\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2936\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2937\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2938\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2940\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2941\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2942\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2943\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2944\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2945\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2946\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2947\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2948\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2949\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2950\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2951\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2952\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2953\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2954\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2955\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2956\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2957\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2958\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2959\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2960\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2961\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2962\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2963\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2964\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2965\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2966\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2967\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2968\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2969\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2970\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2971\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2972\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2973\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2974\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2975\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2976\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2977\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2978\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2979\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2980\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2981\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2982\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2983\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2984\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2985\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2986\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2987\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2988\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2989\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2990\n",
      "Loss: tensor(0.0928, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2991\n",
      "Loss: tensor(0.0927, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2992\n",
      "Loss: tensor(0.0927, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2993\n",
      "Loss: tensor(0.0927, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2994\n",
      "Loss: tensor(0.0927, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2995\n",
      "Loss: tensor(0.0927, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2996\n",
      "Loss: tensor(0.0927, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2997\n",
      "Loss: tensor(0.0927, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2998\n",
      "Loss: tensor(0.0927, dtype=torch.float64, grad_fn=<MseLossBackward>)  at epoch: 2999\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):  \n",
    "        \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net(input.float())\n",
    "\n",
    "    loss = criterion(output, targets)\n",
    "    print('Loss:', loss, ' at epoch:', epoch)\n",
    "\n",
    "    loss.backward()  #backprop\n",
    "    optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the FCNN model\n",
    "\n",
    "stage='NNetwork/'\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/\"+stage\n",
    "PATH = SavesDirectory+'Tanh_MSE_adam4856.pth'\n",
    "\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# more on saving pytorch networks: https://pytorch.org/docs/stable/notes/serialization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load previously saved FCNN model \n",
    "\n",
    "stage='NNetwork/'\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/\"+stage\n",
    "PATH = SavesDirectory+'Tanh_MSE_adam4856.pth'\n",
    "\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 4 4 4 1 4 5 5 5 3 3 4 5 3 5 3 3 4 3 4 2 5 1 3 5 5 5 5 1 4 1 3 3 5 5 3 5 5 4 3 4 5 4 5 5 2 1 5 1 4 5 3 1 5 3 5 2 1 3 3 4 5 5 3 3 4 5 4 3 4 3 5 1 4 4 4 3 3 1 4 4 5 4 4 3 3 1 3 1 3 3 3 1 3 5 5 1 3 4 5 0 4 4 4 3 1 3 5 3 1 2 3 3 3 3 3 4 4 3 4 2 2 5 1 4 5 3 1 2 3 3 4 5 4 5 3 5 4 1 1 3 5 3 3 3 5 5 4 2 1 3 1 4 3 1 5 2 5 3 5 4 3 5 1 0 3 3 3 3 4 4 4 3 3 5 2 2 0 3 4 5 3 4 5 1 5 5 3 5 5 1 3 4 5 5 2 4 3 2 1 2 0 5 0 5 1 5 5 5 3 1 0 0 1 3 5 3 4 5 4 5 3 2 1 2 0 3 0 3 5 5 1 1 2 3 5 5 5 5 5 4 1 5 5 5 1 5 3 4 3 3 5 4 2 5 3 5 5 5 5 0 5 5 5 3 4 4 1 1 1 5 5 0 5 1 0 0 3 4 3 0 3 1 5 1 4 3 5 1 5 1 1 4 2 0 3 1 3 4 4 3 3 4 1 1 3 4 4 1 1 1 2 1 3 4 2 0 3 0 3 1 4 1 5 5 5 5 1 5 5 1 1 4 5 4 1 5 2 3 4 5 5 4 3 1 3 4 2 3 0 2 3 5 4 0 1 3 5 2 0 5 4 3 4 4 3 4 5 3 4 1 1 5 5 1 2 3 3 3 3 5 1 5 5 1 5 5 5 3 5 1 0 1 5 3 1 4 3 4 5 4 5 1 3 1 2 1 3 5 4 1 3 3 2 2 3 5 3 3 4 5 3 3 1 1 4 1 1 5 5 3 1 0 1 1 1 1 1 2 3 4 4 4 5 4 5 5 1 1 1 1 3 3 3 1 1 1 4 1 1 2 0 1 1 1 5 4 1 1 1 1 1 3 1 3 4 4 2 3 1 2 3 1 5 5 2 0 2 1 1 1 1 5 5 5 3 3 3 3 3 1 4 4 3 4 4 3 4 3 1 0 1 2 4 3 4 3 3 3 4 1 1 1 5 5 5 1 3 1 1 2 1 1 1 5 1 1 1 5 1 1 4 3 1 1 5 0 3 1 0 3 3 0 4 4 1 5 5 1 4 4 5 3 5 5 2 3 1 3 1 5 5 5 1 4 1 3 4 5 3 5 5 4 4 4 3 5 1 5 1 1 1 2 3 5 1 3 4 1 5 3 4 4 1 3 1 4 3 1 5 3 3 1 2 2 1 1 3 3 4 1 4 1 1 1 2 1 2 0 1 5 1 3 1 0 1 4 5 4 5 2 4 1 3 2 3 1 5 5 1 1 4 1 2 3 2 3 5 2 1 3 1 1 3 4 4 2 0 0 5 1 3 5 1 1 3 5 1 3 3 1 3 1 4 4 1 5 5 1 1 5 5 0 4 4 2 3 1 5 0 3 5 5 5 5 1 5 1 1 0 1 1 3 1 3 4 2 4 1 3 4 1 4 1 5 3 1 0 0 1 3 2 5 5 1 1 3 1 1 5 1 3 5 4 3 3 3 2 3 5 1 5 2 2 1 5 1 3 4 1 4 5 4 4 1 0 2 5 1 3 1 5 5 0 0 0 3 1 4 4 2 1 3 0 4 1 4 1 1 3 3 1 1 3 1 0 4 4 4 4 0 1 0 4 4 4 1 1 3 5 4 5 0 5 1 5 1 1 2 1 1 4 2 4 3 5 4 1 5 1 0 2 0 4 3 3 1 2 0 3 0 3 3 3 2 3 2 4 0 1 1 0 3 1 0 5 5 1 1 2 1 3 3 1 0 1 4 2 5 1 1 2 2 1 1 3 5 4 1 5 1 3 1 3 1 1 1 3 1 3 1 5 1 2 2 1 4 4 4 3 2 4 1 5 1 3 1 1 5 5 1 0 0 0 3 4 3 2 4 3 1 0 2 3 0 2 5 4 1 2 1 0 1 3 1 3 1 3 0 3 2 3 1 4 0 0 3 2 3 3 3 2 4 1 3 0 1 1 1 2 1 3 0 1 1 4 3 2 1 5 0 0 2 1 4 4 0 5 1 1 2 1 3 4 5 1 3 3 4 1 3 3 1 3 5 4 3 5 1 5 1 1 2 2 3 3 3 5 2 4 5 1 3 3 1 1 5 4 0 4 0 3 4 1 1 3 4 1 1 2 5 4 4 5 1 5 0 1 5 5 5 1 1 2 1 4 0 5 1 5 4 4 5 5 5 3 0 1 5 4 1 1 1 0 4 3 4 4 3 1 1 4 1 2 3 3 5 1 1 5 1 5 3 3 1 5 2 3 3 0 4 1 3 1 3 4 4 3 1 2 4 2 3 1 1 4 1 4 1 4 4 3 0 2 3 2 2 4 3 2 3 1 5 5 0 3 2 3 5 4 3 3 3 4 4 3 4 4 3 3 2 4 1 1 4 5 3 5 5 5 3 5 2 2 5 3 1 1 1 4 1 4 2 4 0 0 2 0 0 5 3 2 3 3 0 3 5 0 3 0 3 5 3 4 2 2 3 3 5 1 1 1 2 2 3 3 4 2 1 5 3 1 1 4 1 1 1 3 1 2 3 1 4 3 2 2 3 3 3 2 3 1 5 1 4 3 5 3 5 3 4 1 2 4 4 4 4 2 5 5 3 1 0 1 3 5 5 5 2 3 1 1 2 1 3 5 1 3 5 5 0 4 3 1 2 3 4 3 3 1 3 1 5 3 4 2 1 5 4 1 5 1 0 4 0 0 5 5 1 4 1 3 5 1 4 1 3 4 4 3 1 1 3 4 4 1 1 4 2 5 1 4 4 5 1 0 5 1 1 5 1 5 5 2 3 1 3 2 4 4 5 1 2 1 4 4 4 3 3 4 0 3 3 5 3 3 3 5 5 4 1 2 1 3 3 5 5 4 3 5 1 4 5 3 4 4 5 1 0 2 3 4 4 4 5 1 5 3 3 2 0 3 3 3 0 3 1 3 2 3 3 5 4 5 3 1 1 5 3 5 4 0 1 5 2 5 4 0 1 0 5 5 1 3 3 3 2 0 0 4 2 4 3 4 0 2 3 4 1 2 3 5 4 3 1 0 1 3 4 1 2 5 5 4 5 2 5 3 1 4 2 5 1 1 1 5 0 3 1 4 3 1 3 5 1 5 5 4 1 3 5 3 3 1 3 4 2 4 4 1 1 4 0 0 1 3 2 5 3 3 0 3 3 1 5 5 5 1 0 2 3 2 4 3 3 2 0 0 1 2 5 5 5 4 2 2 3 4 5 3 5 0 1 2 1 3 2 4 3 5 2 1 0 4 4 3 5 5 2 3 3 1 3 3 1 2 3 5 4 3 4 1 1 5 4 2 0 5 3 2 5 5 0 1 3 4 4 3 2 4 4 5 1 1 5 5 3 4 0 3 3 3 5 3 3 5 5 5 1 0 3 3 3 2 2 3 3 5 3 1 1 4 1 5 4 5 3 1 5 3 1 5 0 3 1 1 3 3 1 5 2 0 2 2 5 4 4 5 5 3 3 3 5 3 3 1 0 1 3 0 4 3 0 3 1 1 3 3 3 3 5 0 1 1 2 5 4 1 1 1 3 5 5 3 1 0 0 4 3 3 0 4 4 4 2 5 5 4 5 3 3 4 1 1 5 3 4 3 4 1 3 2 2 1 2 2 5 5 4 2 2 1 4 1 1 1 4 0 1 3 2 3 2 2 4 1 1 4 2 2 0 2 1 4 5 4 1 5 4 1 5 3 3 4 4 5 2 3 5 0 5 2 2 1 1 5 2 3 5 2 0 3 1 3 5 4 1 1 1 1 0 1 3 1 5 3 5 4 1 3 0 0 1 5 2 2 3 2 3 5 5 3 1 4 3 3 2 3 5 3 0 5 4 3 3 2 4 2 1 4 5 1 5 4 3 2 1 2 3 2 1 3 2 2 2 2 2 3 2 3 3 1 3 2 1 2 4 1 2 0 4 3 3 4 3 3 5 5 4 1 1 3 3 2 3 5 0 3 2 4 4 3 1 3 3 3 4 1 2 5 1 3 1 1 4 5 3 1 4 2 3 1 1 2 1 5 2 2 5 3 0 2 4 4 5 3 2 3 0 1 0 3 1 1 1 1 5 1 1 0 1 0 2 3 5 2 2 4 0 5 2 4 2 3 2 4 4 1 1 1 1 0 4 1 4 2 1 5 3 2 4 3 3 1 3 1 1 1 3 3 3 4 2 4 1 3 3 2 5 4 4 3 5 1 4 2 0 3 5 1 0 4 2 3 1 1 2 2 2 5 5 5 0 3 2 2 2 1 3 1 5 2 0 4 1 2 1 4 2 3 0 1 5 3 3 3 4 3 4 4 3 0 0 0 1 5 0 3 2 2 2 2 3 2 1 4 1 3 3 1 1 3 4 2 3 3 3 5 1 1 4 3 5 3 5 4 2 2 2 2 5 0 1 3 2 1 1 1 1 5 1 0 2 4 3 3 1 0 3 0 2 3 5 3 1 0 2 3 3 2 2 2 5 1 2 1 1 3 3 1 1 0 2 3 4 1 3 1 4 3 3 3 5 1 2 1 4 3 1 2 5 1 5 4 0 5 3 3 1 0 4 2 5 5 0 2 5 2 5 1 1 4 2 0 5 5 0 5 1 0 1 3 4 4 1 5 5 5 1 5 4 1 5 2 5 3 2 2 3 3 5 4 5 1 3 3 3 2 1 4 3 2 2 5 3 3 2 4 3 1 5 3 3 4 1 5 2 1 5 5 5 1 5 4 4 3 1 3 3 1 1 5 4 3 3 4 3 5 1 0 4 1 2 4 3 2 5 3 1 5 5 3 1 5 5 3 4 0 1 1 4 4 1 4 4 4 2 4 2 2 0 5 3 1 4 1 1 3 1 4 3 3 4 0 4 4 4 5 4 1 4 2 3 0 1 5 1 1 5 1 4 2 4 3 1 2 5 1 3 5 3 4 4 4 5 5 3 1 4 2 1 4 1 5 3 4 0 5 1 5 5 1 5 5 3 4 5 5 3 0 1 1 4 2 1 5 3 4 5 2 4 5 3 2 5 4 4 0 1 0 5 2 1 5 1 1 5 0 5 3 1 4 5 5 1 4 4 4 2 4 1 5 1 2 0 4 4 4 3 4 1 1 4 4 3 4 3 5 5 2 3 4 1 4 1 4 2 2 3 1 1 1 5 0 5 4 4 1 4 4 2 1 1 3 3 5 4 4 0 4 4 0 3 1 5 3 1 1 5 4 1 5 2 5 3 0 1 3 4 5 4 1 5 1 1 5 3 2 1 3 3 4 5 5 4 5 5 4 3 1 5 1 3 4 3 4 1 2 2 4 0 3 1 1 3 1 3 4 5 2 1 3 3 3 4 2 5 5 1 1 0 3 1 1 1 0 1 1 3 3 0 5 1 5 2 2 0 1 3 2 5 1 4 3 3 3 3 0 1 4 4 1 3 3 4 3 1 3 1 5 5 0 2 5 2 1 0 1 2 3 4 2 5 1 4 4 1 3 2 4 1 3 1 4 5 2 0 2 3 4 5 1 5 1 5 2 3 5 1 5 3 4 4 0 1 3 4 5 0 4 1 3 0 3 1 1 3 2 0 4 1 1 5 5 0 3 3 1 1 1 4 2 1 0 4 4 3 1 2 1 4 3 4 1 5 4 4 5 1 3 3 4 4 1 5 2 1 4 4 3 5 4 5 2 1 1 2 5 1 5 4 4 4 3 4 2 5 1 3 5 3 3 3 3 2 0 3 3 4 3 1 3 1 5 4 1 5 1 5 1 1 5 5 0 4 0 2 3 4 1 1 4 2 1 1 3 1 4 3 4 2 1 4 3 5 1 5 3 0 5 3 4 5 5 1 4 5 0 4 0 5 2 4 5 3 4 4 3 3 3 1 5 4 3 3 5 3 5 3 1 4 1 1 1 4 0 5 1 4 4 1 3 2 3 1 1 4 4 3 1 5 1 1 4 3 1 4 0 1 1 1 1 2 4 3 0 4 4 2 4 4 4 1 5 5 2 4 2 3 5 5 1 3 1 1 5 5 3 5 3 0 1 3 1 5 5 1 4 0 1 2 5 0 5 1 3 1 3 4 5 4 3 3 5 5 3 1 2 1 4 1 5 0 2 2 4 5 3 1 5 3 2 1 3 0 4 1 4 4 5 0 3 3 1 1 0 1 2 5 5 3 2 5 3 3 4 2 4 4 4 4 3 3 5 2 4 4 1 5 4 3 1 1 1 3 3 5 4 3 4 3 3 1 5 3 4 1 5 0 5 3 2 5 5 0 3 1 4 5 4 3 2 2 3 2 1 1 4 3 4 4 1 3 0 0 3 5 3 1 2 4 4 2 5 4 1 2 4 3 4 0 5 4 0 5 1 5 1 1 2 3 3 3 1 3 5 4 3 0 5 3 3 1 2 2 0 3 4 4 2 1 0 4 3 5 3 5 3 0 1 1 2 4 1 1 2 2 3 2 5 4 2 2 2 0 5 1 4 4 1 1 2 4 5 1 0 5 1 1 3 4 3 5 1 1 0 4 5 3 4 5 2 1 5 3 5 4 3 4 0 1 1 3 4 1 3 3 4 4 4 5 5 5 3 4 1 3 4 2 1 3 2 4 1 4 2 1 3 0 4 5 5 3 3 1 1 5 1 4 5 4 3 5 1 4 1 5 2 1 3 3 4 1 4 3 1 1 4 2 1 2 3 5 0 1 5 3 2 5 2 4 1 1 3 3 5 2 0 3 3 1 1 4 2 2 3 3 5 2 1 1 5 5 2 5 4 5 1 0 5 3 4 3 3 3 1 2 1 0 4 5 4 5 3 2 3 0 4 5 2 5 3 2 3 4 3 3 1 1 4 4 4 4 3 3 3 1 4 1 4 5 5 3 0 1 5 1 4 5 2 1 3 4 0 1 0 1 1 4 4 5 4 0 4 1 3 5 5 2 1 4 3 1 2 3 4 4 5 5 2 1 5 2 2 1 1 2 2 3 3 1 5 4 0 2 1 4 4 0 4 3 2 4 2 1 5 5 4 1 5 5 3 0 5 5 1 1 3 4 2 5 4 2 5 2 1 3 4 2 4 3 5 3 3 1 2 4 5 1 2 4 2 4 3 1 4 5 4 3 1 3 1 4 3 5 1 0 3 0 1 1 2 5 2 1 5 3 3 5 3 3 1 2 4 2 4 3 4 1 4 3 3 3 0 1 1 3 5 5 5 3 1 1 2 4 1 1 1 2 3 2 2 1 1 5 3 3 1 3 1 1 0 0 4 3 2 5 1 5 4 2 2 3 1 5 4 2 1 1 4 4 5 2 4 4 4 4 1 3 5 2 5 3 1 1 1 1 3 4 2 1 4 3 4 0 5 2 3 1 3 5 1 1 0 3 4 5 4 1 3 3 0 0 1 5 1 0 0 3 2 0 5 5 4 5 3 0 3 4 5 2 3 3 4 2 1 4 3 1 1 5 3 3 3 2 2 0 5 5 1 3 4 2 1 4 5 2 3 2 1 3 1 2 4 4 2 5 3 1 2 5 1 4 1 4 3 2 0 5 4 1 1 3 1 2 1 3 4 4 5 3 5 3 4 5 5 1 5 2 2 2 4 5 4 0 4 2 2 1 1 1 5 1 2 2 4 2 1 5 1 2 3 4 4 1 2 1 1 5 4 3 3 3 0 4 1 3 3 2 5 3 3 3 3 4 3 2 5 5 1 1 2 3 1 4 4 4 5 1 3 3 2 4 0 1 4 4 5 1 1 5 4 2 4 1 1 3 3 2 1 3 3 5 3 1 4 5 3 1 1 3 4 1 2 1 5 3 0 3 2 5 4 4 2 1 2 3 4 5 1 0 2 4 0 3 3 1 0 3 3 1 1 3 5 3 1 4 1 1 1 5 5 1 3 4 2 4 0 1 1 1 3 3 3 3 3 5 1 3 1 3 1 3 5 3 4 3 1 2 5 3 0 3 1 1 3 1 0 3 5 4 3 3 1 3 0 5 1 4 3 1 3 3 3 5 2 1 1 3 1 2 1 4 1 3 5 5 2 1 3 3 5 4 1 3 0 3 5 4 4 3 3 0 5 2 5 3 2 3 2 3 1 3 1 4 1 3 3 4 2 4 3 2 4 3 1 3 2 3 4 5 1 1 1 1 1 1 0 4 1 2 5 4 2 0 1 3 4 0 3 3 5 0 3 4 1 1 2 1 3 1 4 1 5 3 3 5 3 3 1 3 1 5 3 3 0 5 4 1 2 3 1 3 2 5 3 3 4 5 1 3 3 1 3 5 1 4 3 3 2 5 5 4 1 1 2 1 2 3 0 0 1 5 2 4 4 4 5 4 2 0 4 3 0 1 4 5 5 1 1 1 1 5 3 3 1 5 5 5 4 3 4 2 4 3 0 5 4 3 4 1 3 3 3 1 2 3 5 4 2 1 1 1 3 4 3 2 1 2 1 0 4 3 3 4 2 2 5 1 2 4 3 1 3 3 4 5 5 3 1 1 0 5 5 3 3 3 1 5 5 2 4 3 3 1 3 1 2 1 2 4 3 2 4 3 3 5 5 4 3 3 1 3 3 1 5 1 1 2 3 1 1 3 5 3 2 3 4 0 4 2 1 4 4 1 3 3 4 1 3 1 4 1 2 1 3 2 3 2 5 4 3 2 3 5 5 1 2 1 4 0 1 3 3 3 3 0 0 3 0 1 3 3 5 5 1 1 5 1 3 4 1 1 3 3 1 1 1 3 1 2 3 1 3 2 3 1 3 5 4 5 3 1 3 3 4 3 4 4 4 2 3 3 1 4 1 5 3 5 4 1 4 1 4 3 2 4 3 2 3 1 1 2 5 3 2 5 4 1 3 3 1 4 4 2 3 3 1 0 4 2 0 3 4 5 5 3 0 3 4 1 5 4 4 3 3 3 1 1 4 1 1 1 3 3 1 3 2 1 3 3 4 1 4 1 5 5 1 2 3 4 3 1 2 3 3 3 3 3 3 2 3 4 1 4 3 5 4 5 5 0 5 3 3 0 2 5 4 1 1 2 5 5 2 1 0 2 4 1 5 2 3 4 3 3 1 1 4 3 4 3 4 3 3 5 4 1 3 4 0 5 3 5 1 5 3 1 2 3 0 5 5 2 1 5 0 3 1 1 5 1 3 5 1 1 3 2 1 5 5 2 1 3 1 4 2 5 3 5 1 4 5 4 5 3 4 5 5 2 3 3 3 5 4 3 1 4 5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3 3 3 4 2 1 4 3 3 4 5 1 4 5 3 1 4 1 3 3 1 3 2 5 4 5 2 4 3 4 5 3 5 3 3 5 1 1 0 4 3 2 2 4 3 0 4 0 2 4 0 4 3 4 4 3 4 0 1 2 1 3 5 3 4 1 5 1 4 3 4 5 5 4 1 4 5 3 4 4 1 5 1 3 3 5 4 5 1 1 3 0 3 3 0 4 2 5 2 2 3 0 0 2 1 1 1 1 5 1 2 3 0 3 3 5 4 4 3 2 3 2 4 2 5 4 5 3 2 5 5 3 3 3 1 4 3 1 5 1 3 1 5 2 2 3 1 2 5 4 3 3 1 3 3 4 3 3 5 2 3 0 2 5 2 1 4 4 1 1 4 4 2 2 2 1 3 3 3 1 2 3 2 5 4 5 3 4 1 4 1 2 1 4 5 1 0 4 1 4 5 3 4 5 3 5 5 3 4 2 3 3 3 2 4 4 3 3 4 3 3 1 3 1 5 4 2 1 3 0 1 0 4 5 4 3 2 0 5 5 0 5 3 5 1 0 3 3 4 2 5 2 5 5 0 2 2 2 3 3 4 4 2 3 1 3 1 1 4 1 5 4 1 4 5 4 5 3 3 3 4 5 5 5 5 3 2 1 2 5 1 2 4 1 1 4 3 1 4 1 2 5 1 3 2 4 3 2 0 4 2 5 1 1 0 2 2 3 3 0 1 2 3 1 1 3 3 4 1 4 1 0 4 4 2 5 3 2 3 3 2 0 3 3 1 2 2 4 2 3 1 0 3 2 3 1 1 4 5 2 3 3 4 3 2 1 3 2 2 4 5 4 3 5 0 5 1 3 4 5 0 4 5 3 3 5 4 2 3 5 5 3 3 3 2 2 4 0 4 2 1 3 1 4 5 3 3 3 0 1 5 4 4 1 1 4 1 3 3 1 2 5 3 1 3 5 5 5 0 1 5 5 2 3 3 3 3 5 1 2 2 1 2 5 4 5 5 1 5 3 1 2 1 2 0 4 1 4 3 2 1 3 5 5 2 5 4 1 0 1 2 3 4 1 3 0 3 5 3 5 4 5 1 3 3 3 3 3 0 4 5 5 4 5 3 4 1 1 0 0 5 4 5 2 3 4 0 5 4 1 4 3 4 0 1 1 3 4 4 2 3 4 1 5 1 3 5 2 4 1 1 3 4 4 3 1 2 1 5 2 3 5 5 1 2 1 3 3 1 0 1 3 2 3 4 4 0 1 2 5 1 5 1 1 2 3 2 5 5 0 0 5 3 4 3 3 4 2 0 1 2 5 4 1 4 3 0 3 4 1 0 5 4 3 2 2 1 0 2 4 1 1 0 1 3 3 3 4 1 2 3 2 3 5 1 5 1 1 5 1 3 3 0 2 2 0 2 2 5 5 1 3 3 3 5 2 5 3 0 2 3 3 2 3 2 1 5 1 2 1 3 4 4 1 3 0 2 3 4 1 2 1 5 2 3 2 2 2 4 5 3 4 2 3 5 5 3 4 3 5 4 5 1 5 1 1 0 2 4 1 3 3 0 1 4 2 2 4 4 1 0 0 2 3 1 0 5 3 3 2 2 5 1 5 1 1 0 3 5 1 1 2 1 5 4 2 4 3 4 1 1 2 3 4 2 2 3 5 2 5 3 2 3 5 4 5 0 2 4 5 4 3 4 0 0 2 2 5 3 2 5 5 5 5 2 2 5 3 2 4 2 3 4 5 5 1 5 5 2 3 5 0 3 4 1 4 2 3 1 1 5 4 2 3 3 1 4 1 1 1 4 3 1 5 2 3 1 2 2 3 2 3 2 4 1 2 1 3 5 1 1 1 3 3 1 5 3 3 2 4 5 3 3 5 3 3 5 4 5 5 4 3 1 4 3 3 3 3 3 1 4 2 5 4 3 3 3 3 3 3 3 2 1 1 5 4 4 1 4 1 5 2 5 3 3 2 2 2 3 4 3 3 3 3 1 3 4 4 5 4 2 5 4 1 0 1 2 5 2 4 3 4 2 5 1 4 3 4 4 0 1 1 3 3 0 4 4 1 3 1 0 3 2 3 0 2 2 5 3 3 2 1 1 3 0 1 1 1 0 0 0 5 0 0 4 5 3 2 4 0 3 5 3 3 4 2 4 3 3 5 2 3 3 3 3 5 0 4 2 3 1 0 3 4 5 2 4 5 2 3 2 3 3 3 1 3 1 4 3 0 1 3 5 2 1 3 5 2 1 5 3 3 1 1 1 4 3 3 0 5 1 1 3 2 2 1 4 4 2 3 3 2 1 2 3 4 2 1 2 2 3 2 2 1 1 0 2 5 2 3 5 1 2 5 2 2 1 3 2 3 3 3 2 3 0 4 4 2 5 4 2 2 5 3 5 2 1 4 5 0 2 4 4 4 1 2 3 4 4 1 3 1 2 2 5 5 3 5 3 5 5 3 3 4 5 0 2 3 1 4 5 3 5 5 5 3 2 4 2 2 5 5 3 3 5 0 2 2 3 5 2 4 4 2 0 2 1 4 0 3 2 3 2 5 5 1 4 4 3 0 1 2 2 3 3 3 4 2 0 1 0 2 0 4 4 3 4 5 1 2 3 4 3 0 1 3 4 0 2 2 2 4 5 5 3 2 5 4 4 3 3 0 1 4 1 4 3 4 5 3 3 3 3 1 5 4 5 0 4 1 5 1 1 2 3 3 2 5 2 1 0 4 1 5 5 5 5 5 4 5 3 2 4 4 1 1 5 4 4 5 5 3 2 2 4 4 3 3 0 3 4 2 1 2 5 4 1 5 3 3 3 3 5 2 3 2 1 4 4 5 1 3 5 4 5 3 1 5 4 3 3 5 5 5 4 3 5 4 4 5 4 3 4 2 1 2 4 3 2 4 1 4 2 2 1 3 5 3 5 3 0 3 5 3 5 1 4 2 3 3 1 5 1 1 1 4 4 5 3 1 5 4 4 2 5 3 5 1 5 5 4 4 4 4 4 1 3 1 4 3 4 0 2 4 1 3 4 3 3 2 0 4 5 4 1 5 5 5 4 5 4 3 5 2 4 3 4 4 3 0 5 4 0 3 5 3 1 3 3 1 5 3 1 3 1 4 4 4 4 4 4 5 5 3 4 5 3 5 1 5 4 3 4 4 3 4 4 3 1 5 4 5 2 4 3 5 5 1 5 5 4 2 5 3 1 1 2 5 1 4 5 4 1 1 1 3 3 5 1 4 4 3 3 3 2 0 2 1 2 2 2 3 3 1 5 2 1 4 4 2 3 5 4 4 2 3 4 1 4 1 5 4 2 5 3 1 4 2 4 1 4 3 3 3 3 1 5 4 1 4 5 1 5 3 1 3 2 5 4 5 5 3 1 4 4 2 2 4 3 1 4 5 3 2 1 1 4 3 0 1 4 5 3 5 5 4 5 4 0 1 4 3 4 1 5 0 2 3 2 5 3 1 1 1 5 5 4 3 5 4 3 5 1 5 3 4 3 2 4 3 3 1 3 5 5 2 5 1 4 4 3 4 4 4 3 3 3 0 1 3 5 1 3 4 1 4 3 3 3 3 1 4 1 2 2 4 3 3 3 5 3 5 5 4 1 4 3 2 1 4 3 1 3 3 1 0 5 1 3 3 4 1 2 5 4 3 5 2 3 3 1 5 4 3 1 5 1 3 4 5 2 3 0 4 4 3 3 5 4 2 0 4 4 1 1 0 3 4 4 3 0 3 4 2 1 3 2 4 2 1 3 3 4 5 2 2 0 2 3 5 3 4 2 4 3 3 3 2 3 3 1 3 5 4 4 3 3 2 3 3 4 4 3 3 5 3 4 3 3 3 3 3 4 4 4 3 5 5 5 1 1 3 3 1 3 3 3 5 5 2 5 4 2 2 4 5 5 3 2 5 3 2 3 3 4 3 3 4 1 4 0 4 4 3 2 4 1 1 2 3 4 2 4 3 5 5 5 3 1 3 4 0 3 4 4 5 5 4 1 5 5 1 3 2 5 5 1 1 3 2 5 1 5 2 4 2 2 2 0 5 1 4 5 4 0 5 5 2 1 4 4 4 1 3 4 3 3 4 4 4 0 2 0 4 5 0 5 5 4 3 3 1 5 2 4 0 1 4 3 4 5 5 1 3 4 3 4 5 4 5 3 1 1 3 1 1 4 0 0 2 3 1 1 2 4 1 5 1 1 2 1 4 1 4 2 4 0 2 4 1 1 4 4 2 5 0 4 5 4 3 3 3 2 2 4 1 1 2 3 1 0 1 3 2 4 3 4 1 3 4 1 0 2 1 1 3 5 4 5 1 3 1 3 1 3 3 4 3 1 4 2 3 1 5 3 5 3 2 3 2 5 5 4 4 2 4 2 0 4 2 5 4 3 1 1 5 2 3 1 3 1 5 4 1 3 1 3 1 2 4 1 1 4 3 5 5 4 2 4 1 4 4 3 3 0 5 5 2 3 3 3 3 5 5 1 5 5 1 2 2 2 1 3 3 1 4 5 3 4 2 3 4 2 2 3 4 4 5 2 1 1 1 4 4 1 4 4 3 3 1 3 4 5 1 0 4 3 4 5 4 3 4 5 4 4 4 3 3 1 1 2 4 1 4 5 1 4 4 3 1 1 1 4 3 0 3 2 3 4 4 1 3 2 3 0 1 5 1 4 4 5 1 2 2 2 5 3 3 4 5 2 2 5 5 1 4 2 4 2 2 4 2 4 1 4 5 1 5 4 2 2 2 5 3 0 2 1 3 2 3 4 3 4 4 3 2 0 4 5 0 5 5 4 1 0 0 5 3 2 1 2 5 5 3 1 1 1 5 4 2 2 2 5 1 1 2 0 2 2 5 4 1 1 2 3 2 3 3 0 5 3 3 4 1 3 1 5 2 1 1 4 1 2 1 0 1 1 3 1 1 1 4 0 4 1 2 1 2 0 3 4 0 2 1 3 1 4 1 2 4 3 4 0 4 5 1 4 0 2 4 5 2 5 0 4 1 4 0 2 0 3 4 1 2 5 4 1 3 1 1 5 4 2 4 3 1 3 4 1 0 4 3 3 3 3 4 3 5 2 4 4 2 4 5 4 3 2 3 5 2 0 3 1 3 1 4 3 4 1 2 1 5 3 2 3 1 3 4 3 4 2 2 2 2 3 5 1 3 4 4 4 5 4 0 2 4 1 2 1 1 1 4 4 5 4 4 3 4 2 4 2 1 1 3 3 3 1 2 4 2 4 1 1 5 4 4 2 2 3 3 0 4 2 3 5 2 5 3 5 4 3 1 1 3 3 0 4 3 1 3 3 0 4 5 0 3 4 1 1 3 3 3 0 1 4 4 4 4 3 0 3 4 1 3 3 4 4 3 3 3 2 1 3 3 1 1 1 3 4 4 1 4 5 2 4 3 1 5 4 4 0 4 0 4 3 3 1 2 5 4 4 4 4 4 4 5 4 5 1 5 5 1 4 1 5 4 5 2 2 2 1 4 5 5 1 4 2 1 4 1 4 4 3 5 5 4 0 0 5 4 4 1 4 3 4 3 4 1 2 5 5 1 3 5 2 2 2 3 5 4 3 4 4 4 4 5 2 2 3 1 5 1 1 2 3 5 5 5 4 4 0 2 2 4 5 1 5 5 1 0 4 5 3 5 5 1 4 1 4 1 5 3 1 1 2 4 5 3 5 3 5 5 1 1 1 5 2 1 1 3 3 1 1 5 1 5 4 4 5 3 4 1 3 1 2 1 3 3 3 1 3 1 1 3 1 1 1 3 0 2 5 2 2 4 2 2 4 5 3 4 4 3 3 4 4 2 5 1 1 3 5 3 3 5 3 4 5 3 4 2 3 5 3 1 0 4 4 1 1 0 2 1 4 5 4 4 2 4 3 5 5 2 4 4 4 3 1 1 4 3 4 2 1 5 3 4 3 3 3 4 5 5 1 1 4 2 5 1 3 3 1 5 1 3 3 2 2 1 2 2 2 5 5 1 1 3 2 3 2 0 1 5 0 2 1 3 4 2 1 1 4 5 4 2 1 5 4 2 1 2 3 3 4 4 3 4 2 2 4 4 4 5 3 3 1 3 5 3 3 2 4 2 1 4 0 5 2 4 4 4 2 3 2 5 2 1 1 2 2 2 1 0 4 2 1 4 4 4 4 2 5 3 3 5 3 1 2 2 2 4 3 4 1 1 0 3 2 2 2 3 5 5 4 2 4 0 5 2 2 5 4 4 3 2 2 4 3 2 2 4 1 3 3 2 1 5 4 5 3 4 4 5 3 4 1 4 2 4 4 3 3 0 2 2 0 0 3 3 0 2 1 2 4 0 3 5 0 2 4 4 3 5 2 3 4 2 4 3 1 2 2 1 1 4 0 1 2 1 4 2 1 2 1 5 3 0 3 3 2 0 4 4 4 2 4 4 2 1 3 3 1 2 1 1 3 1 2 1 4 5 3 1 1 4 4 2 2 1 2 1 2 4 1 5 2 4 1 4 4 1 3 1 5 5 1 0 4 3 4 0 4 3 2 3 1 1 2 3 1 3 1 3 2 3 5 0 1 3 3 3 1 4 0 2 4 4 3 2 1 1 1 4 4 3 1 2 2 5 0 3 0 2 5 1 4 1 1 4 0 5 2 2 0 5 5 3 4 5 2 4 4 1 1 1 3 5 3 4 1 1 3 4 4 0 1 4 1 2 0 5 4 4 1 2 5 5 1 1 3 1 0 1 4 1 2 1 4 4 5 3 3 4 3 4 1 4 0 3 2 4 2 1 3 5 4 4 1 5 3 1 2 5 2 1 1 1 3 4 1 3 4 3 1 3 1 4 4 3 1 4 5 5 2 3 1 3 1 4 4 5 3 1 2 2 0 2 4 2 3 5 3 3 1 4 3 4 1 0 0 1 1 3 1 1 2 1 4 3 2 1 4 3 4 3 4 0 5 0 3 0 1 1 4 2 1 3 3 3 5 2 2 2 3 4 4 2 3 1 2 4 4 3 1 2 4 2 4 4 4 3 4 4 1 4 3 2 0 1 4 3 1 1 1 4 3 2 2 4 4 3 4 1 2 2 4 2 4 3 5 2 2 5 0 5 1 3 3 3 5 1 2 1 4 1 3 1 4 0 5 3 3 0 5 0 2 1 1 3 1 2 3 5 1 0 2 2 4 4 4 3 5 1 1 0 4 4 2 1 4 4 3 1 2 1 2 0 4 3 3 0 3 2 3 1 0 2 1 4 1 3 3 1 0 4 5 5 1 2 2 1 5 2 5 1 3 3 1 3 1 3 3 3 4 4 2 3 3 4 5 1 1 2 2 1 4 3 1 2 2 3 4 4 3 3 4 2 1 2 3 3 5 3 4 1 1 3 4 2 4 2 5 4 1 4 3 1 1 5 1 3 2 2 4 1 2 1 3 4 3 2 5 1 3 4 3 4 0 2 1 3 3 3 2 4 1 5 2 2 1 4 1 4 3 4 5 4 4 4 1 2 2 2 2 5 0 0 2 3 1 0 4 1 5 2 2 1 3 3 2 0 1 3 5 1 5 4 4 4 2 2 4 3 4 4 4 1 1 4 3 1 2 2 2 1 1 3 4 2 0 1 1 5 2 3 3 2 1 3 4 2 5 3 3 3 4 4 5 3 4 2 4 1 5 1 1 3 5 4 4 5 1 2 4 2 4 1 4 2 0 3 1 1 3 4 4 2 2 3 3 0 4 2 2 1 1 5 3 4 1 3 2 1 2 2 4 4 3 1 3 2 5 0 1 1 3 3 4 2 3 4 0 2 1 4 3 2 3 2 2 1 4 1 3 4 1 3 4 3 4 3 1 1 1 1 2 2 4 2 2 2 2 5 4 1 1 3 5 2 1 3 3 3 3 1 1 4 1 2 2 5 0 1 3 2 3 4 4 1 3 3 5 4 2 3 5 4 2 5 5 1 1 1 2 1 1 4 3 4 1 4 5 4 4 1 0 1 0 5 4 5 2 2 1 2 0 4 5 4 2 5 3 4 0 5 4 3 5 2 1 4 0 1 4 2 2 3 1 0 4 1 4 5 3 4 5 1 3 4 1 4 4 5 1 4 4 3 2 5 4 4 4 3 0 3 4 2 5 5 5 4 1 3 5 3 4 5 3 1 4 4 4 2 3 1 1 1 4 4 2 1 2 5 3 4 0 2 4 4 0 4 3 3 4 1 2 3 5 5 5 3 4 2 2 5 3 4 3 0 1 0 0 3 1 1 2 3 4 3 1 1 5 4 5 1 5 2 5 4 1 2 2 3 4 3 5 2 1 0 4 1 1 5 3 4 3 3 3 4 4 3 1 5 4 4 3 4 4 4 3 5 4 4 4 5 4 4 5 4 3 1 4 5 4 4 3 4 4 3 0 4 5 1 2 5 1 4 1 4 4 5 1 1 4 2 3 2 4 3 3 4 4 0 2 5 5 1 5 3 2 4 3 3 4 4 0 5 1 5 4 4 1 4 0 4 0 4 5 0 1 4 1 3 4 4 3 4 1 1 2 3 1 1 1 0 4 3 1 3 2 5 5 1 5 5 3 4 0 3 0 5 2 3 1 5 5 3 2 3 4 2 4 5 5 4 5 5 4 1 2 1 3 5 4 4 4 4 2 2 2 1 1 1 2 4 3 1 4 5 4 1 1 3 1 1 4 4 2 1 4 5 4 1 3 1 5 4 1 4 4 2 3 4 1 4 2 1 5 4 2 3 2 4 2 2 3 2 1 1 2 5 4 1 2 5 1 5 1 4 3 4 2 5 5 4 2 4 1 5 3 3 5 1 3 5 3 1 3 1 4 1 1 5 1 4 0 5 1 1 2 1 3 2 4 4 3 3 1 2 5 1 5 3 4 4 4 3 2 4 4 1 3 4 1 5 1 4 3 5 5 2 1 3 1 4 4 3 1 5 5 4 2 2 5 3 4 3 2 3 4 3 3 5 4 5 1 4 5 3 4 2 2 3 0 1 4 1 2 3 3 1 3 4 2 3 0 1 3 2 3 5 1 1 5 3 4 4 1 5 1 5 4 3 2 5 0 2 5 1 5 5 4 4 5 4 3 3 5 1 3 1 1 5 4 3 1 2 1 3 3 2 5 3 3 4 2 1 2 0 2 4 3 4 2 5 0 4 5 3 4 2 1 3 5 3 5 3 3 3 5 3 0 1 2 0 4 5 5 3 2 1 4 3 5 1 4 4 5 4 4 3 4 5 0 1 1 1 3 3 4 3 4 4 4 2 2 3 3 4 1 1 4 1 2 1 1 1 4 0 1 4 4 3 2 3 2 5 1 1 5 2 3 3 2 4 5 4 5 4 1 2 0 1 3 4 3 2 2 3 4 1 2 0 2 4 1 3 4 4 4 1 5 3 1 4 1 3 4 1 0 5 1 5 5 3 4 0 4 3 4 5 4 5 4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4 4 1 0 4 4 4 4 2 4 3 2 1 3 2 4 0 1 4 3 3 4 1 4 5 1 1 5 1 2 1 0 2 5 0 1 1 1 1 2 1 3 1 5 1 1 4 1 0 0 3 3 2 1 0 3 2 1 3 4 2 4 4 1 4 0 3 0 4 1 4 1 3 5 4 4 3 1 1 2 3 4 4 1 1 1 3 5 4 3 1 2 4 3 2 1 4 2 1 2 5 4 1 5 1 4 0 2 5 1 4 1 2 0 5 4 2 2 3 4 4 4 1 1 3 4 1 4 4 3 4 4 4 3 3 0 1 5 5 3 1 5 1 2 4 1 0 5 4 1 4 2 3 3 3 2 5 2 1 3 4 1 2 1 5 0 5 5 2 1 2 1 4 1 1 4 1 3 5 4 3 4 3 0 1 4 4 1 4 3 3 4 5 1 4 2 5 2 2 0 1 1 3 2 3 4 5 1 5 4 0 4 4 1 2 1 5 0 4 4 5 1 4 4 5 4 1 4 1 4 4 5 4 2 1 3 5 2 3 5 5 4 4 5 5 5 1 4 3 1 4 5 2 4 1 0 1 2 4 0 5 0 3 1 4 4 4 1 4 1 5 4 0 4 4 4 1 3 3 5 0 4 1 4 5 1 4 1 1 3 5 5 4 4 1 4 2 2 4 1 4 4 0 4 1 3 1 4 1 2 0 4 4 0 2 1 1 4 1 4 0 2 4 3 5 2 1 3 5 4 5 5 1 3 4 4 1 4 0 0 1 5 1 3 3 1 4 1 3 1 1 2 1 4 1 2 1 5 4 5 4 4 2 1 1 3 1 3 1 3 4 3 1 5 3 4 4 1 1 3 1 1 3 1 3 2 3 2 3 3 1 3 2 4 2 4 0 1 1 1 4 3 1 1 1 2 3 3 3 4 1 4 4 5 2 3 4 4 4 0 2 0 4 3 0 2 0 1 4 2 1 2 5 3 3 3 1 5 0 0 4 3 1 1 3 1 2 2 4 4 3 2 4 2 1 1 4 5 2 1 2 1 4 5 0 4 4 1 3 4 4 2 4 3 2 2 1 5 5 4 3 1 0 4 5 1 3 2 5 3 4 5 0 4 1 3 1 3 1 1 3 3 3 4 1 0 4 3 1 2 3 4 2 5 2 4 2 2 4 4 1 4 0 3 1 1 1 3 0 1 4 3 1 3 2 1 0 1 2 1 2 3 3 5 4 4 4 4 3 1 2 0 4 2 2 1 2 5 2 2 5 4 1 4 4 1 5 1 1 4 0 3 4 2 5 4 3 3 1 1 1 1 4 2 4 5 3 3 2 1 1 2 1 1 1 2 5 3 5 3 3 2 4 2 1 2 3 0 0 2 5 2 4 3 3 5 1 3 4 0 4 2 4 5 3 2 2 1 1 1 4 1 3 0 1 4 5 1 4 1 3 1 5 3 4 2 1 4 4 4 5 4 2 2 0 4 3 1 4 2 1 5 1 4 5 4 3 4 2 1 3 4 5 4 4 4 4 3 2 4 4 4 0 4 5 1 0 4 3 4 1 1 3 3 4 4 4 1 1 3 2 1 4 1 2 2 4 5 4 1 4 3 3 2 1 5 3 1 2 4 0 5 1 0 2 2 3 1 4 1 4 4 4 1 5 4 5 2 5 2 3 4 1 2 4 3 1 1 4 3 1 4 3 2 5 2 2 1 5 3 1 1 5 3 1 1 4 1 1 4 5 4 0 2 2 4 3 3 5 4 5 2 2 0 3 4 5 4 1 3 4 3 4 1 0 4 2 1 5 2 2 1 3 3 3 5 0 3 5 1 2 1 4 5 2 4 4 4 3 4 4 4 5 3 3 2 1 0 1 1 0 4 1 5 3 4 5 4 5 5 1 1 2 1 1 5 1 3 4 1 1 0 2 2 2 4 1 1 4 1 4 4 4 4 4 4 1 3 4 5 1 3 1 3 4 5 4 3 0 3 2 5 3 5 3 1 3 1 0 3 4 4 3 3 4 2 4 3 3 5 3 1 3 5 4 5 4 4 3 0 3 5 3 3 1 5 4 4 4 4 3 3 1 2 1 4 2 1 3 0 1 4 4 1 3 3 2 1 4 2 5 4 4 3 3 2 4 4 2 1 0 1 5 0 1 1 3 1 0 4 5 4 4 5 4 3 4 1 3 2 5 3 0 2 3 4 4 3 0 1 0 3 3 2 3 2 4 3 4 3 3 5 3 1 1 1 1 5 5 1 2 2 2 5 2 1 1 2 0 3 1 4 1 4 3 4 1 4 4 4 3 3 3 0 4 3 2 3 2 3 1 2 2 5 5 1 5 5 1 0 3 1 5 5 5 1 3 5 3 4 0 3 1 0 4 4 4 4 5 4 4 3 4 3 3 4 3 3 4 5 0 1 4 4 1 2 4 1 3 0 1 1 2 5 5 4 0 5 5 2 1 5 5 2 3 3 1 4 2 3 4 3 1 0 4 1 2 0 2 5 3 3 1 4 3 1 3 4 4 2 5 5 1 1 4 4 2 4 1 1 4 1 2 5 5 5 3 3 1 2 3 4 4 1 1 0 3 5 1 0 4 4 0 3 1 0 1 5 4 4 5 0 5 1 1 5 4 5 5 1 5 4 1 5 2 4 1 3 3 4 4 3 3 3 3 1 5 0 4 5 2 4 2 2 5 3 1 3 5 3 3 0 4 5 0 4 2 0 2 5 2 5 5 1 3 2 5 2 2 2 2 0 3 3 3 3 4 5 1 5 1 4 4 3 3 3 1 5 0 1 2 4 3 5 3 1 4 4 3 4 2 1 2 0 4 1 3 4 1 4 2 1 1 0 4 2 1 2 1 4 3 3 3 4 3 4 1 4 2 1 1 3 0 4 3 3 4 1 3 5 0 5 1 4 1 2 4 4 4 1 3 4 3 4 4 3 4 4 3 5 4 3 1 3 4 1 1 1 1 2 4 5 3 4 3 4 4 3 2 2 1 3 1 5 5 5 3 4 3 5 4 4 3 1 3 2 1 5 0 3 1 2 1 0 3 2 1 4 3 5 1 0 1 2 0 5 3 2 1 2 1 2 5 1 1 2 4 3 3 3 3 0 4 3 0 4 2 4 4 4 4 2 2 4 4 2 5 1 1 5 1 5 2 4 2 0 5 3 1 1 3 2 2 0 2 3 5 3 3 1 3 4 3 0 2 1 4 1 3 2 3 3 5 1 0 2 4 4 4 3 1 4 4 4 1 5 1 2 3 5 0 0 2 3 3 1 1 5 3 1 3 3 1 2 1 3 2 3 4 1 0 1 3 1 3 4 2 5 3 2 3 5 2 2 3 0 3 2 5 2 5 4 3 3 1 4 1 4 2 2 1 3 1 2 1 3 2 1 1 5 2 0 3 4 1 1 2 2 4 4 3 2 3 3 2 5 0 1 4 5 1 4 3 4 5 5 3 1 3 1 4 3 1 3 1 1 1 0 5 1 2 4 3 3 1 1 5 3 2 2 5 1 1 4 4 1 1 5 2 1 3 5 1 2 1 5 5 4 1 2 4 3 1 1 4 4 3 1 0 4 2 1 1 2 2 1 2 2 1 3 0 3 1 1 2 3 2 2 3 2 5 3 1 2 5 2 5 1 1 1 3 2 3 3 2 3 1 3 3 3 1 5 2 3 3 2 1 1 1 5 3 3 4 2 2 2 4 3 2 2 3 2 1 2 4 1 3 5 2 0 3 4 0 3 1 0 4 1 1 1 1 5 1 1 2 2 1 1 3 2 1 0 4 2 2 0 2 5 1 1 2 3 3 1 5 3 1 1 3 1 0 4 1 3 3 2 1 5 3 3 3 2 0 3 0 1 3 1 3 0 1 1 1 4 5 4 1 2 1 5 5 1 1 3 3 5 3 3 2 3 1 4 4 5 4 1 3 2 2 4 2 5 1 4 1 0 5 5 5 4 1 1 5 1 2 3 4 3 1 3 2 2 3 3 4 2 2 4 0 1 3 0 2 4 2 3 0 0 3 3 2 3 3 1 3 2 2 5 0 2 2 3 0 1 5 4 3 2 0 3 1 2 2 0 4 4 0 3 1 4 2 3 3 1 1 3 3 3 2 3 4 0 3 1 1 2 4 1 4 3 0 1 0 0 1 2 5 0 1 2 1 3 5 5 5 4 0 1 1 0 0 1 4 1 4 1 1 3 1 0 4 0 0 0 1 4 5 2 2 0 2 2 2 0 0 5 2 1 1 3 5 0 3 3 3 1 1 Correct: 5786 out of: 10269\n",
      "Accuracy of the network :  56.344337325932415\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "countCorrect0=0\n",
    "countCorrect1=0\n",
    "count0=0\n",
    "count1=0\n",
    "labels=pd.read_excel('trainReputation.xlsx' )\n",
    "\n",
    "Y=[]  #target\n",
    "Pred=[]  #predicted\n",
    "\n",
    "with torch.no_grad():\n",
    "    for row in range(len(input)):\n",
    "        outputs = net(input[row,:].float())\n",
    "        result=0\n",
    "        total+=1\n",
    "        if outputs[0]<outputs[1]:result=1\n",
    "        if outputs[result]<outputs[2]:result=2\n",
    "        if outputs[result]<outputs[3]:result=3\n",
    "        if outputs[result]<outputs[4]:result=4\n",
    "        if outputs[result]<outputs[5]:result=5\n",
    "        \n",
    "        if TrainLables.iloc[row,result]==1: correct+=1\n",
    "        \n",
    "        Y.append(labels.iloc[row])\n",
    "        Pred.append(result)\n",
    "        \n",
    "        print(result, end=' ')\n",
    "        \n",
    "    \n",
    "print('Correct:', correct, 'out of:', total )\n",
    "print('Accuracy of the network : ',( 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0200,  0.0500,  0.0550,  ..., -0.7241, -1.0283, -0.5869],\n",
       "        [ 0.0100,  0.0300,  0.0300,  ...,  0.9609,  0.2737, -0.0728],\n",
       "        [ 0.0450,  0.3550,  0.3500,  ...,  0.2751, -0.1869, -0.5029],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.5620,  1.2666,  1.1582],\n",
       "        [ 0.0000,  0.0500,  0.0400,  ...,  0.5962,  1.1602,  1.1270],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.7217,  1.0645,  0.8315]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the validation data\n",
    "\n",
    "ValidData=pd.read_excel('validReputation.xlsx' )\n",
    "ValidData=ValidData.iloc[:,:-2].astype(float)\n",
    "ValidData=ValidData/200\n",
    "\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Saves/\"\n",
    "TF_Output=pd.read_csv( SavesDirectory+'evalOut.tsv', sep='\\t')\n",
    "\n",
    "ValidData=pd.concat([ValidData,TF_Output], axis=1)\n",
    "\n",
    "\n",
    "ValidData=torch.tensor(ValidData.values)\n",
    "ValidData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1284 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5\n",
       "0     1  0  0  0  0  0\n",
       "1     0  0  0  1  0  0\n",
       "2     0  0  0  1  0  0\n",
       "3     0  0  0  0  1  0\n",
       "4     0  0  0  0  1  0\n",
       "...  .. .. .. .. .. ..\n",
       "1279  0  1  0  0  0  0\n",
       "1280  0  0  1  0  0  0\n",
       "1281  0  0  0  0  1  0\n",
       "1282  0  0  0  0  1  0\n",
       "1283  0  0  0  1  0  0\n",
       "\n",
       "[1284 rows x 6 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=pd.read_excel('validReputation.xlsx' )\n",
    "\n",
    "labels=labels.iloc[:,-1] \n",
    "labelsOneHot=pd.get_dummies(labels)\n",
    "labelsOneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ValidLables =torch.tensor(labelsOneHot.values)\n",
    "ValidLables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 2 5 4 2 5 5 4 1 5 3 3 5 5 0 1 5 0 3 4 5 1 1 3 3 5 5 3 2 5 4 1 1 0 5 3 4 4 0 0 4 1 0 5 3 4 1 4 4 4 1 0 1 5 1 5 3 5 5 3 5 1 1 1 3 1 5 1 1 1 1 5 0 2 5 2 1 3 3 1 2 4 1 4 4 3 4 1 1 1 2 3 1 4 1 2 1 0 1 0 3 1 4 5 1 0 1 4 1 3 1 4 1 1 1 1 5 1 1 1 1 5 0 3 2 1 3 5 5 4 3 5 4 4 3 3 2 3 3 3 4 4 1 3 3 1 3 1 4 1 1 1 1 5 2 5 2 3 2 3 5 3 4 5 4 1 5 2 3 3 3 1 2 1 4 5 4 2 3 1 5 5 2 3 5 4 5 5 4 0 5 3 4 4 5 2 3 4 5 3 4 3 2 4 2 4 4 3 2 2 1 1 1 1 3 3 3 2 1 4 2 1 3 3 4 2 1 2 3 3 2 2 2 1 2 2 2 1 4 3 5 2 3 3 4 3 4 5 3 2 4 1 1 2 2 0 4 1 1 4 5 3 4 5 2 2 3 3 1 2 1 4 1 4 1 5 1 5 5 3 2 1 4 3 1 1 0 3 1 1 1 1 4 4 1 1 1 2 3 4 2 2 5 0 3 1 3 3 0 5 3 4 2 4 3 5 1 5 4 5 2 1 2 1 1 4 4 1 4 3 2 3 5 4 2 4 3 1 5 1 1 1 1 3 0 1 2 2 3 1 4 3 3 2 1 4 1 2 2 2 4 3 2 2 5 1 1 5 1 4 1 2 1 2 1 5 5 2 1 1 4 2 1 5 3 2 3 5 5 1 1 5 0 4 3 5 1 1 2 4 4 3 4 5 5 4 1 4 3 0 1 2 3 3 4 5 3 4 4 4 3 4 4 5 1 2 4 1 1 3 0 5 3 2 0 1 5 1 1 4 1 4 4 5 4 1 2 3 3 5 2 3 3 3 4 1 5 1 2 2 4 2 3 1 1 1 4 5 0 3 4 4 5 4 1 5 1 3 4 5 0 1 3 1 3 2 3 2 4 5 4 4 1 2 2 3 1 1 2 3 4 4 3 5 4 3 1 4 2 4 0 5 5 5 4 3 2 3 3 1 1 3 5 3 3 3 1 1 1 2 5 4 3 4 3 2 1 0 2 4 2 0 4 0 1 5 4 2 4 2 3 4 1 3 4 4 2 1 4 2 3 3 1 1 5 2 2 3 1 2 3 3 2 1 4 3 4 0 1 0 4 3 2 3 5 4 4 3 1 1 1 5 4 2 5 1 3 5 3 1 2 5 3 5 0 3 4 5 2 5 4 1 3 2 2 2 1 1 1 4 5 3 0 4 1 5 3 2 4 2 1 3 2 4 1 1 5 2 3 2 1 1 1 3 1 1 3 4 2 1 3 2 3 0 3 4 4 2 3 2 1 3 4 5 1 1 5 4 2 1 4 0 4 1 4 4 3 2 5 5 4 1 3 2 3 3 3 4 4 4 5 2 1 5 1 3 5 3 4 1 5 4 1 4 1 2 1 4 0 2 0 1 5 1 5 3 3 3 2 3 4 3 1 3 0 3 3 5 3 5 5 1 3 5 2 4 5 5 5 3 3 1 5 3 0 2 2 4 1 1 1 4 2 0 0 4 2 0 2 4 2 5 3 4 3 2 1 3 3 3 1 3 3 1 2 5 3 2 1 3 3 4 0 2 4 3 2 4 3 1 3 1 1 2 1 5 2 4 4 4 0 3 4 3 1 2 4 4 5 2 5 2 1 1 3 3 1 5 1 4 1 3 3 2 5 1 4 5 3 3 4 1 3 5 3 0 4 4 2 2 3 0 1 3 1 3 1 4 2 5 5 3 5 0 5 3 2 1 5 2 4 3 3 0 0 0 1 1 1 4 4 3 2 3 5 0 4 2 0 4 3 1 1 2 3 3 4 4 5 1 1 4 2 1 1 1 4 4 2 3 1 2 1 1 3 1 3 2 5 4 2 5 3 2 1 1 3 4 2 5 4 4 0 3 2 3 1 3 3 3 4 1 3 3 1 5 4 1 3 2 5 5 0 0 5 3 4 3 2 0 4 0 2 4 2 2 4 0 2 2 3 5 1 3 3 4 2 1 1 2 2 2 4 4 5 5 1 2 3 4 3 0 4 5 1 5 2 3 0 4 4 1 1 1 1 3 4 4 5 3 4 1 1 4 4 1 2 5 4 4 4 4 3 5 1 5 3 2 5 1 0 3 5 2 4 2 5 2 2 3 2 4 2 4 1 1 4 0 2 2 0 4 1 2 3 4 2 4 1 5 3 2 2 5 2 4 4 1 3 2 3 5 1 2 3 4 5 0 2 5 1 2 3 4 3 2 4 4 4 5 4 3 4 0 1 3 4 3 1 1 5 2 4 4 1 3 4 1 1 5 1 3 5 0 4 0 4 1 4 5 3 2 3 4 3 1 4 2 5 1 3 1 4 1 4 2 4 2 0 1 1 4 3 3 5 4 1 4 5 4 3 2 2 4 4 1 3 4 1 5 4 4 3 4 1 2 4 2 4 2 1 5 5 3 5 1 5 5 2 3 1 4 1 2 1 1 1 5 5 1 2 4 3 4 0 3 3 3 1 4 1 1 4 5 4 1 5 3 2 4 0 4 1 1 3 4 4 1 3 1 0 4 4 4 3 2 1 1 3 4 2 3 1 3 3 4 3 1 2 4 3 3 5 0 3 5 1 1 1 2 5 1 1 1 5 4 3 2 4 1 5 1 3 2 4 4 3 3 5 3 2 1 4 3 2 1 2 1 1 3 3 1 0 0 5 2 1 0 0 2 4 3 3 Correct: 645 out of: 1284\n",
      "Accuracy of the network :  50.23364485981308\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "countCorrect0=0\n",
    "countCorrect1=0\n",
    "count0=0\n",
    "count1=0\n",
    "\n",
    "Y=[]  #target\n",
    "Pred=[]  #predicted\n",
    "\n",
    "with torch.no_grad():\n",
    "    for row in range(len(ValidData)):\n",
    "        outputs = net(ValidData[row,:].float())\n",
    "        result=0\n",
    "        total+=1\n",
    "        if outputs[0]<outputs[1]:result=1\n",
    "        if outputs[result]<outputs[2]:result=2\n",
    "        if outputs[result]<outputs[3]:result=3\n",
    "        if outputs[result]<outputs[4]:result=4\n",
    "        if outputs[result]<outputs[5]:result=5\n",
    "        \n",
    "        if labelsOneHot.iloc[row,result]==1: correct+=1\n",
    "        \n",
    "        Y.append(labels.iloc[row])\n",
    "        Pred.append(result)\n",
    "        \n",
    "        print(result, end=' ')\n",
    "        \n",
    "    \n",
    "print('Correct:', correct, 'out of:', total )\n",
    "print('Accuracy of the network : ',( 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0050,  0.0100,  0.0050,  ...,  0.7627,  0.9614,  0.9639],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.1152, -0.4983, -0.6050],\n",
       "        [ 0.0200,  0.0250,  0.0600,  ...,  0.1076, -0.3293, -0.4626],\n",
       "        ...,\n",
       "        [ 0.0100,  0.0050,  0.0250,  ..., -0.4456, -1.0010, -0.7314],\n",
       "        [ 0.2200,  0.0950,  0.0350,  ..., -0.0431, -0.6367, -0.5122],\n",
       "        [ 0.0050,  0.0600,  0.0100,  ...,  0.0622, -0.0334,  0.0844]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the test data\n",
    "\n",
    "TestData=pd.read_excel('testReputation.xlsx' )\n",
    "TestData=TestData.iloc[:,:-2].astype(float)\n",
    "TestData=TestData/200\n",
    "\n",
    "SavesDirectory='./TunedModels/'+model_class+'/'+model_version+\"/Saves/\"\n",
    "TF_Output=pd.read_csv( SavesDirectory+'testOut.tsv', sep='\\t')\n",
    "\n",
    "TestData=pd.concat([TestData,TF_Output], axis=1)\n",
    "\n",
    "\n",
    "TestData=torch.tensor(TestData.values)\n",
    "TestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1283 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5\n",
       "0     0  0  0  0  1  0\n",
       "1     0  0  0  1  0  0\n",
       "2     0  1  0  0  0  0\n",
       "3     0  0  0  0  0  1\n",
       "4     0  0  0  0  0  1\n",
       "...  .. .. .. .. .. ..\n",
       "1278  0  1  0  0  0  0\n",
       "1279  0  0  0  0  1  0\n",
       "1280  0  0  1  0  0  0\n",
       "1281  1  0  0  0  0  0\n",
       "1282  0  1  0  0  0  0\n",
       "\n",
       "[1283 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=pd.read_excel('testReputation.xlsx' )\n",
    "\n",
    "labels=labels.iloc[:,-1] \n",
    "labelsOneHot=pd.get_dummies(labels)\n",
    "labelsOneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestLables =torch.tensor(labelsOneHot.values)\n",
    "TestLables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3 3 4 3 3 3 4 5 4 3 3 3 1 5 3 4 1 1 4 4 4 5 3 4 3 4 3 3 5 4 5 0 3 4 4 2 3 5 5 1 1 3 1 1 3 5 2 2 5 4 4 3 1 3 3 5 5 5 1 1 4 1 3 1 4 3 4 2 5 5 1 3 4 1 1 5 1 5 3 2 1 5 4 1 5 1 2 5 4 4 1 1 1 5 3 5 5 3 2 4 0 4 1 3 1 1 2 1 5 1 3 3 4 1 0 1 4 1 2 2 2 4 3 1 1 1 4 5 1 3 3 0 1 1 2 5 1 2 3 4 3 1 3 3 4 4 3 4 1 0 3 1 4 5 3 4 5 1 3 4 1 5 1 3 5 4 4 3 1 2 3 5 5 1 3 3 2 1 2 1 4 1 1 1 0 2 1 4 4 2 5 5 0 1 1 2 2 3 1 1 4 5 3 4 1 3 5 4 3 2 3 1 0 1 1 3 4 1 1 4 1 5 3 3 5 0 4 3 0 4 1 0 1 3 2 2 0 4 2 2 1 2 1 4 4 2 1 3 3 0 5 5 4 3 3 5 0 4 1 3 1 5 3 0 3 1 4 4 2 1 3 1 2 5 2 5 1 5 4 5 1 3 5 1 5 3 1 5 0 3 4 0 1 1 4 3 5 3 4 1 1 5 5 5 3 0 5 3 4 1 4 1 3 3 4 5 3 3 3 3 4 5 4 1 0 5 1 5 3 4 3 3 2 5 1 1 1 4 2 4 2 5 1 3 5 5 3 5 2 1 0 4 1 4 1 1 4 1 2 4 1 1 1 2 0 4 2 3 1 0 1 5 5 4 3 4 5 3 1 3 1 3 4 1 3 3 3 3 3 3 2 1 1 2 2 3 3 3 4 3 3 0 1 4 5 2 5 3 3 1 2 1 3 1 1 1 4 3 4 2 4 4 1 1 3 4 3 4 3 5 4 2 5 2 4 3 1 2 1 5 5 1 1 3 1 3 3 1 5 1 3 1 3 4 3 3 3 3 1 5 2 3 4 5 2 1 3 1 3 0 4 1 4 3 2 3 1 3 3 2 3 3 1 2 5 2 0 1 5 1 5 4 3 2 0 3 4 0 4 3 4 1 0 5 5 1 3 5 3 1 5 2 3 4 5 1 3 1 5 1 3 2 4 1 5 1 1 3 5 5 0 0 1 1 3 4 1 3 5 2 5 4 3 1 3 0 5 3 2 1 3 2 2 3 3 2 2 2 1 1 5 1 1 0 1 5 4 1 1 4 3 5 0 1 4 3 4 0 4 1 2 3 1 2 1 3 5 5 5 3 0 3 2 1 1 1 3 5 5 2 5 2 4 3 3 4 3 2 4 1 2 4 2 2 5 4 3 4 4 2 4 3 1 2 5 5 5 4 0 1 2 1 4 3 1 3 3 2 2 2 3 3 1 3 4 2 3 3 4 3 2 5 2 5 2 1 5 5 3 5 2 4 5 3 4 4 4 4 0 4 5 5 3 3 3 1 4 5 4 5 4 4 1 2 2 3 4 2 4 3 5 2 4 4 4 4 4 4 0 1 5 4 1 1 1 3 3 3 2 4 4 3 4 3 4 4 4 4 4 1 4 4 4 1 4 5 5 3 1 5 2 3 4 4 3 3 5 0 1 4 4 4 5 3 2 1 0 5 0 3 4 4 3 3 5 4 1 1 3 2 5 0 1 1 4 1 1 4 2 4 4 4 4 0 2 1 5 3 4 3 3 5 2 0 5 2 1 2 4 4 4 5 5 5 3 2 2 5 3 5 4 3 2 5 1 0 3 3 5 1 4 5 4 3 0 5 4 1 4 1 5 4 4 3 3 3 4 3 3 4 3 1 5 4 3 4 3 0 2 4 1 4 5 5 2 4 0 3 5 3 3 2 3 4 4 5 2 1 2 4 1 2 5 1 1 3 3 3 0 4 5 3 1 0 1 3 0 2 4 5 1 2 0 3 3 2 3 4 4 4 2 2 3 4 1 3 5 3 2 1 4 1 2 3 1 2 1 4 5 2 1 1 2 4 4 4 0 0 2 3 1 1 3 2 5 3 2 3 2 2 1 3 5 3 3 4 1 3 1 4 4 3 4 4 5 5 5 3 3 3 3 1 3 3 3 2 1 5 5 0 4 3 1 1 5 4 2 3 1 1 1 1 4 0 4 0 2 4 4 5 2 4 4 5 4 3 1 5 5 2 1 5 4 5 4 1 1 5 0 1 2 5 1 1 3 0 4 4 2 1 4 2 2 4 4 1 3 5 1 4 3 5 4 5 1 4 4 2 2 1 4 5 0 5 4 4 1 4 4 4 5 1 0 4 4 5 3 3 1 2 3 4 3 4 5 1 5 0 2 3 0 4 4 1 0 4 1 2 1 2 3 4 4 4 0 4 5 1 1 4 3 3 4 3 3 1 4 2 5 1 5 4 0 2 2 2 1 2 1 4 4 4 2 3 4 4 0 1 2 5 3 0 2 1 2 2 5 2 1 4 1 3 5 1 2 1 5 1 1 4 1 2 3 4 1 0 3 4 5 4 2 0 3 2 2 3 4 0 3 1 2 1 4 2 4 5 3 4 4 1 5 2 5 2 4 1 5 5 4 3 1 4 1 5 3 0 2 4 1 3 4 1 1 3 0 5 4 5 2 3 1 4 4 1 0 4 5 1 3 1 4 4 4 1 3 4 4 1 1 5 5 3 2 4 1 1 2 5 0 2 0 1 5 1 1 4 1 2 5 3 2 3 2 1 1 2 2 4 3 3 1 5 1 5 3 3 3 3 2 0 4 3 4 0 2 3 1 3 1 3 1 1 2 3 1 3 3 3 4 3 3 1 3 0 1 1 2 4 2 0 0 1 Correct: 623 out of: 1283\n",
      "Accuracy of the network :  48.5580670303975\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "countCorrect0=0\n",
    "countCorrect1=0\n",
    "count0=0\n",
    "count1=0\n",
    "\n",
    "Y=[]  #target\n",
    "Pred=[]  #predicted\n",
    "\n",
    "with torch.no_grad():\n",
    "    for row in range(len(TestData)):\n",
    "        outputs = net(TestData[row,:].float())\n",
    "        result=0\n",
    "        total+=1\n",
    "        if outputs[0]<outputs[1]:result=1\n",
    "        if outputs[result]<outputs[2]:result=2\n",
    "        if outputs[result]<outputs[3]:result=3\n",
    "        if outputs[result]<outputs[4]:result=4\n",
    "        if outputs[result]<outputs[5]:result=5\n",
    "        \n",
    "        if labelsOneHot.iloc[row,result]==1: correct+=1\n",
    "        \n",
    "        Y.append(result)\n",
    "        Pred.append(labels.iloc[row])\n",
    "        \n",
    "        print(result, end=' ')\n",
    "        \n",
    "       \n",
    "print('Correct:', correct, 'out of:', total )\n",
    "print('Accuracy of the network : ',( 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 50  10   9   6   1   3]\n",
      " [ 17 145  35  48  20  25]\n",
      " [  6  26  89  30  20  12]\n",
      " [ 12  31  40 112  52  26]\n",
      " [  3  25  24  47 126  44]\n",
      " [  4  13  17  24  30 101]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    " \n",
    "print(metrics.confusion_matrix(Y,Pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Pants       0.65      0.47      0.54       116\n",
      "       False       0.48      0.53      0.51       263\n",
      " Barely-True       0.50      0.45      0.47       237\n",
      "   Hlaf-True       0.48      0.51      0.49       248\n",
      " Mostly-True       0.49      0.51      0.50       251\n",
      "        True       0.43      0.44      0.43       169\n",
      "\n",
      "    accuracy                           0.49      1284\n",
      "   macro avg       0.50      0.48      0.49      1284\n",
      "weighted avg       0.49      0.49      0.49      1284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Pants', 'False', 'Barely-True','Hlaf-True','Mostly-True','True']\n",
    "\n",
    "print(metrics.classification_report(Y, Pred,target_names =target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
